Current configuration:
{
    "concurrency_flag": true,
    "data_distribution": "iid",
    "dataset": "twitter_dep",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedprox",
    "layers": 3,
    "learning_rate": 0.0006,
    "max_length": 512,
    "ml_mode": "bcfl",
    "model": "bert_small",
    "model_name": "fedprox_iid_seed4_twitter_dep",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/phase-4/final/data-dist/twitter_dep",
    "mu": 0.5,
    "num_clients": 5,
    "num_epochs": 3,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": "random",
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
seed set:  250462
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 3 layers.

Total parameters count: 28764674
Trainable parameters count: 3416066
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 1982 total sentences. 496 batches of size 4.
Eval Loader: 496 total sentences. 124 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Training --------
Training with Blockchain-Based Federated Learning technology.
Training without differential privacy.
Federated proximal algorithm selected.
Using mu value of 0.5.
Client 0: Label 0: 281, Label 1: 115
Client 1: Label 0: 280, Label 1: 116
Client 2: Label 0: 293, Label 1: 103
Client 3: Label 0: 303, Label 1: 93
Client 4: Label 0: 275, Label 1: 123

Round 1 of 4
-------------------------------
Predictions - 1s: 65, 0s: 331
True Labels - 1s: 116, 0s: 280
Client 2 of 5: Local Epoch [1/3] Loss: 0.9622, Accuracy: 68.94 %
Predictions - 1s: 63, 0s: 333
True Labels - 1s: 115, 0s: 281
Client 1 of 5: Local Epoch [1/3] Loss: 0.9478, Accuracy: 68.18 %
Predictions - 1s: 33, 0s: 363
True Labels - 1s: 93, 0s: 303
Client 4 of 5: Local Epoch [1/3] Loss: 0.8593, Accuracy: 74.75 %
Predictions - 1s: 25, 0s: 371
True Labels - 1s: 103, 0s: 293
Client 3 of 5: Local Epoch [1/3] Loss: 0.9425, Accuracy: 70.20 %
Predictions - 1s: 77, 0s: 321
True Labels - 1s: 123, 0s: 275
Client 5 of 5: Local Epoch [1/3] Loss: 0.9789, Accuracy: 68.34 %
Predictions - 1s: 36, 0s: 360
True Labels - 1s: 116, 0s: 280
Client 2 of 5: Local Epoch [2/3] Loss: 0.7606, Accuracy: 71.21 %
Predictions - 1s: 44, 0s: 352
True Labels - 1s: 116, 0s: 280
Client 2 of 5: Local Epoch [3/3] Loss: 0.6348, Accuracy: 74.24 %
Predictions - 1s: 25, 0s: 371
True Labels - 1s: 93, 0s: 303
Client 4 of 5: Local Epoch [2/3] Loss: 0.7104, Accuracy: 75.76 %
Predictions - 1s: 29, 0s: 367
True Labels - 1s: 115, 0s: 281
Client 1 of 5: Local Epoch [2/3] Loss: 0.7810, Accuracy: 70.71 %
Predictions - 1s: 53, 0s: 345
True Labels - 1s: 123, 0s: 275
Client 5 of 5: Local Epoch [2/3] Loss: 0.7591, Accuracy: 71.36 %
Predictions - 1s: 26, 0s: 370
True Labels - 1s: 103, 0s: 293
Client 3 of 5: Local Epoch [2/3] Loss: 0.7357, Accuracy: 72.47 %
Predictions - 1s: 18, 0s: 378
True Labels - 1s: 93, 0s: 303
Client 4 of 5: Local Epoch [3/3] Loss: 0.5697, Accuracy: 76.52 %
Predictions - 1s: 44, 0s: 352
True Labels - 1s: 115, 0s: 281
Client 1 of 5: Local Epoch [3/3] Loss: 0.6558, Accuracy: 72.98 %
Predictions - 1s: 67, 0s: 331
True Labels - 1s: 123, 0s: 275
Client 5 of 5: Local Epoch [3/3] Loss: 0.6348, Accuracy: 73.87 %
Predictions - 1s: 23, 0s: 373
True Labels - 1s: 103, 0s: 293
Client 3 of 5: Local Epoch [3/3] Loss: 0.5911, Accuracy: 77.27 %
-------------------------------
Round [1/4] Average Local Loss: 0.6172, Average Local Accuracy: 74.98 %
-------------------------------
