Current configuration:
{
    "concurrency_flag": true,
    "data_distribution": "iid",
    "dataset": "twitter_dep",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0.0,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedavg",
    "layers": 3,
    "learning_rate": 6e-05,
    "max_length": 512,
    "ml_mode": "ml",
    "model": "bert_small",
    "model_name": "ml_small_seed9_twitter_dep",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/phase-4/final/accs-times/twitter_dep/",
    "mu": 0.5,
    "num_clients": 5,
    "num_epochs": 12,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": "random",
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
seed set:  578832
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 3 layers.

Total parameters count: 28764674
Trainable parameters count: 3416066
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 1982 total sentences. 496 batches of size 4.
Eval Loader: 496 total sentences. 124 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Creating Optimizer --------
-------- Optimizer created --------

-------- Creating Scheduler --------
-------- Scheduler created --------

-------- Training --------
Training with Centralised Machine Learning technology.
Training without differential privacy.
Concurrency flag is set to True, but ml mode is selected. Concurrency flag will be ignored.
Epoch [1/12], Step [100/496], Loss: 0.5209, Accuracy: 75.25 %
Epoch [1/12], Step [200/496], Loss: 0.5046, Accuracy: 74.50 %
Epoch [1/12], Step [300/496], Loss: 0.4900, Accuracy: 75.50 %
Epoch [1/12], Step [400/496], Loss: 0.4868, Accuracy: 75.25 %
Predictions - 1s: 354, 0s: 1628
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [1/12] Loss: 0.4853, Accuracy: 75.48 %
-------------------------------
-------- Validation --------
Predictions - 1s: 94, 0s: 402
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.4063, Validation Accuracy: 81.45 %
-------- Validation finished --------
Updated best model in epoch 1 saved with Validation Loss: 0.4063 and Validation Accuracy: 81.45 %
-------------------------------
Epoch [2/12], Step [100/496], Loss: 0.3861, Accuracy: 81.50 %
Epoch [2/12], Step [200/496], Loss: 0.4146, Accuracy: 79.62 %
Epoch [2/12], Step [300/496], Loss: 0.3922, Accuracy: 81.00 %
Epoch [2/12], Step [400/496], Loss: 0.3912, Accuracy: 81.62 %
Predictions - 1s: 458, 0s: 1524
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [2/12] Loss: 0.3923, Accuracy: 81.63 %
-------------------------------
-------- Validation --------
Predictions - 1s: 176, 0s: 320
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.4586, Validation Accuracy: 75.81 %
-------- Validation finished --------
Epoch [3/12], Step [100/496], Loss: 0.3228, Accuracy: 85.25 %
Epoch [3/12], Step [200/496], Loss: 0.3162, Accuracy: 86.00 %
Epoch [3/12], Step [300/496], Loss: 0.3186, Accuracy: 85.67 %
Epoch [3/12], Step [400/496], Loss: 0.3151, Accuracy: 85.75 %
Predictions - 1s: 521, 0s: 1461
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [3/12] Loss: 0.3200, Accuracy: 85.22 %
-------------------------------
-------- Validation --------
Predictions - 1s: 99, 0s: 397
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.4068, Validation Accuracy: 81.65 %
-------- Validation finished --------
Epoch [4/12], Step [100/496], Loss: 0.2373, Accuracy: 90.75 %
Epoch [4/12], Step [200/496], Loss: 0.2458, Accuracy: 90.00 %
Epoch [4/12], Step [300/496], Loss: 0.2342, Accuracy: 90.17 %
Epoch [4/12], Step [400/496], Loss: 0.2534, Accuracy: 89.38 %
Predictions - 1s: 520, 0s: 1462
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [4/12] Loss: 0.2554, Accuracy: 88.90 %
-------------------------------
-------- Validation --------
Predictions - 1s: 128, 0s: 368
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.4501, Validation Accuracy: 79.44 %
-------- Validation finished --------
Epoch [5/12], Step [100/496], Loss: 0.1670, Accuracy: 93.00 %
Epoch [5/12], Step [200/496], Loss: 0.1839, Accuracy: 91.38 %
Epoch [5/12], Step [300/496], Loss: 0.2067, Accuracy: 90.92 %
Epoch [5/12], Step [400/496], Loss: 0.2038, Accuracy: 91.44 %
Predictions - 1s: 540, 0s: 1442
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [5/12] Loss: 0.1985, Accuracy: 92.03 %
-------------------------------
-------- Validation --------
Predictions - 1s: 109, 0s: 387
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.5188, Validation Accuracy: 79.64 %
-------- Validation finished --------
Epoch [6/12], Step [100/496], Loss: 0.1324, Accuracy: 95.00 %
Epoch [6/12], Step [200/496], Loss: 0.1414, Accuracy: 94.75 %
Epoch [6/12], Step [300/496], Loss: 0.1389, Accuracy: 94.67 %
Epoch [6/12], Step [400/496], Loss: 0.1387, Accuracy: 94.62 %
Predictions - 1s: 549, 0s: 1433
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [6/12] Loss: 0.1376, Accuracy: 94.60 %
-------------------------------
-------- Validation --------
Predictions - 1s: 137, 0s: 359
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.6006, Validation Accuracy: 78.43 %
-------- Validation finished --------
Epoch [7/12], Step [100/496], Loss: 0.0906, Accuracy: 97.00 %
Epoch [7/12], Step [200/496], Loss: 0.1048, Accuracy: 96.38 %
Epoch [7/12], Step [300/496], Loss: 0.1135, Accuracy: 95.42 %
Epoch [7/12], Step [400/496], Loss: 0.1090, Accuracy: 95.69 %
Predictions - 1s: 552, 0s: 1430
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [7/12] Loss: 0.1127, Accuracy: 95.46 %
-------------------------------
-------- Validation --------
Predictions - 1s: 160, 0s: 336
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.7028, Validation Accuracy: 77.02 %
-------- Validation finished --------
Epoch [8/12], Step [100/496], Loss: 0.1058, Accuracy: 95.50 %
Epoch [8/12], Step [200/496], Loss: 0.0883, Accuracy: 96.25 %
Epoch [8/12], Step [300/496], Loss: 0.0865, Accuracy: 96.50 %
Epoch [8/12], Step [400/496], Loss: 0.0860, Accuracy: 96.50 %
Predictions - 1s: 559, 0s: 1423
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [8/12] Loss: 0.0878, Accuracy: 96.32 %
-------------------------------
-------- Validation --------
Predictions - 1s: 116, 0s: 380
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.6697, Validation Accuracy: 79.44 %
-------- Validation finished --------
Epoch [9/12], Step [100/496], Loss: 0.0529, Accuracy: 98.50 %
Epoch [9/12], Step [200/496], Loss: 0.0614, Accuracy: 98.25 %
Epoch [9/12], Step [300/496], Loss: 0.0686, Accuracy: 97.67 %
Epoch [9/12], Step [400/496], Loss: 0.0668, Accuracy: 97.88 %
Predictions - 1s: 557, 0s: 1425
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [9/12] Loss: 0.0611, Accuracy: 98.03 %
-------------------------------
-------- Validation --------
Predictions - 1s: 142, 0s: 354
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.7665, Validation Accuracy: 78.63 %
-------- Validation finished --------
Epoch [10/12], Step [100/496], Loss: 0.0456, Accuracy: 98.25 %
Epoch [10/12], Step [200/496], Loss: 0.0434, Accuracy: 98.50 %
Epoch [10/12], Step [300/496], Loss: 0.0496, Accuracy: 98.17 %
Epoch [10/12], Step [400/496], Loss: 0.0573, Accuracy: 97.81 %
Predictions - 1s: 551, 0s: 1431
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [10/12] Loss: 0.0551, Accuracy: 97.93 %
-------------------------------
-------- Validation --------
Predictions - 1s: 128, 0s: 368
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.7924, Validation Accuracy: 78.23 %
-------- Validation finished --------
Epoch [11/12], Step [100/496], Loss: 0.0387, Accuracy: 99.00 %
Epoch [11/12], Step [200/496], Loss: 0.0390, Accuracy: 99.00 %
Epoch [11/12], Step [300/496], Loss: 0.0426, Accuracy: 98.83 %
Epoch [11/12], Step [400/496], Loss: 0.0391, Accuracy: 99.00 %
Predictions - 1s: 556, 0s: 1426
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [11/12] Loss: 0.0418, Accuracy: 98.89 %
-------------------------------
-------- Validation --------
Predictions - 1s: 142, 0s: 354
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.8115, Validation Accuracy: 78.63 %
-------- Validation finished --------
Epoch [12/12], Step [100/496], Loss: 0.0432, Accuracy: 98.75 %
Epoch [12/12], Step [200/496], Loss: 0.0383, Accuracy: 98.88 %
Epoch [12/12], Step [300/496], Loss: 0.0359, Accuracy: 98.92 %
Epoch [12/12], Step [400/496], Loss: 0.0388, Accuracy: 98.88 %
Predictions - 1s: 557, 0s: 1425
True Labels - 1s: 556, 0s: 1426
-------------------------------
Epoch [12/12] Loss: 0.0383, Accuracy: 98.94 %
-------------------------------
-------- Validation --------
Predictions - 1s: 124, 0s: 372
True Labels - 1s: 118, 0s: 378
Validation Loss: 0.7977, Validation Accuracy: 79.03 %
-------- Validation finished --------
Best model in epoch 1 saved with with Validation Loss: 0.4063 and Validation Accuracy: 81.45 %

-------- Training finished in 1:35 --------

Test flag enabled. Testing the model

-------- Loading best model from model_path --------
Loaded model bert-small from date 20-04-2024 10:04. Trained with Centralised Machine Learning technology.
Epoch 1, lr: 6e-05, optimizer: AdamW
Train accuracy: 75.48 %, Validation accuracy: 81.45 %
-------- Best model loaded --------

-------- Creating Test Dataloader --------
Test Loader: 618 total sentences. 78 batches of size 8.
-------- Test Dataloader created --------

-------- Testing --------
Predictions - 1s: 121, 0s: 497
True Labels - 1s: 168, 0s: 450
NÂº of test samples: 618
Accuracy: 81.72%
Accuracy 2 : 81.72%
Precision: 78.32%
Recall: 72.52%
F1 Score: 74.48%
Classification report:
              precision    recall  f1-score   support

           0     0.8390    0.9267    0.8807       450
           1     0.7273    0.5238    0.6090       168

    accuracy                         0.8172       618
   macro avg     0.7832    0.7252    0.7448       618
weighted avg     0.8087    0.8172    0.8068       618

-------- Testing finished --------
