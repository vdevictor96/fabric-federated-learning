Current configuration:
{
    "concurrency_flag": false,
    "data_distribution": "non_iid",
    "dataset": "mixed_depression",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedprox",
    "layers": 3,
    "learning_rate": 6e-05,
    "max_length": 512,
    "ml_mode": "fl",
    "model": "bert_small",
    "model_name": "fedprox_non_iid_mixed_depression",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/5-fed-data_dist-dataset/mixed_depression",
    "mu": 0.5,
    "num_clients": 5,
    "num_epochs": 3,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": 0,
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 3 layers.

Total parameters count: 28764674
Trainable parameters count: 3416066
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 1806 total sentences. 452 batches of size 4.
Eval Loader: 452 total sentences. 113 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Training --------
Training with Federated Learning technology.
Training without differential privacy.
Federated proximal algorithm selected.
Using mu value of 0.5.
Client 0: Label 0: 36, Label 1: 362
Client 1: Label 0: 36, Label 1: 362
Client 2: Label 0: 325, Label 1: 73
Client 3: Label 0: 325, Label 1: 73
Client 4: Label 0: 141, Label 1: 73

Round 1 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.5080, Accuracy: 88.69 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.4192, Accuracy: 90.95 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.3915, Accuracy: 90.95 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.5105, Accuracy: 88.44 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.4144, Accuracy: 90.95 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.3938, Accuracy: 90.95 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.5413, Accuracy: 81.66 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.5066, Accuracy: 81.91 %
Client 3 of 5: Local Epoch [3/3] Loss: 0.4866, Accuracy: 81.66 %
Client 4 of 5: Local Epoch [1/3] Loss: 0.5452, Accuracy: 81.41 %
Client 4 of 5: Local Epoch [2/3] Loss: 0.5103, Accuracy: 81.66 %
Client 4 of 5: Local Epoch [3/3] Loss: 0.4904, Accuracy: 81.66 %
Client 5 of 5: Local Epoch [1/3] Loss: 0.7200, Accuracy: 66.82 %
Client 5 of 5: Local Epoch [2/3] Loss: 0.6524, Accuracy: 67.76 %
Client 5 of 5: Local Epoch [3/3] Loss: 0.6426, Accuracy: 68.69 %
-------------------------------
Round [1/4] Average Local Loss: 0.4810, Average Local Accuracy: 82.78 %
-------------------------------
-------- Validation --------
Round [1/4] Global Model Validation Loss: 0.6190, Validation Accuracy: 74.56 %
-------- Validation finished --------
Updated best model in round 1 saved with Validation Accuracy: 74.56 %
-------------------------------

Round 2 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.4214, Accuracy: 90.45 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.3798, Accuracy: 90.95 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.3544, Accuracy: 90.95 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.4290, Accuracy: 90.20 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.3693, Accuracy: 90.95 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.3482, Accuracy: 90.95 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.5384, Accuracy: 81.41 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.4769, Accuracy: 83.17 %
