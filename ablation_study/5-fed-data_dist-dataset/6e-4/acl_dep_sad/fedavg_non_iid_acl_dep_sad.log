Current configuration:
{
    "concurrency_flag": false,
    "data_distribution": "non_iid",
    "dataset": "acl_dep_sad",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedavg",
    "layers": 3,
    "learning_rate": 0.0006,
    "max_length": 512,
    "ml_mode": "fl",
    "model": "bert_small",
    "model_name": "fedavg_non_iid_acl_dep_sad",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/5-fed-data_dist-dataset/6e-4/acl_dep_sad",
    "mu": 0.5,
    "num_clients": 5,
    "num_epochs": 3,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": 0,
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 3 layers.

Total parameters count: 28764674
Trainable parameters count: 3416066
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 2085 total sentences. 522 batches of size 4.
Eval Loader: 521 total sentences. 131 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Training --------
Training with Federated Learning technology.
Training without differential privacy.
Federated averaging algorithm selected.
Client 0: Label 0: 41, Label 1: 376
Client 1: Label 0: 41, Label 1: 376
Client 2: Label 0: 376, Label 1: 41
Client 3: Label 0: 376, Label 1: 41
Client 4: Label 0: 379, Label 1: 38

Round 1 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.2634, Accuracy: 90.89 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.1049, Accuracy: 95.68 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.0613, Accuracy: 97.60 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.2010, Accuracy: 91.85 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.0641, Accuracy: 97.84 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.0380, Accuracy: 98.32 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.2767, Accuracy: 91.85 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.1792, Accuracy: 93.05 %
Client 3 of 5: Local Epoch [3/3] Loss: 0.0781, Accuracy: 96.88 %
Client 4 of 5: Local Epoch [1/3] Loss: 0.3097, Accuracy: 88.97 %
Client 4 of 5: Local Epoch [2/3] Loss: 0.1582, Accuracy: 93.76 %
Client 4 of 5: Local Epoch [3/3] Loss: 0.0819, Accuracy: 97.12 %
Client 5 of 5: Local Epoch [1/3] Loss: 0.2996, Accuracy: 91.13 %
Client 5 of 5: Local Epoch [2/3] Loss: 0.1391, Accuracy: 95.92 %
Client 5 of 5: Local Epoch [3/3] Loss: 0.0802, Accuracy: 98.08 %
-------------------------------
Round [1/4] Average Local Loss: 0.0679, Average Local Accuracy: 97.60 %
-------------------------------
-------- Validation --------
Round [1/4] Global Model Validation Loss: 0.2337, Validation Accuracy: 90.60 %
-------- Validation finished --------
Updated best model in round 1 saved with Validation Accuracy: 90.60 %
-------------------------------

Round 2 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.1901, Accuracy: 93.76 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.1243, Accuracy: 95.92 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.0450, Accuracy: 99.04 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.1379, Accuracy: 95.92 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.0946, Accuracy: 96.88 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.0380, Accuracy: 98.56 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.2069, Accuracy: 92.57 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.0961, Accuracy: 96.40 %
Client 3 of 5: Local Epoch [3/3] Loss: 0.0510, Accuracy: 97.84 %
Client 4 of 5: Local Epoch [1/3] Loss: 0.1890, Accuracy: 93.53 %
Client 4 of 5: Local Epoch [2/3] Loss: 0.1545, Accuracy: 93.76 %
Client 4 of 5: Local Epoch [3/3] Loss: 0.1032, Accuracy: 96.88 %
Client 5 of 5: Local Epoch [1/3] Loss: 0.2096, Accuracy: 93.29 %
Client 5 of 5: Local Epoch [2/3] Loss: 0.1176, Accuracy: 97.12 %
Client 5 of 5: Local Epoch [3/3] Loss: 0.0768, Accuracy: 97.60 %
-------------------------------
Round [2/4] Average Local Loss: 0.0628, Average Local Accuracy: 97.99 %
-------------------------------
-------- Validation --------
Round [2/4] Global Model Validation Loss: 0.2563, Validation Accuracy: 90.79 %
-------- Validation finished --------
Updated best model in round 2 saved with Validation Accuracy: 90.79 %
-------------------------------

Round 3 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.1680, Accuracy: 94.72 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.0885, Accuracy: 96.64 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.0261, Accuracy: 99.28 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.1693, Accuracy: 96.16 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.0703, Accuracy: 97.60 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.0275, Accuracy: 99.28 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.1473, Accuracy: 94.96 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.0917, Accuracy: 97.12 %
Client 3 of 5: Local Epoch [3/3] Loss: 0.0399, Accuracy: 99.28 %
Client 4 of 5: Local Epoch [1/3] Loss: 0.1667, Accuracy: 92.81 %
Client 4 of 5: Local Epoch [2/3] Loss: 0.0919, Accuracy: 97.36 %
Client 4 of 5: Local Epoch [3/3] Loss: 0.0465, Accuracy: 98.80 %
Client 5 of 5: Local Epoch [1/3] Loss: 0.1919, Accuracy: 94.00 %
Client 5 of 5: Local Epoch [2/3] Loss: 0.1071, Accuracy: 96.64 %
Client 5 of 5: Local Epoch [3/3] Loss: 0.0565, Accuracy: 98.80 %
-------------------------------
Round [3/4] Average Local Loss: 0.0393, Average Local Accuracy: 99.09 %
-------------------------------
-------- Validation --------
Round [3/4] Global Model Validation Loss: 0.2895, Validation Accuracy: 90.02 %
-------- Validation finished --------

Round 4 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.1307, Accuracy: 95.92 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.0958, Accuracy: 96.64 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.0367, Accuracy: 98.56 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.1420, Accuracy: 94.96 %
Client 2 of 5: Local Epoch [2/3] Loss: 0.0444, Accuracy: 99.04 %
Client 2 of 5: Local Epoch [3/3] Loss: 0.0537, Accuracy: 98.08 %
Client 3 of 5: Local Epoch [1/3] Loss: 0.1299, Accuracy: 96.40 %
Client 3 of 5: Local Epoch [2/3] Loss: 0.0712, Accuracy: 97.36 %
Client 3 of 5: Local Epoch [3/3] Loss: 0.0394, Accuracy: 98.80 %
Client 4 of 5: Local Epoch [1/3] Loss: 0.1786, Accuracy: 94.72 %
Client 4 of 5: Local Epoch [2/3] Loss: 0.0950, Accuracy: 96.88 %
Client 4 of 5: Local Epoch [3/3] Loss: 0.0589, Accuracy: 98.32 %
Client 5 of 5: Local Epoch [1/3] Loss: 0.2155, Accuracy: 94.48 %
Client 5 of 5: Local Epoch [2/3] Loss: 0.0818, Accuracy: 97.84 %
Client 5 of 5: Local Epoch [3/3] Loss: 0.0539, Accuracy: 98.80 %
-------------------------------
Round [4/4] Average Local Loss: 0.0485, Average Local Accuracy: 98.51 %
-------------------------------
-------- Validation --------
Round [4/4] Global Model Validation Loss: 0.2886, Validation Accuracy: 91.36 %
-------- Validation finished --------
Updated best model in round 4 saved with Validation Accuracy: 91.36 %
-------------------------------
Best model in round 4 saved with Validation Accuracy: 91.36 %

-------- Training finished in 17:31 --------

Test flag enabled. Testing the model

-------- Loading best model from model_path --------
Loaded model bert-small from date 06-02-2024 15:48. Trained with Federated Learning technology.
Round 4, lr: 0.0006, optimizer: AdamW
Average train accuracy: 98.51 %, Validation accuracy: 91.36 %
-------- Best model loaded --------

-------- Creating Test Dataloader --------
Test Loader: 650 total sentences. 82 batches of size 8.
-------- Test Dataloader created --------

-------- Testing --------
NÂº of test samples: 650
Accuracy: 92.46%
Precision: 92.49%
Recall: 91.92%
F1 Score: 92.17%
-------- Testing finished --------
