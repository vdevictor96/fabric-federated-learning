5. data dist effect on several dataset with differnt fedAlg (fl-bcf / 2xfederation algorithm / 2xdata distribution / 1xmodel (bert_small) / 4xdataset (16 runs)

twitter_dep

fedavg_iid - 81
fedavg_noniid - 72.82
fedprox_iid - 75.57
fedprox_noniid - 72.82

acl_dep_sad

fedavg_iid - 91.4
fedavg_noniid - 91
fedprox_iid - 86
fedprox_noniid - 84

dreaddit

fedavg_iid - 75
fedavg_iid (8 rounds) - 76
fedavg_noniid - 74.3
fedavg_noniid (0.05) - 72
fedavg_noniid (8 rounds) - 75
fedprox_iid - 69.5
fedprox_iid (8 rounds) - 69.5
fedprox_noniid - 68
fedprox_noniid (0.05) - 65.7
fedprox_noniid (8 rounds) - 70.6

mixed_depression

fedavg_iid - 88 - (6e-4) 89
fedavg_noniid - 87 - (6e-4) 89
fedprox_iid - 77 - (6e-4) 87.6
fedprox_noniid - 76 - (6e-4) 78.5

Conclusions:
- Fedprox lowers accuracy in all cases, does not work
- Non_iid 0.1 has little effect -1/2% on accuracy (except twitter_dep)
- Non_iid 0.05 has little effect -3/4% on accuracy
Arguments
- Finetuning with non_iid does not have a big effect on accuracy so there is no need for fedprox algorithm. 
- Fedprox reduces the negative effect of non_iid (already low in fedavg) but is still worse than fedavg since the proximal term hinders the finetuning of the last layers


learning rate 6e-4

twitter_dep
fedavg_iid - 81
fedavg_noniid - 80.5
fedprox_iid - 80.5
fedprox_noniid - 72.82

acl_dep_sad
fedavg_iid - 92.7
fedavg_noniid - 92.5
fedprox_iid - 83
fedprox_noniid - 83

dreaddit
fedavg_iid - 72
fedavg_noniid - 73
fedprox_iid - 74
fedprox_noniid - 70

mixed_depression
fedavg_iid - 88 - (6e-4) 89
fedavg_noniid - 87 - (6e-4) 89
fedprox_iid - 77 - (6e-4) 87.6
fedprox_noniid - 76 - (6e-4) 78.5

Conclusions:
- higher learning rate improves fedprox for iid data but not for non_iid
- non_iid does not affect in fedavg
- non_iid worse in fedprox 
- fedprox does not work properly, it is worse 