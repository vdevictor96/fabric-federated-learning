Current configuration:
{
    "concurrency_flag": false,
    "data_distribution": "iid",
    "dataset": "mixed_depression",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0.5,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedavg",
    "layers": 3,
    "learning_rate": 0.006,
    "max_length": 512,
    "ml_mode": "fl",
    "model": "bert_small",
    "model_name": "fl_0.5_c10_mixed_depression",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/phase-3/1-ml_mode-dataset-dp/mixed_depression",
    "mu": 0.5,
    "num_clients": 10,
    "num_epochs": 3,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": 0,
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 3 layers.

Total parameters count: 28764674
Trainable parameters count: 3416066
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 1806 total sentences. 452 batches of size 4.
Eval Loader: 452 total sentences. 113 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Training --------
Training with Federated Learning technology.
Training with differential privacy.
Federated averaging algorithm selected.
Client 0: Label 0: 88, Label 1: 92
Client 1: Label 0: 88, Label 1: 92
Client 2: Label 0: 83, Label 1: 97
Client 3: Label 0: 87, Label 1: 93
Client 4: Label 0: 84, Label 1: 96
Client 5: Label 0: 82, Label 1: 98
Client 6: Label 0: 89, Label 1: 91
Client 7: Label 0: 81, Label 1: 99
Client 8: Label 0: 89, Label 1: 91
Client 9: Label 0: 92, Label 1: 94

Round 1 of 4
-------------------------------
max_grad_norm:  1.5
Predictions - 1s: 2, 0s: 178
True Labels - 1s: 92, 0s: 88
Client 1 of 10: Local Epoch [1/3] Loss: 0.8706, Accuracy: 47.78 %, Epsilon: 0.49, Delta: 0.0030
Predictions - 1s: 30, 0s: 150
True Labels - 1s: 92, 0s: 88
Client 1 of 10: Local Epoch [2/3] Loss: 0.7614, Accuracy: 54.44 %, Epsilon: 0.71, Delta: 0.0030
Predictions - 1s: 56, 0s: 124
True Labels - 1s: 92, 0s: 88
Client 1 of 10: Local Epoch [3/3] Loss: 0.7398, Accuracy: 53.33 %, Epsilon: 0.88, Delta: 0.0030
max_grad_norm:  1.5
Predictions - 1s: 28, 0s: 152
True Labels - 1s: 92, 0s: 88
Client 2 of 10: Local Epoch [1/3] Loss: 0.7086, Accuracy: 53.33 %, Epsilon: 0.49, Delta: 0.0030
Predictions - 1s: 145, 0s: 35
True Labels - 1s: 92, 0s: 88
Client 2 of 10: Local Epoch [2/3] Loss: 0.8230, Accuracy: 53.89 %, Epsilon: 0.71, Delta: 0.0030
Predictions - 1s: 161, 0s: 19
True Labels - 1s: 92, 0s: 88
Client 2 of 10: Local Epoch [3/3] Loss: 0.8136, Accuracy: 55.00 %, Epsilon: 0.88, Delta: 0.0030
max_grad_norm:  1.5
Predictions - 1s: 154, 0s: 26
True Labels - 1s: 97, 0s: 83
Client 3 of 10: Local Epoch [1/3] Loss: 0.8640, Accuracy: 53.89 %, Epsilon: 0.49, Delta: 0.0030
Predictions - 1s: 179, 0s: 1
True Labels - 1s: 97, 0s: 83
Client 3 of 10: Local Epoch [2/3] Loss: 1.0202, Accuracy: 54.44 %, Epsilon: 0.71, Delta: 0.0030
Predictions - 1s: 178, 0s: 2
True Labels - 1s: 97, 0s: 83
Client 3 of 10: Local Epoch [3/3] Loss: 0.9560, Accuracy: 53.89 %, Epsilon: 0.88, Delta: 0.0030
max_grad_norm:  1.5
Predictions - 1s: 28, 0s: 152
True Labels - 1s: 93, 0s: 87
Client 4 of 10: Local Epoch [1/3] Loss: 0.7080, Accuracy: 56.11 %, Epsilon: 0.49, Delta: 0.0030
