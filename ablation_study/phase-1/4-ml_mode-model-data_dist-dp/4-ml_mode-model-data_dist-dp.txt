4. Table results on dp (2xml-bcfl / 2xbert_models / 1.5x data distribution / 4x dp_epsilon) 24 runs (acl_dep_sad, the highest in acc)

acl_dep_sad

fl_bert_small_iid_0 - 92.5
fl_bert_small_iid_0.5 - 60
fl_bert_small_iid 5 - 59.9
fl_bert_small_iid 15 - 59.7

(broken non_iid)
fl_bert_small_non_iid_0 - 93.4
fl_bert_small_non_iid_0.5 - 59.7
fl_bert_small_non_iid_5 - 59.9
fl_bert_small_non_iid_15 - 59.5

(fixed non_iid)
fl_bert_small_non_iid_0 - 93.2 (0.05 ratio) 91 (0 ratio) 89
fl_bert_small_non_iid_0.5 - ? (0.05 ratio) 59.8
fl_bert_small_non_iid_5 - ? (0.05 ratio) 59.9
fl_bert_small_non_iid_15 - ? (0.05 ratio) 

fl_bert_tiny_iid_0 - 89.4
fl_bert_tiny_iid_0.5 - 58.8
fl_bert_tiny_iid_5 - 58.8
fl_bert_tiny_iid_15 - 58.8

fl_bert_tiny_non_iid_0 - 89.5
fl_bert_tiny_non_iid_0.5 - 58.8
fl_bert_tiny_non_iid_5 - 58.8
fl_bert_tiny_non_iid_15 - 58.8

ml_bert_small_0 - 94
ml_bert_small_0.5 - 59.7
ml_bert_small_5 - 58.8
ml_bert_small_15 - 58.8
ml_bert_tiny_0 - 94
ml_bert_tiny_0.5 - 58.8
ml_bert_tiny_5 - 58.8
ml_bert_tiny_15 - 58.8



Conclusions:
- DP lowers a lot the accuracy. It affects a lot more the recall than the precision (51% vs 80%) (invalid in experiment 6)
??!! - Non-iid data has no effect in fl - It was broken - Fixed
    - Fixed non_iid with ratio 0.05 yields just -2%, has little effect. (better with fedprox?)
    - Tried with another dataset: twitter_dep (experiment 4b) It does ha effect -8% (invalid in experiment 5 with increase on lr)

??!!- Different levels of DP does not change the result.
    - Tried version of DP with noise multiplier:

??!! Each first epoch of each round does not improve upon the previos and it has really low value
    - Tried with another dataset: twitter_dep (experiment 4b) It does not happen


4b. Check if non_iid affects results to other datasets
Table results on dp (2xml-bcfl / 2xbert_models / 1.5x data distribution / 4x dp_epsilon) 24 runs (twitter_dep)

twitter_dep

fl_bert_small_iid_0 - 80.6
fl_bert_small_iid_0.5 - 72.8
fl_bert_small_iid 5 - 72.8
fl_bert_small_iid 15 - 72.8

non_iid 0.1 ratio
fl_bert_small_non_iid_0 - 72.8
fl_bert_small_non_iid_0.5 - 72.8
fl_bert_small_non_iid_5 - 72.8
fl_bert_small_non_iid_15 - 72.8

ml_bert_small_0 - 82
ml_bert_small_0.5 - 
ml_bert_small_5 - 72.8
ml_bert_small_15 - 72.8

Conclusions
- Non_iid has lowering effect on accuracy 
- In iid and dp first epoch in each round improves upon the previous

- ??!! DP and non_iid Average local accuracy gets worse in each round (does not happen in DP-iid and no_DP-non_iid)

twitter_dep 3 layers

fl_bert_small_iid_0 - 81
fl_bert_small_iid_0.5 - 72.82
fl_bert_small_iid 15 - 72.82

non_iid 0.1 ratio
fl_bert_small_non_iid_0 - 72.82
fl_bert_small_non_iid_0.5 - 72.82


ml_bert_small_0 - 82
ml_bert_small_0.5 - 72.82


- Global conclusions:
- Non_iid has lowering effect on accuracy (but not for all datasets)
- Non_iid has better train acc (close to 100%) but worse test acc (clients overfit)
??- DP same acc for all models and values of epsilon. Broken?
!- Set the layers to 3 to avoid bcfl to fail (too many pings)
!- Test DP and non_iid separately



4b. Run and compare results and times:
- bcfl_bert_small_iid_0_config.json (fl 8:23 vs bcfl 9:08)
- bcfl_bert_small_iid_0.5_config.json (fl 10:01 vs bcfl 10:01)

Conclusions:
- In a simple architecture with two peer nodes, one orderer and Raft consensus in the same local network there is not overhead in communication.
    This shows that there is not software overhead with the gateway.
    We can expect more network overhead in a decentralised network.
    Overhead for the consensus mechanisms.



