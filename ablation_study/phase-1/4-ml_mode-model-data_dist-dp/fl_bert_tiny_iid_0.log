Current configuration:
{
    "concurrency_flag": false,
    "data_distribution": "iid",
    "dataset": "acl_dep_sad",
    "device": "cuda",
    "dp_delta": 0.003,
    "dp_epsilon": 0,
    "eval_batch_size": 4,
    "eval_flag": true,
    "eval_size": 0.2,
    "fed_alg": "fedavg",
    "layers": 4,
    "learning_rate": 6e-05,
    "max_length": 512,
    "ml_mode": "fl",
    "model": "bert_tiny",
    "model_name": "fl_bert_tiny_iid_0",
    "models_path": "/local/vpaloma/fabric-federated-learning/ablation_study/4-ml_mode-model-data_dist-dp/",
    "mu": 0.5,
    "num_clients": 5,
    "num_epochs": 3,
    "num_rounds": 4,
    "optimizer": "AdamW",
    "progress_bar_flag": false,
    "save_model": true,
    "scheduler": "linear",
    "scheduler_warmup_steps": 0,
    "seed": 0,
    "test_flag": true,
    "train_batch_size": 4,
    "train_size": 0.8
}
-------- Configuration loaded --------

-------- Setting device --------
cuda device selected and available.
-------- Device set --------

-------- Setting seed --------
-------- Seed set --------

-------- Creating Model --------
-------- Model created --------

-------- Setting Trainable Layers --------
Training the last 4 layers.

Total parameters count: 4386178
Trainable parameters count: 413314
-------- Trainable Layers set --------

-------- Creating Tokenizer --------
-------- Tokenizer created --------

-------- Creating Train and Eval Dataloaders --------
Train Loader: 2085 total sentences. 522 batches of size 4.
Eval Loader: 521 total sentences. 131 batches of size 4.
-------- Train and Eval Dataloaders created --------

-------- Training --------
Training with Federated Learning technology.
Training without differential privacy.
Federated averaging algorithm selected.

Round 1 of 4
-------------------------------
Client 1 of 5: Local Epoch [1/3] Loss: 0.6546, Accuracy: 62.35 %
Client 1 of 5: Local Epoch [2/3] Loss: 0.5477, Accuracy: 73.86 %
Client 1 of 5: Local Epoch [3/3] Loss: 0.4740, Accuracy: 82.25 %
Client 2 of 5: Local Epoch [1/3] Loss: 0.6489, Accuracy: 63.55 %
