1. Decide lr depending on trainable layers and datasets (on bert_tiny)
Conclusion:
  - 4 layers average acc: 81%
  - 2 layers average acc: 76%
  - 4 layers - lr 6e-5
  - 2 layers - lr 9e-5

1b. Check if the same lr applies to bert_medium
Conclusion:
  - 4 layers avg acc: 82%
  - 2 layers avg acc: 81.5%

  - 4 layers - lr 6e-5
  - 2 layers - lr 9e-5


1. Log:
BERT_TINY WITH TWITTER_DEP:


Maximum trainable layers (4):

- Best lr is 6e-5 (better than 3e-5 and 9e-5) 



For just 2 layers:

- Optimal lr: 9e-5


Conclusion:

- 4 layers - lr 6e-5
- 2 layers - lr 9e-5

BERT_TINY WITH ACL_DEP_SAD:


- 4 layers - lr 6e-5/9e-5
- 2 layers - lr 9e-5


BERT_TINY WITH MIXED_DEPRESSION:


- 4 layers - lr 6e-5
- 2 layers - lr 9e-5