---- bert_tiny - twitter_dep ----
[I 2024-01-31 15:49:32,694] Trial 1 finished with value: 0.7963709677419355 and parameters: {'learning_rate': 3.191759474369995e-05, 'num_train_epochs': 4, 'seed': 8, 'per_device_train_batch_size': 4}. Best is trial 1 with value: 0.7963709677419355.

[I 2024-01-31 16:00:57,359] Trial 2 finished with value: 0.7923387096774194 and parameters: {'learning_rate': 2.612484308800896e-05, 'num_train_epochs': 5, 'seed': 24, 'per_device_train_batch_size': 8}. Best is trial 2 with value: 0.7923387096774194.

[I 2024-01-31 16:17:44,743] Trial 13 finished with value: 0.7923387096774194 and parameters: {'learning_rate': 9.889081463998571e-06, 'num_train_epochs': 9, 'per_device_train_batch_size': 4}. Best is trial 13 with value: 0.7923387096774194.

[I 2024-01-31 16:46:19,056] Trial 14 finished with value: 0.7883064516129032 and parameters: {'learning_rate': 7.56322241935493e-06, 'num_train_epochs': 10, 'per_device_train_batch_size': 4, 'weight_decay': 0.19003530779215305, 'warmup_steps': 410}. Best is trial 14 with value: 0.7883064516129032.

[I 2024-01-31 16:59:06,944] Trial 13 finished with value: 0.7762096774193549 and parameters: {'learning_rate': 6.480578395439071e-06, 'num_train_epochs': 10, 'per_device_train_batch_size': 4}. Best is trial 13 with value: 0.7762096774193549.

[I 2024-02-01 07:49:57,585] Trial 1 finished with value: 0.7983870967741935 and parameters: {'learning_rate': 1.4887946879777368e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 4}. Best is trial 1 with value: 0.7983870967741935.


Conclusions:
- High learning rates approx 1e-5 - 3e-5
- 5 epochs are sufficient for these high leaning rates
- Batch_size: 4 or 8
- No weight_decay nor warmup_steps

---- bert_tiny - dreaddit ----

[I 2024-01-31 17:15:12,257] Trial 10 finished with value: 0.772887323943662 and parameters: {'learning_rate': 1.841083296350536e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 4}. Best is trial 10 with value: 0.772887323943662.

[I 2024-01-31 17:35:50,507] Trial 11 finished with value: 0.7711267605633803 and parameters: {'learning_rate': 3.613707768639147e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 4}. Best is trial 11 with value: 0.7711267605633803.

(trying without batches and with higher epochs 10-15)

[I 2024-02-01 06:34:43,495] Trial 11 finished with value: 0.7834507042253521 and parameters: {'learning_rate': 1.6943481793361124e-05, 'num_train_epochs': 13, 'per_device_train_batch_size': 4}. Best is trial 11 with value: 0.7834507042253521.


Conclusions:
- High learning rates approx 1e-5 - 3e-5
- More epochs: 10-15 for this dataset


---- bert_tiny - mixed_depression ----

[I 2024-02-01 07:01:50,329] Trial 11 finished with value: 0.8915929203539823 and parameters: {'learning_rate': 2.7209947788958356e-05, 'num_train_epochs': 13, 'per_device_train_batch_size': 4}. Best is trial 11 with value: 0.8915929203539823.

(trying just with high epochs 10-15 epochs range to get a more exact amount of epochs )

[I 2024-02-01 07:27:50,612] Trial 12 finished with value: 0.8893805309734514 and parameters: {'learning_rate': 2.8657683233126037e-05, 'num_train_epochs': 13, 'per_device_train_batch_size': 8}. Best is trial 12 with value: 0.8893805309734514.


Conclusions:
- High learning rates approx 3e-5
- More epochs: 10-15 for this dataset (13)


---- bert_tiny - acl_dep_dataset ----

[I 2024-02-01 07:36:07,326] Trial 2 finished with value: 0.9424184261036468 and parameters: {'learning_rate': 6.340642483795974e-05, 'num_train_epochs': 15, 'per_device_train_batch_size': 4}. Best is trial 2 with value: 0.9424184261036468.



