{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One round federated learning simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available and set the default tensor type to CUDA\n",
    "print('Using device: %s' % device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config variables for models and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.train import train\n",
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, weights_zero_init,  weights_init, update_lr\n",
    "from client.aggregators import federated_aggregate\n",
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "model_path = 'client/models/'\n",
    "train_flag = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = 'client/data/'\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(\n",
    "    root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(\n",
    "    root, batch_size, num_training, num_validation, device)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two local models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "\n",
    "model1 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model1.to(device)\n",
    "print(model1)\n",
    "\n",
    "model2 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model2.to(device)\n",
    "print(model2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first local model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/245], Loss: 2.3026\n",
      "Epoch [1/1], Step [2/245], Loss: 2.3012\n",
      "Epoch [1/1], Step [3/245], Loss: 2.2948\n",
      "Epoch [1/1], Step [4/245], Loss: 2.2914\n",
      "Epoch [1/1], Step [5/245], Loss: 2.2693\n",
      "Epoch [1/1], Step [6/245], Loss: 2.2452\n",
      "Epoch [1/1], Step [7/245], Loss: 2.2338\n",
      "Epoch [1/1], Step [8/245], Loss: 2.2168\n",
      "Epoch [1/1], Step [9/245], Loss: 2.1777\n",
      "Epoch [1/1], Step [10/245], Loss: 2.1701\n",
      "Epoch [1/1], Step [11/245], Loss: 2.1618\n",
      "Epoch [1/1], Step [12/245], Loss: 2.1476\n",
      "Epoch [1/1], Step [13/245], Loss: 2.0951\n",
      "Epoch [1/1], Step [14/245], Loss: 2.1586\n",
      "Epoch [1/1], Step [15/245], Loss: 2.0220\n",
      "Epoch [1/1], Step [16/245], Loss: 2.0514\n",
      "Epoch [1/1], Step [17/245], Loss: 2.1096\n",
      "Epoch [1/1], Step [18/245], Loss: 2.0108\n",
      "Epoch [1/1], Step [19/245], Loss: 2.0342\n",
      "Epoch [1/1], Step [20/245], Loss: 2.0621\n",
      "Epoch [1/1], Step [21/245], Loss: 2.0065\n",
      "Epoch [1/1], Step [22/245], Loss: 2.0699\n",
      "Epoch [1/1], Step [23/245], Loss: 2.0451\n",
      "Epoch [1/1], Step [24/245], Loss: 1.9981\n",
      "Epoch [1/1], Step [25/245], Loss: 2.0046\n",
      "Epoch [1/1], Step [26/245], Loss: 1.9130\n",
      "Epoch [1/1], Step [27/245], Loss: 2.0240\n",
      "Epoch [1/1], Step [28/245], Loss: 1.9895\n",
      "Epoch [1/1], Step [29/245], Loss: 1.9846\n",
      "Epoch [1/1], Step [30/245], Loss: 1.8946\n",
      "Epoch [1/1], Step [31/245], Loss: 1.9344\n",
      "Epoch [1/1], Step [32/245], Loss: 1.8797\n",
      "Epoch [1/1], Step [33/245], Loss: 1.9620\n",
      "Epoch [1/1], Step [34/245], Loss: 1.9987\n",
      "Epoch [1/1], Step [35/245], Loss: 1.9208\n",
      "Epoch [1/1], Step [36/245], Loss: 1.9554\n",
      "Epoch [1/1], Step [37/245], Loss: 1.9363\n",
      "Epoch [1/1], Step [38/245], Loss: 1.8858\n",
      "Epoch [1/1], Step [39/245], Loss: 1.8949\n",
      "Epoch [1/1], Step [40/245], Loss: 1.8073\n",
      "Epoch [1/1], Step [41/245], Loss: 1.9835\n",
      "Epoch [1/1], Step [42/245], Loss: 1.8917\n",
      "Epoch [1/1], Step [43/245], Loss: 1.9108\n",
      "Epoch [1/1], Step [44/245], Loss: 1.9729\n",
      "Epoch [1/1], Step [45/245], Loss: 1.8507\n",
      "Epoch [1/1], Step [46/245], Loss: 1.7993\n",
      "Epoch [1/1], Step [47/245], Loss: 1.8832\n",
      "Epoch [1/1], Step [48/245], Loss: 1.7877\n",
      "Epoch [1/1], Step [49/245], Loss: 2.0207\n",
      "Epoch [1/1], Step [50/245], Loss: 1.7378\n",
      "Epoch [1/1], Step [51/245], Loss: 1.8514\n",
      "Epoch [1/1], Step [52/245], Loss: 1.8219\n",
      "Epoch [1/1], Step [53/245], Loss: 1.8038\n",
      "Epoch [1/1], Step [54/245], Loss: 1.7742\n",
      "Epoch [1/1], Step [55/245], Loss: 1.7552\n",
      "Epoch [1/1], Step [56/245], Loss: 1.7576\n",
      "Epoch [1/1], Step [57/245], Loss: 1.8021\n",
      "Epoch [1/1], Step [58/245], Loss: 1.8188\n",
      "Epoch [1/1], Step [59/245], Loss: 1.7351\n",
      "Epoch [1/1], Step [60/245], Loss: 1.7561\n",
      "Epoch [1/1], Step [61/245], Loss: 1.7280\n",
      "Epoch [1/1], Step [62/245], Loss: 1.8313\n",
      "Epoch [1/1], Step [63/245], Loss: 1.7074\n",
      "Epoch [1/1], Step [64/245], Loss: 1.8708\n",
      "Epoch [1/1], Step [65/245], Loss: 1.9691\n",
      "Epoch [1/1], Step [66/245], Loss: 1.7851\n",
      "Epoch [1/1], Step [67/245], Loss: 1.7736\n",
      "Epoch [1/1], Step [68/245], Loss: 1.8278\n",
      "Epoch [1/1], Step [69/245], Loss: 1.8056\n",
      "Epoch [1/1], Step [70/245], Loss: 1.8636\n",
      "Epoch [1/1], Step [71/245], Loss: 1.7705\n",
      "Epoch [1/1], Step [72/245], Loss: 1.7686\n",
      "Epoch [1/1], Step [73/245], Loss: 1.7740\n",
      "Epoch [1/1], Step [74/245], Loss: 1.8209\n",
      "Epoch [1/1], Step [75/245], Loss: 1.8429\n",
      "Epoch [1/1], Step [76/245], Loss: 1.7286\n",
      "Epoch [1/1], Step [77/245], Loss: 1.8541\n",
      "Epoch [1/1], Step [78/245], Loss: 1.6708\n",
      "Epoch [1/1], Step [79/245], Loss: 1.8012\n",
      "Epoch [1/1], Step [80/245], Loss: 1.8615\n",
      "Epoch [1/1], Step [81/245], Loss: 1.8396\n",
      "Epoch [1/1], Step [82/245], Loss: 1.8493\n",
      "Epoch [1/1], Step [83/245], Loss: 1.7706\n",
      "Epoch [1/1], Step [84/245], Loss: 1.6253\n",
      "Epoch [1/1], Step [85/245], Loss: 1.7528\n",
      "Epoch [1/1], Step [86/245], Loss: 1.8289\n",
      "Epoch [1/1], Step [87/245], Loss: 1.7550\n",
      "Epoch [1/1], Step [88/245], Loss: 1.7694\n",
      "Epoch [1/1], Step [89/245], Loss: 1.8150\n",
      "Epoch [1/1], Step [90/245], Loss: 1.8183\n",
      "Epoch [1/1], Step [91/245], Loss: 1.7161\n",
      "Epoch [1/1], Step [92/245], Loss: 1.7112\n",
      "Epoch [1/1], Step [93/245], Loss: 1.7291\n",
      "Epoch [1/1], Step [94/245], Loss: 1.7625\n",
      "Epoch [1/1], Step [95/245], Loss: 1.7077\n",
      "Epoch [1/1], Step [96/245], Loss: 1.7787\n",
      "Epoch [1/1], Step [97/245], Loss: 1.7073\n",
      "Epoch [1/1], Step [98/245], Loss: 1.7356\n",
      "Epoch [1/1], Step [99/245], Loss: 1.7195\n",
      "Epoch [1/1], Step [100/245], Loss: 1.7408\n",
      "Epoch [1/1], Step [101/245], Loss: 1.7590\n",
      "Epoch [1/1], Step [102/245], Loss: 1.7264\n",
      "Epoch [1/1], Step [103/245], Loss: 1.6714\n",
      "Epoch [1/1], Step [104/245], Loss: 1.7745\n",
      "Epoch [1/1], Step [105/245], Loss: 1.7973\n",
      "Epoch [1/1], Step [106/245], Loss: 1.8419\n",
      "Epoch [1/1], Step [107/245], Loss: 1.8296\n",
      "Epoch [1/1], Step [108/245], Loss: 1.7160\n",
      "Epoch [1/1], Step [109/245], Loss: 1.7836\n",
      "Epoch [1/1], Step [110/245], Loss: 1.7970\n",
      "Epoch [1/1], Step [111/245], Loss: 1.7151\n",
      "Epoch [1/1], Step [112/245], Loss: 1.6800\n",
      "Epoch [1/1], Step [113/245], Loss: 1.7841\n",
      "Epoch [1/1], Step [114/245], Loss: 1.6305\n",
      "Epoch [1/1], Step [115/245], Loss: 1.6178\n",
      "Epoch [1/1], Step [116/245], Loss: 1.8057\n",
      "Epoch [1/1], Step [117/245], Loss: 1.7476\n",
      "Epoch [1/1], Step [118/245], Loss: 1.6783\n",
      "Epoch [1/1], Step [119/245], Loss: 1.7517\n",
      "Epoch [1/1], Step [120/245], Loss: 1.7276\n",
      "Epoch [1/1], Step [121/245], Loss: 1.7016\n",
      "Epoch [1/1], Step [122/245], Loss: 1.7611\n",
      "Epoch [1/1], Step [123/245], Loss: 1.7106\n",
      "Epoch [1/1], Step [124/245], Loss: 1.8513\n",
      "Epoch [1/1], Step [125/245], Loss: 1.8159\n",
      "Epoch [1/1], Step [126/245], Loss: 1.7926\n",
      "Epoch [1/1], Step [127/245], Loss: 1.7125\n",
      "Epoch [1/1], Step [128/245], Loss: 1.7667\n",
      "Epoch [1/1], Step [129/245], Loss: 1.8025\n",
      "Epoch [1/1], Step [130/245], Loss: 1.6257\n",
      "Epoch [1/1], Step [131/245], Loss: 1.7831\n",
      "Epoch [1/1], Step [132/245], Loss: 1.7764\n",
      "Epoch [1/1], Step [133/245], Loss: 1.6408\n",
      "Epoch [1/1], Step [134/245], Loss: 1.7854\n",
      "Epoch [1/1], Step [135/245], Loss: 1.6906\n",
      "Epoch [1/1], Step [136/245], Loss: 1.7074\n",
      "Epoch [1/1], Step [137/245], Loss: 1.7970\n",
      "Epoch [1/1], Step [138/245], Loss: 1.6725\n",
      "Epoch [1/1], Step [139/245], Loss: 1.7315\n",
      "Epoch [1/1], Step [140/245], Loss: 1.5657\n",
      "Epoch [1/1], Step [141/245], Loss: 1.7446\n",
      "Epoch [1/1], Step [142/245], Loss: 1.7614\n",
      "Epoch [1/1], Step [143/245], Loss: 1.7300\n",
      "Epoch [1/1], Step [144/245], Loss: 1.6116\n",
      "Epoch [1/1], Step [145/245], Loss: 1.5873\n",
      "Epoch [1/1], Step [146/245], Loss: 1.6068\n",
      "Epoch [1/1], Step [147/245], Loss: 1.6419\n",
      "Epoch [1/1], Step [148/245], Loss: 1.6533\n",
      "Epoch [1/1], Step [149/245], Loss: 1.6732\n",
      "Epoch [1/1], Step [150/245], Loss: 1.6632\n",
      "Epoch [1/1], Step [151/245], Loss: 1.6954\n",
      "Epoch [1/1], Step [152/245], Loss: 1.5640\n",
      "Epoch [1/1], Step [153/245], Loss: 1.7188\n",
      "Epoch [1/1], Step [154/245], Loss: 1.6388\n",
      "Epoch [1/1], Step [155/245], Loss: 1.8034\n",
      "Epoch [1/1], Step [156/245], Loss: 1.6753\n",
      "Epoch [1/1], Step [157/245], Loss: 1.7476\n",
      "Epoch [1/1], Step [158/245], Loss: 1.6841\n",
      "Epoch [1/1], Step [159/245], Loss: 1.7266\n",
      "Epoch [1/1], Step [160/245], Loss: 1.7308\n",
      "Epoch [1/1], Step [161/245], Loss: 1.7102\n",
      "Epoch [1/1], Step [162/245], Loss: 1.6623\n",
      "Epoch [1/1], Step [163/245], Loss: 1.7318\n",
      "Epoch [1/1], Step [164/245], Loss: 1.6546\n",
      "Epoch [1/1], Step [165/245], Loss: 1.7650\n",
      "Epoch [1/1], Step [166/245], Loss: 1.5398\n",
      "Epoch [1/1], Step [167/245], Loss: 1.6474\n",
      "Epoch [1/1], Step [168/245], Loss: 1.7586\n",
      "Epoch [1/1], Step [169/245], Loss: 1.5185\n",
      "Epoch [1/1], Step [170/245], Loss: 1.7085\n",
      "Epoch [1/1], Step [171/245], Loss: 1.7022\n",
      "Epoch [1/1], Step [172/245], Loss: 1.7225\n",
      "Epoch [1/1], Step [173/245], Loss: 1.7857\n",
      "Epoch [1/1], Step [174/245], Loss: 1.5992\n",
      "Epoch [1/1], Step [175/245], Loss: 1.7797\n",
      "Epoch [1/1], Step [176/245], Loss: 1.6069\n",
      "Epoch [1/1], Step [177/245], Loss: 1.6268\n",
      "Epoch [1/1], Step [178/245], Loss: 1.6602\n",
      "Epoch [1/1], Step [179/245], Loss: 1.7134\n",
      "Epoch [1/1], Step [180/245], Loss: 1.7421\n",
      "Epoch [1/1], Step [181/245], Loss: 1.6751\n",
      "Epoch [1/1], Step [182/245], Loss: 1.6359\n",
      "Epoch [1/1], Step [183/245], Loss: 1.7343\n",
      "Epoch [1/1], Step [184/245], Loss: 1.5942\n",
      "Epoch [1/1], Step [185/245], Loss: 1.6549\n",
      "Epoch [1/1], Step [186/245], Loss: 1.7506\n",
      "Epoch [1/1], Step [187/245], Loss: 1.6410\n",
      "Epoch [1/1], Step [188/245], Loss: 1.6669\n",
      "Epoch [1/1], Step [189/245], Loss: 1.6696\n",
      "Epoch [1/1], Step [190/245], Loss: 1.6456\n",
      "Epoch [1/1], Step [191/245], Loss: 1.6543\n",
      "Epoch [1/1], Step [192/245], Loss: 1.6662\n",
      "Epoch [1/1], Step [193/245], Loss: 1.7161\n",
      "Epoch [1/1], Step [194/245], Loss: 1.6710\n",
      "Epoch [1/1], Step [195/245], Loss: 1.6755\n",
      "Epoch [1/1], Step [196/245], Loss: 1.8140\n",
      "Epoch [1/1], Step [197/245], Loss: 1.6222\n",
      "Epoch [1/1], Step [198/245], Loss: 1.7244\n",
      "Epoch [1/1], Step [199/245], Loss: 1.6571\n",
      "Epoch [1/1], Step [200/245], Loss: 1.6413\n",
      "Epoch [1/1], Step [201/245], Loss: 1.7528\n",
      "Epoch [1/1], Step [202/245], Loss: 1.6350\n",
      "Epoch [1/1], Step [203/245], Loss: 1.6047\n",
      "Epoch [1/1], Step [204/245], Loss: 1.5839\n",
      "Epoch [1/1], Step [205/245], Loss: 1.7317\n",
      "Epoch [1/1], Step [206/245], Loss: 1.6736\n",
      "Epoch [1/1], Step [207/245], Loss: 1.6051\n",
      "Epoch [1/1], Step [208/245], Loss: 1.6531\n",
      "Epoch [1/1], Step [209/245], Loss: 1.6196\n",
      "Epoch [1/1], Step [210/245], Loss: 1.6422\n",
      "Epoch [1/1], Step [211/245], Loss: 1.7100\n",
      "Epoch [1/1], Step [212/245], Loss: 1.6010\n",
      "Epoch [1/1], Step [213/245], Loss: 1.7349\n",
      "Epoch [1/1], Step [214/245], Loss: 1.7892\n",
      "Epoch [1/1], Step [215/245], Loss: 1.7015\n",
      "Epoch [1/1], Step [216/245], Loss: 1.7495\n",
      "Epoch [1/1], Step [217/245], Loss: 1.7301\n",
      "Epoch [1/1], Step [218/245], Loss: 1.6983\n",
      "Epoch [1/1], Step [219/245], Loss: 1.5861\n",
      "Epoch [1/1], Step [220/245], Loss: 1.7244\n",
      "Epoch [1/1], Step [221/245], Loss: 1.5627\n",
      "Epoch [1/1], Step [222/245], Loss: 1.7080\n",
      "Epoch [1/1], Step [223/245], Loss: 1.7814\n",
      "Epoch [1/1], Step [224/245], Loss: 1.5744\n",
      "Epoch [1/1], Step [225/245], Loss: 1.6770\n",
      "Epoch [1/1], Step [226/245], Loss: 1.5849\n",
      "Epoch [1/1], Step [227/245], Loss: 1.5983\n",
      "Epoch [1/1], Step [228/245], Loss: 1.6016\n",
      "Epoch [1/1], Step [229/245], Loss: 1.6733\n",
      "Epoch [1/1], Step [230/245], Loss: 1.6211\n",
      "Epoch [1/1], Step [231/245], Loss: 1.5822\n",
      "Epoch [1/1], Step [232/245], Loss: 1.5787\n",
      "Epoch [1/1], Step [233/245], Loss: 1.7121\n",
      "Epoch [1/1], Step [234/245], Loss: 1.5799\n",
      "Epoch [1/1], Step [235/245], Loss: 1.7229\n",
      "Epoch [1/1], Step [236/245], Loss: 1.6450\n",
      "Epoch [1/1], Step [237/245], Loss: 1.6891\n",
      "Epoch [1/1], Step [238/245], Loss: 1.5393\n",
      "Epoch [1/1], Step [239/245], Loss: 1.6286\n",
      "Epoch [1/1], Step [240/245], Loss: 1.6310\n",
      "Epoch [1/1], Step [241/245], Loss: 1.6206\n",
      "Epoch [1/1], Step [242/245], Loss: 1.6837\n",
      "Epoch [1/1], Step [243/245], Loss: 1.7029\n",
      "Epoch [1/1], Step [244/245], Loss: 1.6643\n",
      "Epoch [1/1], Step [245/245], Loss: 1.7116\n",
      "Train accuracy is: 36.47142857142857 %\n",
      "Validation accuracy is: 42.8 %\n"
     ]
    }
   ],
   "source": [
    "model1_name = 'fl_model1'\n",
    "\n",
    "# Training\n",
    "model1.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model1.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model1, model_path, model1_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train second local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/245], Loss: 2.3026\n",
      "Epoch [1/1], Step [2/245], Loss: 2.3014\n",
      "Epoch [1/1], Step [3/245], Loss: 2.2973\n",
      "Epoch [1/1], Step [4/245], Loss: 2.2880\n",
      "Epoch [1/1], Step [5/245], Loss: 2.2750\n",
      "Epoch [1/1], Step [6/245], Loss: 2.2597\n",
      "Epoch [1/1], Step [7/245], Loss: 2.2329\n",
      "Epoch [1/1], Step [8/245], Loss: 2.2412\n",
      "Epoch [1/1], Step [9/245], Loss: 2.1947\n",
      "Epoch [1/1], Step [10/245], Loss: 2.2021\n",
      "Epoch [1/1], Step [11/245], Loss: 2.1993\n",
      "Epoch [1/1], Step [12/245], Loss: 2.1529\n",
      "Epoch [1/1], Step [13/245], Loss: 2.1605\n",
      "Epoch [1/1], Step [14/245], Loss: 2.0533\n",
      "Epoch [1/1], Step [15/245], Loss: 2.0696\n",
      "Epoch [1/1], Step [16/245], Loss: 2.1049\n",
      "Epoch [1/1], Step [17/245], Loss: 2.0760\n",
      "Epoch [1/1], Step [18/245], Loss: 2.0217\n",
      "Epoch [1/1], Step [19/245], Loss: 2.0295\n",
      "Epoch [1/1], Step [20/245], Loss: 2.0319\n",
      "Epoch [1/1], Step [21/245], Loss: 2.0052\n",
      "Epoch [1/1], Step [22/245], Loss: 2.0004\n",
      "Epoch [1/1], Step [23/245], Loss: 1.9725\n",
      "Epoch [1/1], Step [24/245], Loss: 2.0768\n",
      "Epoch [1/1], Step [25/245], Loss: 2.0046\n",
      "Epoch [1/1], Step [26/245], Loss: 1.9925\n",
      "Epoch [1/1], Step [27/245], Loss: 1.9198\n",
      "Epoch [1/1], Step [28/245], Loss: 1.9314\n",
      "Epoch [1/1], Step [29/245], Loss: 2.0080\n",
      "Epoch [1/1], Step [30/245], Loss: 1.9483\n",
      "Epoch [1/1], Step [31/245], Loss: 1.9655\n",
      "Epoch [1/1], Step [32/245], Loss: 1.8935\n",
      "Epoch [1/1], Step [33/245], Loss: 1.9707\n",
      "Epoch [1/1], Step [34/245], Loss: 1.9566\n",
      "Epoch [1/1], Step [35/245], Loss: 1.9916\n",
      "Epoch [1/1], Step [36/245], Loss: 1.8422\n",
      "Epoch [1/1], Step [37/245], Loss: 1.9703\n",
      "Epoch [1/1], Step [38/245], Loss: 1.8942\n",
      "Epoch [1/1], Step [39/245], Loss: 1.9121\n",
      "Epoch [1/1], Step [40/245], Loss: 1.9016\n",
      "Epoch [1/1], Step [41/245], Loss: 1.8548\n",
      "Epoch [1/1], Step [42/245], Loss: 1.9074\n",
      "Epoch [1/1], Step [43/245], Loss: 1.9144\n",
      "Epoch [1/1], Step [44/245], Loss: 1.8426\n",
      "Epoch [1/1], Step [45/245], Loss: 1.8817\n",
      "Epoch [1/1], Step [46/245], Loss: 1.8134\n",
      "Epoch [1/1], Step [47/245], Loss: 1.8521\n",
      "Epoch [1/1], Step [48/245], Loss: 1.8420\n",
      "Epoch [1/1], Step [49/245], Loss: 1.9636\n",
      "Epoch [1/1], Step [50/245], Loss: 1.8338\n",
      "Epoch [1/1], Step [51/245], Loss: 1.7505\n",
      "Epoch [1/1], Step [52/245], Loss: 1.8485\n",
      "Epoch [1/1], Step [53/245], Loss: 1.9209\n",
      "Epoch [1/1], Step [54/245], Loss: 1.7950\n",
      "Epoch [1/1], Step [55/245], Loss: 1.7921\n",
      "Epoch [1/1], Step [56/245], Loss: 1.8919\n",
      "Epoch [1/1], Step [57/245], Loss: 1.9783\n",
      "Epoch [1/1], Step [58/245], Loss: 1.8862\n",
      "Epoch [1/1], Step [59/245], Loss: 1.8218\n",
      "Epoch [1/1], Step [60/245], Loss: 1.8616\n",
      "Epoch [1/1], Step [61/245], Loss: 1.7313\n",
      "Epoch [1/1], Step [62/245], Loss: 1.9349\n",
      "Epoch [1/1], Step [63/245], Loss: 1.8268\n",
      "Epoch [1/1], Step [64/245], Loss: 1.7714\n",
      "Epoch [1/1], Step [65/245], Loss: 1.8624\n",
      "Epoch [1/1], Step [66/245], Loss: 1.7842\n",
      "Epoch [1/1], Step [67/245], Loss: 1.8208\n",
      "Epoch [1/1], Step [68/245], Loss: 1.7245\n",
      "Epoch [1/1], Step [69/245], Loss: 1.7614\n",
      "Epoch [1/1], Step [70/245], Loss: 1.8651\n",
      "Epoch [1/1], Step [71/245], Loss: 1.7975\n",
      "Epoch [1/1], Step [72/245], Loss: 1.8356\n",
      "Epoch [1/1], Step [73/245], Loss: 1.8183\n",
      "Epoch [1/1], Step [74/245], Loss: 1.7030\n",
      "Epoch [1/1], Step [75/245], Loss: 1.8573\n",
      "Epoch [1/1], Step [76/245], Loss: 1.7198\n",
      "Epoch [1/1], Step [77/245], Loss: 1.7822\n",
      "Epoch [1/1], Step [78/245], Loss: 1.8077\n",
      "Epoch [1/1], Step [79/245], Loss: 1.8311\n",
      "Epoch [1/1], Step [80/245], Loss: 1.7506\n",
      "Epoch [1/1], Step [81/245], Loss: 1.7389\n",
      "Epoch [1/1], Step [82/245], Loss: 1.6929\n",
      "Epoch [1/1], Step [83/245], Loss: 1.8302\n",
      "Epoch [1/1], Step [84/245], Loss: 1.8383\n",
      "Epoch [1/1], Step [85/245], Loss: 1.7522\n",
      "Epoch [1/1], Step [86/245], Loss: 1.6820\n",
      "Epoch [1/1], Step [87/245], Loss: 1.8110\n",
      "Epoch [1/1], Step [88/245], Loss: 1.8124\n",
      "Epoch [1/1], Step [89/245], Loss: 1.6783\n",
      "Epoch [1/1], Step [90/245], Loss: 1.7672\n",
      "Epoch [1/1], Step [91/245], Loss: 1.6867\n",
      "Epoch [1/1], Step [92/245], Loss: 1.8473\n",
      "Epoch [1/1], Step [93/245], Loss: 1.7656\n",
      "Epoch [1/1], Step [94/245], Loss: 1.7632\n",
      "Epoch [1/1], Step [95/245], Loss: 1.6982\n",
      "Epoch [1/1], Step [96/245], Loss: 1.7041\n",
      "Epoch [1/1], Step [97/245], Loss: 1.8925\n",
      "Epoch [1/1], Step [98/245], Loss: 1.8560\n",
      "Epoch [1/1], Step [99/245], Loss: 1.7720\n",
      "Epoch [1/1], Step [100/245], Loss: 1.8123\n",
      "Epoch [1/1], Step [101/245], Loss: 1.7387\n",
      "Epoch [1/1], Step [102/245], Loss: 1.7390\n",
      "Epoch [1/1], Step [103/245], Loss: 1.6569\n",
      "Epoch [1/1], Step [104/245], Loss: 1.8101\n",
      "Epoch [1/1], Step [105/245], Loss: 1.7984\n",
      "Epoch [1/1], Step [106/245], Loss: 1.6542\n",
      "Epoch [1/1], Step [107/245], Loss: 1.7469\n",
      "Epoch [1/1], Step [108/245], Loss: 1.7511\n",
      "Epoch [1/1], Step [109/245], Loss: 1.7840\n",
      "Epoch [1/1], Step [110/245], Loss: 1.6710\n",
      "Epoch [1/1], Step [111/245], Loss: 1.8821\n",
      "Epoch [1/1], Step [112/245], Loss: 1.7407\n",
      "Epoch [1/1], Step [113/245], Loss: 1.7412\n",
      "Epoch [1/1], Step [114/245], Loss: 1.6669\n",
      "Epoch [1/1], Step [115/245], Loss: 1.8135\n",
      "Epoch [1/1], Step [116/245], Loss: 1.8139\n",
      "Epoch [1/1], Step [117/245], Loss: 1.7119\n",
      "Epoch [1/1], Step [118/245], Loss: 1.7599\n",
      "Epoch [1/1], Step [119/245], Loss: 1.6737\n",
      "Epoch [1/1], Step [120/245], Loss: 1.6702\n",
      "Epoch [1/1], Step [121/245], Loss: 1.7898\n",
      "Epoch [1/1], Step [122/245], Loss: 1.8209\n",
      "Epoch [1/1], Step [123/245], Loss: 1.7647\n",
      "Epoch [1/1], Step [124/245], Loss: 1.7196\n",
      "Epoch [1/1], Step [125/245], Loss: 1.6842\n",
      "Epoch [1/1], Step [126/245], Loss: 1.7179\n",
      "Epoch [1/1], Step [127/245], Loss: 1.7815\n",
      "Epoch [1/1], Step [128/245], Loss: 1.8517\n",
      "Epoch [1/1], Step [129/245], Loss: 1.6699\n",
      "Epoch [1/1], Step [130/245], Loss: 1.6455\n",
      "Epoch [1/1], Step [131/245], Loss: 1.7031\n",
      "Epoch [1/1], Step [132/245], Loss: 1.7313\n",
      "Epoch [1/1], Step [133/245], Loss: 1.7295\n",
      "Epoch [1/1], Step [134/245], Loss: 1.6685\n",
      "Epoch [1/1], Step [135/245], Loss: 1.7085\n",
      "Epoch [1/1], Step [136/245], Loss: 1.7037\n",
      "Epoch [1/1], Step [137/245], Loss: 1.7630\n",
      "Epoch [1/1], Step [138/245], Loss: 1.7073\n",
      "Epoch [1/1], Step [139/245], Loss: 1.7047\n",
      "Epoch [1/1], Step [140/245], Loss: 1.6979\n",
      "Epoch [1/1], Step [141/245], Loss: 1.7385\n",
      "Epoch [1/1], Step [142/245], Loss: 1.6542\n",
      "Epoch [1/1], Step [143/245], Loss: 1.6673\n",
      "Epoch [1/1], Step [144/245], Loss: 1.7028\n",
      "Epoch [1/1], Step [145/245], Loss: 1.6956\n",
      "Epoch [1/1], Step [146/245], Loss: 1.7347\n",
      "Epoch [1/1], Step [147/245], Loss: 1.6062\n",
      "Epoch [1/1], Step [148/245], Loss: 1.6456\n",
      "Epoch [1/1], Step [149/245], Loss: 1.6369\n",
      "Epoch [1/1], Step [150/245], Loss: 1.6414\n",
      "Epoch [1/1], Step [151/245], Loss: 1.6811\n",
      "Epoch [1/1], Step [152/245], Loss: 1.6837\n",
      "Epoch [1/1], Step [153/245], Loss: 1.6514\n",
      "Epoch [1/1], Step [154/245], Loss: 1.7260\n",
      "Epoch [1/1], Step [155/245], Loss: 1.8665\n",
      "Epoch [1/1], Step [156/245], Loss: 1.8373\n",
      "Epoch [1/1], Step [157/245], Loss: 1.6545\n",
      "Epoch [1/1], Step [158/245], Loss: 1.7409\n",
      "Epoch [1/1], Step [159/245], Loss: 1.6833\n",
      "Epoch [1/1], Step [160/245], Loss: 1.5962\n",
      "Epoch [1/1], Step [161/245], Loss: 1.8239\n",
      "Epoch [1/1], Step [162/245], Loss: 1.7986\n",
      "Epoch [1/1], Step [163/245], Loss: 1.6908\n",
      "Epoch [1/1], Step [164/245], Loss: 1.7250\n",
      "Epoch [1/1], Step [165/245], Loss: 1.6688\n",
      "Epoch [1/1], Step [166/245], Loss: 1.7400\n",
      "Epoch [1/1], Step [167/245], Loss: 1.6755\n",
      "Epoch [1/1], Step [168/245], Loss: 1.6685\n",
      "Epoch [1/1], Step [169/245], Loss: 1.5975\n",
      "Epoch [1/1], Step [170/245], Loss: 1.6327\n",
      "Epoch [1/1], Step [171/245], Loss: 1.6030\n",
      "Epoch [1/1], Step [172/245], Loss: 1.6243\n",
      "Epoch [1/1], Step [173/245], Loss: 1.7199\n",
      "Epoch [1/1], Step [174/245], Loss: 1.6914\n",
      "Epoch [1/1], Step [175/245], Loss: 1.6077\n",
      "Epoch [1/1], Step [176/245], Loss: 1.6488\n",
      "Epoch [1/1], Step [177/245], Loss: 1.7066\n",
      "Epoch [1/1], Step [178/245], Loss: 1.7062\n",
      "Epoch [1/1], Step [179/245], Loss: 1.7074\n",
      "Epoch [1/1], Step [180/245], Loss: 1.7097\n",
      "Epoch [1/1], Step [181/245], Loss: 1.6660\n",
      "Epoch [1/1], Step [182/245], Loss: 1.5857\n",
      "Epoch [1/1], Step [183/245], Loss: 1.7629\n",
      "Epoch [1/1], Step [184/245], Loss: 1.6077\n",
      "Epoch [1/1], Step [185/245], Loss: 1.7451\n",
      "Epoch [1/1], Step [186/245], Loss: 1.7005\n",
      "Epoch [1/1], Step [187/245], Loss: 1.6309\n",
      "Epoch [1/1], Step [188/245], Loss: 1.6975\n",
      "Epoch [1/1], Step [189/245], Loss: 1.5842\n",
      "Epoch [1/1], Step [190/245], Loss: 1.6567\n",
      "Epoch [1/1], Step [191/245], Loss: 1.7732\n",
      "Epoch [1/1], Step [192/245], Loss: 1.6958\n",
      "Epoch [1/1], Step [193/245], Loss: 1.5078\n",
      "Epoch [1/1], Step [194/245], Loss: 1.5938\n",
      "Epoch [1/1], Step [195/245], Loss: 1.6974\n",
      "Epoch [1/1], Step [196/245], Loss: 1.5102\n",
      "Epoch [1/1], Step [197/245], Loss: 1.6019\n",
      "Epoch [1/1], Step [198/245], Loss: 1.6372\n",
      "Epoch [1/1], Step [199/245], Loss: 1.6726\n",
      "Epoch [1/1], Step [200/245], Loss: 1.7323\n",
      "Epoch [1/1], Step [201/245], Loss: 1.5602\n",
      "Epoch [1/1], Step [202/245], Loss: 1.6824\n",
      "Epoch [1/1], Step [203/245], Loss: 1.5498\n",
      "Epoch [1/1], Step [204/245], Loss: 1.7100\n",
      "Epoch [1/1], Step [205/245], Loss: 1.7541\n",
      "Epoch [1/1], Step [206/245], Loss: 1.5132\n",
      "Epoch [1/1], Step [207/245], Loss: 1.6482\n",
      "Epoch [1/1], Step [208/245], Loss: 1.6460\n",
      "Epoch [1/1], Step [209/245], Loss: 1.7311\n",
      "Epoch [1/1], Step [210/245], Loss: 1.5317\n",
      "Epoch [1/1], Step [211/245], Loss: 1.6478\n",
      "Epoch [1/1], Step [212/245], Loss: 1.6330\n",
      "Epoch [1/1], Step [213/245], Loss: 1.6058\n",
      "Epoch [1/1], Step [214/245], Loss: 1.6454\n",
      "Epoch [1/1], Step [215/245], Loss: 1.6293\n",
      "Epoch [1/1], Step [216/245], Loss: 1.7620\n",
      "Epoch [1/1], Step [217/245], Loss: 1.5684\n",
      "Epoch [1/1], Step [218/245], Loss: 1.6597\n",
      "Epoch [1/1], Step [219/245], Loss: 1.6721\n",
      "Epoch [1/1], Step [220/245], Loss: 1.6103\n",
      "Epoch [1/1], Step [221/245], Loss: 1.5891\n",
      "Epoch [1/1], Step [222/245], Loss: 1.6285\n",
      "Epoch [1/1], Step [223/245], Loss: 1.7488\n",
      "Epoch [1/1], Step [224/245], Loss: 1.7094\n",
      "Epoch [1/1], Step [225/245], Loss: 1.5828\n",
      "Epoch [1/1], Step [226/245], Loss: 1.5778\n",
      "Epoch [1/1], Step [227/245], Loss: 1.7512\n",
      "Epoch [1/1], Step [228/245], Loss: 1.4856\n",
      "Epoch [1/1], Step [229/245], Loss: 1.6724\n",
      "Epoch [1/1], Step [230/245], Loss: 1.5448\n",
      "Epoch [1/1], Step [231/245], Loss: 1.7470\n",
      "Epoch [1/1], Step [232/245], Loss: 1.6672\n",
      "Epoch [1/1], Step [233/245], Loss: 1.6038\n",
      "Epoch [1/1], Step [234/245], Loss: 1.5930\n",
      "Epoch [1/1], Step [235/245], Loss: 1.6581\n",
      "Epoch [1/1], Step [236/245], Loss: 1.5389\n",
      "Epoch [1/1], Step [237/245], Loss: 1.6100\n",
      "Epoch [1/1], Step [238/245], Loss: 1.6521\n",
      "Epoch [1/1], Step [239/245], Loss: 1.6820\n",
      "Epoch [1/1], Step [240/245], Loss: 1.6857\n",
      "Epoch [1/1], Step [241/245], Loss: 1.5190\n",
      "Epoch [1/1], Step [242/245], Loss: 1.5895\n",
      "Epoch [1/1], Step [243/245], Loss: 1.7388\n",
      "Epoch [1/1], Step [244/245], Loss: 1.6061\n",
      "Epoch [1/1], Step [245/245], Loss: 1.5411\n",
      "Train accuracy is: 36.39795918367347 %\n",
      "Validation accuracy is: 44.6 %\n"
     ]
    }
   ],
   "source": [
    "model2_name = 'fl_model2'\n",
    "# Training\n",
    "model2.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model2.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model2, model_path, model2_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 43.7 %\n",
      "Accuracy of the network on the 1000 test images: 42.3 %\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "model_path = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "model_ckpt = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model_ckpt = torch.load(pjoin(model_path, model1_name + '.ckpt'))\n",
    "model1.load_state_dict(model_ckpt)\n",
    "model = model1\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "model = model2\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregate the local models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.1760e-02,  4.2900e-02,  5.2851e-02,  5.0147e-02,  5.7749e-02,\n",
      "         3.8284e-02,  6.9570e-02,  3.4316e-02,  1.0920e-02,  5.5295e-02,\n",
      "         4.3783e-02,  1.0032e-01,  6.2822e-02,  5.3711e-03,  8.9889e-02,\n",
      "        -7.9435e-05,  8.7550e-02,  5.3161e-02, -4.7431e-03, -2.6418e-02,\n",
      "         4.5759e-02,  1.8515e-02,  1.9627e-03,  5.3370e-02, -1.1729e-03,\n",
      "         2.1432e-02,  7.6793e-02,  6.1534e-02,  1.6080e-02,  4.0207e-02,\n",
      "         4.6392e-02, -1.2634e-02, -1.0254e-02,  3.3230e-02,  6.8993e-02,\n",
      "         3.1953e-03, -4.5845e-02,  7.1283e-02,  9.0138e-02,  2.0860e-02,\n",
      "        -9.9138e-03,  1.2532e-02,  6.2080e-02,  2.9840e-02,  7.1691e-02,\n",
      "         1.7509e-02,  6.7909e-02,  4.6026e-02,  4.5743e-02,  5.9319e-02],\n",
      "       device='cuda:0')\n",
      "tensor([ 4.0880e-02,  2.1450e-02,  2.6425e-02,  2.5073e-02,  2.8874e-02,\n",
      "         1.9142e-02,  3.4785e-02,  1.7158e-02,  5.4600e-03,  2.7648e-02,\n",
      "         2.1891e-02,  5.0162e-02,  3.1411e-02,  2.6855e-03,  4.4945e-02,\n",
      "        -3.9718e-05,  4.3775e-02,  2.6581e-02, -2.3716e-03, -1.3209e-02,\n",
      "         2.2879e-02,  9.2576e-03,  9.8134e-04,  2.6685e-02, -5.8645e-04,\n",
      "         1.0716e-02,  3.8397e-02,  3.0767e-02,  8.0401e-03,  2.0103e-02,\n",
      "         2.3196e-02, -6.3168e-03, -5.1272e-03,  1.6615e-02,  3.4496e-02,\n",
      "         1.5977e-03, -2.2922e-02,  3.5641e-02,  4.5069e-02,  1.0430e-02,\n",
      "        -4.9569e-03,  6.2658e-03,  3.1040e-02,  1.4920e-02,  3.5846e-02,\n",
      "         8.7544e-03,  3.3954e-02,  2.3013e-02,  2.2872e-02,  2.9660e-02],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from client.utils import weights_zero_init\n",
    "from client.aggregators import federated_aggregate\n",
    "from os.path import join as pjoin\n",
    "\n",
    "\n",
    "model3 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model3.to(device)\n",
    "weights_zero_init(model3)\n",
    "# print(model3)\n",
    "\n",
    "model_avg = federated_aggregate([model1, model3])\n",
    "\n",
    "# model_avg = federated_aggregate([model1, model2])\n",
    "\n",
    "\n",
    "# for parameter in model1.parameters():\n",
    "#     print(parameter)\n",
    "    \n",
    "print(model1.state_dict()['layers.0.bias'])\n",
    "print(model_avg.state_dict()['layers.0.bias'])\n",
    "# the averaged model should be half the model1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Print the new global model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model1.parameters():\n",
    "    print(parameter)\n",
    "  \n",
    "# for parameter in model2.parameters():\n",
    "#     print(parameter)  \n",
    "\n",
    "for parameter in model_avg.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model_avg\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
