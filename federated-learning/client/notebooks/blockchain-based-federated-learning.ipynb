{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of one round of blockchain-based federated learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available and set the default tensor type to CUDA\n",
    "print('Using device: %s' % device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config variables for models and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.train import train\n",
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, weights_zero_init,  weights_init, update_lr\n",
    "from client.aggregators import federated_aggregate\n",
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "modelpath = 'client/models/'\n",
    "train_flag = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = 'client/data/'\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(\n",
    "    root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(\n",
    "    root, batch_size, num_training, num_validation, device)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two local models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "model2 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model2.to(device)\n",
    "print(model2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first local model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fl_model'\n",
    "\n",
    "# Training\n",
    "model.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model, modelpath, model_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train second local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_name = 'fl_model2'\n",
    "# Training\n",
    "model2.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model2.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model2, modelpath, model2_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Forward the two local models to the blockchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregate the local models on the blockchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this should be triggered automatically when the total users in the blockchain have triggered their updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Download the new global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new global model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
