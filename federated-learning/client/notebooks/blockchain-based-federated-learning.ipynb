{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of one round of blockchain-based federated learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available and set the default tensor type to CUDA\n",
    "print('Using device: %s' % device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config variables for models and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.train import train\n",
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json,  weights_init, update_lr\n",
    "from client.aggregators import federated_aggregate\n",
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "modelpath = 'client/models/'\n",
    "train_flag = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = 'client/data/'\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(\n",
    "    root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(\n",
    "    root, batch_size, num_training, num_validation, device)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two local models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "\n",
    "model1 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model1.to(device)\n",
    "print(model1)\n",
    "model1_name = 'bcfl_model1'\n",
    "\n",
    "model2 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model2.to(device)\n",
    "print(model2)\n",
    "model2_name = 'bcfl_model2'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first local model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_name = 'bcfl_model1'\n",
    "\n",
    "# Training\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model1.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model1, modelpath, model1_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train second local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_name = 'bcfl_model4'\n",
    "# Training\n",
    "model2.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model2.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model2, modelpath, model2_name, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "model_ckpt = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model_ckpt = torch.load(pjoin(modelpath, model1_name + '.ckpt'))\n",
    "model1.load_state_dict(model_ckpt)\n",
    "model = model1\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "model = model2\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Forward the two local models to the blockchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Model submitted succesfully'}\n"
     ]
    }
   ],
   "source": [
    "# Send the saved model to Fabric-SDK via Gateway Client (REST call)\n",
    "\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "\n",
    "\n",
    "print(submit_model(model1_name, model1.state_dict()))\n",
    "\n",
    "# print(submit_model(model2_name, model2.state_dict()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit empty model to the blockchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Model submitted succesfully'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO this should be triggered automatically when the total users in the blockchain have triggered their updates\n",
    "\n",
    "model3 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model3.to(device)\n",
    "model3.zero_init()\n",
    "model3_name = 'bcfl_model_empty'\n",
    "# print(model3)\n",
    "submit_model(model3_name, model3.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregate the local models on the blockchain\n",
    "TODO this should be triggered automatically when the total users in the blockchain have triggered their updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Error aggregating models',\n",
       " 'error': '10 ABORTED: failed to endorse transaction, see attached details for more info'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO this should be triggered automatically when the total users in the blockchain have triggered their updates\n",
    "\n",
    "from client.services.gateway_client import aggregate_models\n",
    "\n",
    "\n",
    "\n",
    "aggregate_models([model1_name, model3_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Download the new global model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.5758e-05,  1.0754e-03, -2.2970e-04, -7.9567e-04,  2.7304e-04,\n",
      "         -2.7690e-04,  9.2563e-04, -2.9406e-04,  2.1546e-03, -2.0175e-03,\n",
      "         -7.3503e-05, -5.0823e-06,  7.2380e-04, -3.4622e-04, -3.1366e-04,\n",
      "          9.8631e-04, -6.9316e-04, -8.6765e-04, -5.1616e-05,  4.1474e-04,\n",
      "          1.2934e-03,  5.7558e-04, -1.3560e-03,  1.0217e-03,  8.4389e-04,\n",
      "         -2.4260e-04, -1.0143e-03,  5.9964e-04, -1.3085e-03, -6.9872e-05,\n",
      "         -1.1036e-03,  4.8429e-04,  3.4555e-04,  1.3241e-03,  1.0655e-03,\n",
      "          1.5742e-04, -4.3371e-04, -1.9814e-03,  2.1921e-04, -4.9663e-04,\n",
      "          3.1255e-04,  1.3933e-03,  1.7812e-04,  1.3495e-04,  1.4883e-03,\n",
      "          7.1574e-04, -1.0019e-03,  1.2602e-03, -2.7279e-04, -3.8909e-04]],\n",
      "       device='cuda:0')\n",
      "tensor([[-2.2879e-05,  5.3772e-04, -1.1485e-04, -3.9784e-04,  1.3652e-04,\n",
      "         -1.3845e-04,  4.6281e-04, -1.4703e-04,  1.0773e-03, -1.0088e-03,\n",
      "         -3.6751e-05, -2.5412e-06,  3.6190e-04, -1.7311e-04, -1.5683e-04,\n",
      "          4.9316e-04, -3.4658e-04, -4.3383e-04, -2.5808e-05,  2.0737e-04,\n",
      "          6.4668e-04,  2.8779e-04, -6.7798e-04,  5.1085e-04,  4.2195e-04,\n",
      "         -1.2130e-04, -5.0713e-04,  2.9982e-04, -6.5427e-04, -3.4936e-05,\n",
      "         -5.5182e-04,  2.4215e-04,  1.7277e-04,  6.6207e-04,  5.3277e-04,\n",
      "          7.8709e-05, -2.1685e-04, -9.9068e-04,  1.0961e-04, -2.4832e-04,\n",
      "          1.5627e-04,  6.9667e-04,  8.9062e-05,  6.7476e-05,  7.4415e-04,\n",
      "          3.5787e-04, -5.0095e-04,  6.3010e-04, -1.3640e-04, -1.9454e-04]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, deserialize_model_msgpack\n",
    "\n",
    "# retrieve future global model and convert it again to pytorch model\n",
    "\n",
    "# TODO change\n",
    "model_avg_name = model1_name + model3_name\n",
    "model_data = get_model(model_avg_name)\n",
    "decoded_state_dict = deserialize_model_msgpack(model_data['modelParams'])\n",
    "\n",
    "global_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "global_model.to(device)\n",
    "global_model.load_state_dict(decoded_state_dict)\n",
    "\n",
    "\n",
    "# print(global_model)\n",
    "   \n",
    "print(model1.state_dict()['layers.2.weight'][0:1])\n",
    "print(global_model.state_dict()['layers.2.weight'][0:1])\n",
    "# the averaged model should be half the model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the new global model and compare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.utils import count_parameters, compare_models\n",
    "\n",
    "print(model1.state_dict()['layers.0.bias'])\n",
    "print(global_model.state_dict()['layers.0.bias'])\n",
    "# the global model should be half of the model1\n",
    "\n",
    "print(count_parameters(model1))\n",
    "print(count_parameters(global_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = global_model\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve global model and convert it again to pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, count_parameters, compare_models\n",
    "\n",
    "\n",
    "local_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "local_model = torch.load(pjoin(modelpath,  'ml_model2.ckpt'))\n",
    "model.load_state_dict(local_model)\n",
    "\n",
    "model_params = get_model('ml_model2')\n",
    "# print(client_response)\n",
    "\n",
    "\n",
    "bc_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "bc_model.to(device)\n",
    "# print(bc_model)\n",
    "\n",
    "load_model_from_json(bc_model, model_params)\n",
    "print(bc_model)\n",
    "print(count_parameters(model))\n",
    "print(count_parameters(bc_model))\n",
    "print(compare_models(model, bc_model))\n",
    "# for parameter in model.parameters():\n",
    "#     print(parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bert Tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "\n",
    "from transformers import AutoModel # For BERTs\n",
    "from transformers import AutoModelForSequenceClassification # For models fine-tuned on MNLI\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "print(bert_model)\n",
    "bert_modelname = 'bert_model'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "bert_model2 = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "bert_modelname2 = 'bert_model2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model in a local file, load it again and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'count_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m loaded_bert_model_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(pjoin(modelpath, bert_modelname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m loaded_bert_model\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_bert_model_params)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcount_parameters\u001b[49m(bert_model))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_parameters(loaded_bert_model))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(compare_models(bert_model, loaded_bert_model))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "# Send the saved model to Fabric-SDK via Gateway Client (REST call)\n",
    "\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(bert_model.state_dict(), pjoin(modelpath, bert_modelname + '.pt'))\n",
    "    \n",
    "loaded_bert_model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "\n",
    "loaded_bert_model_params = torch.load(pjoin(modelpath, bert_modelname + '.pt'))\n",
    "loaded_bert_model.load_state_dict(loaded_bert_model_params)\n",
    "\n",
    "print(count_parameters(bert_model))\n",
    "print(count_parameters(loaded_bert_model))\n",
    "print(compare_models(bert_model, loaded_bert_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize and encode model, decode and try if its equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.658535957336426\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(encoded))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# decoded_state_dict = deserialize_model_msgpack(encoded)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m decoded_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_model_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m bert2_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprajjwal1/bert-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# v1 and v2\u001b[39;00m\n\u001b[1;32m     17\u001b[0m bert2_model\u001b[38;5;241m.\u001b[39mload_state_dict(decoded_state_dict)\n",
      "File \u001b[0;32m~/_bcfl/fabric-federated-learning/federated-learning/client/utils.py:73\u001b[0m, in \u001b[0;36mdeserialize_model_numpy\u001b[0;34m(encoded_str)\u001b[0m\n\u001b[1;32m     70\u001b[0m unpacked_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(buffer, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 4. Convert to torch tensors\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {key: torch\u001b[38;5;241m.\u001b[39mtensor(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m unpacked_data\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from client.services.gateway_client import submit_model\n",
    "from client.utils import serialize_model_numpy, deserialize_model_numpy, print_layer_size, serialize_model_msgpack, deserialize_model_msgpack, compare_models, compare_state_dicts, compare_weights\n",
    "\n",
    "\n",
    "encoded = serialize_model_msgpack(bert_model.state_dict(), 0, 0)\n",
    "# encoded = serialize_model_numpy(bert_model.state_dict())\n",
    "\n",
    "print(sys.getsizeof(encoded) / 1024 / 1024)\n",
    "print(type(encoded))\n",
    "decoded_state_dict = deserialize_model_msgpack(encoded)\n",
    "# decoded_state_dict = deserialize_model_numpy(encoded)\n",
    "\n",
    "\n",
    "bert2_model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "\n",
    "bert2_model.load_state_dict(decoded_state_dict)\n",
    "\n",
    "print(compare_state_dicts(bert_model.state_dict(), bert2_model.state_dict()))\n",
    "print(compare_models(bert_model, bert2_model))\n",
    "print(compare_weights(bert_model, bert2_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Bert Tiny model to the blockchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Model submitted succesfully'}\n",
      "{'message': 'Model submitted succesfully'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "from client.services.gateway_client import submit_model\n",
    "from client.utils import print_layer_size, serialize_model_msgpack, deserialize_model_msgpack, compare_models, compare_state_dicts, compare_weights\n",
    "\n",
    "# Send the saved model to Fabric-SDK via Gateway Client (REST call)\n",
    "# print(submit_model(bert_modelname, bert_model.state_dict()))\n",
    "print(submit_model(bert_modelname, bert_model.state_dict(), 0, 2))\n",
    "print(submit_model(bert_modelname2, bert_model2.state_dict(), 0, 2))\n",
    "\n",
    "# print_layer_size(bert_model.state_dict(), 0, 6, 'mb', 'base64')\n",
    "# print(submit_compressed_model(bert_modelname, bert_model.state_dict()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model and load it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "4386178\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bc_bert_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# load_model_from_json(bc_bert_model, bert_model_params)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# compare it with previous bert model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_parameters(bert_model))\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_parameters(\u001b[43mbc_bert_model\u001b[49m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(compare_models(bert_model, bc_bert_model))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bc_bert_model' is not defined"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, count_parameters, compare_models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blockchain_bert_model = get_model(bert_modelname)\n",
    "\n",
    "decoded_state_dict = deserialize_model_msgpack(blockchain_bert_model['modelParams'])\n",
    "\n",
    "\n",
    "bert2_model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\") # v1 and v2\n",
    "\n",
    "bert2_model.load_state_dict(decoded_state_dict)\n",
    "\n",
    "print(compare_state_dicts(bert_model.state_dict(), bert2_model.state_dict()))\n",
    "print(compare_models(bert_model, bert2_model))\n",
    "print(compare_weights(bert_model, bert2_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# NVIDIA drivers not working\n",
    "torch.set_default_device('cpu')\n",
    "device = ('cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
