{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the module directory to import python files (RUN JUST ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available and set the default tensor type to CUDA\n",
    "print('Using device: %s' % device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a sample Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc.weight \t torch.Size([1, 10])\n",
      "fc.bias \t torch.Size([1])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]\n",
      "cuda:0\n",
      "Epoch 0, Loss: 0.7277003526687622\n",
      "Epoch 1, Loss: 0.7271116971969604\n",
      "Epoch 2, Loss: 0.7265263199806213\n",
      "Epoch 3, Loss: 0.7259442210197449\n",
      "Epoch 4, Loss: 0.7253652811050415\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# torch.set_default_device('cpu')\n",
    "\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "num_classes = 1  # Example number of classes\n",
    "# model = Perceptron(n_features, num_classes)  # Instantiate the model (on the default device\n",
    "model = Perceptron(n_features)  # Instantiate the model (on the default device\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "    \n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "# Binary target values (0 or 1)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CIFAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "root = 'client/data/'\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(\n",
    "    root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(\n",
    "    root, batch_size, num_training, num_validation, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/1], Step [1/245], Loss: 2.3026\n",
      "Epoch [1/1], Step [2/245], Loss: 2.3007\n",
      "Epoch [1/1], Step [3/245], Loss: 2.2943\n",
      "Epoch [1/1], Step [4/245], Loss: 2.2792\n",
      "Epoch [1/1], Step [5/245], Loss: 2.2648\n",
      "Epoch [1/1], Step [6/245], Loss: 2.2522\n",
      "Epoch [1/1], Step [7/245], Loss: 2.2217\n",
      "Epoch [1/1], Step [8/245], Loss: 2.2180\n",
      "Epoch [1/1], Step [9/245], Loss: 2.2226\n",
      "Epoch [1/1], Step [10/245], Loss: 2.1662\n",
      "Epoch [1/1], Step [11/245], Loss: 2.1590\n",
      "Epoch [1/1], Step [12/245], Loss: 2.1140\n",
      "Epoch [1/1], Step [13/245], Loss: 2.0922\n",
      "Epoch [1/1], Step [14/245], Loss: 2.1684\n",
      "Epoch [1/1], Step [15/245], Loss: 2.1055\n",
      "Epoch [1/1], Step [16/245], Loss: 2.1231\n",
      "Epoch [1/1], Step [17/245], Loss: 1.9972\n",
      "Epoch [1/1], Step [18/245], Loss: 1.9944\n",
      "Epoch [1/1], Step [19/245], Loss: 2.0158\n",
      "Epoch [1/1], Step [20/245], Loss: 2.0759\n",
      "Epoch [1/1], Step [21/245], Loss: 1.9244\n",
      "Epoch [1/1], Step [22/245], Loss: 2.0151\n",
      "Epoch [1/1], Step [23/245], Loss: 2.0948\n",
      "Epoch [1/1], Step [24/245], Loss: 1.9027\n",
      "Epoch [1/1], Step [25/245], Loss: 1.9266\n",
      "Epoch [1/1], Step [26/245], Loss: 1.9658\n",
      "Epoch [1/1], Step [27/245], Loss: 2.0165\n",
      "Epoch [1/1], Step [28/245], Loss: 1.9480\n",
      "Epoch [1/1], Step [29/245], Loss: 1.9158\n",
      "Epoch [1/1], Step [30/245], Loss: 1.9456\n",
      "Epoch [1/1], Step [31/245], Loss: 1.8775\n",
      "Epoch [1/1], Step [32/245], Loss: 1.9367\n",
      "Epoch [1/1], Step [33/245], Loss: 1.8861\n",
      "Epoch [1/1], Step [34/245], Loss: 1.9632\n",
      "Epoch [1/1], Step [35/245], Loss: 1.8005\n",
      "Epoch [1/1], Step [36/245], Loss: 2.0114\n",
      "Epoch [1/1], Step [37/245], Loss: 1.9341\n",
      "Epoch [1/1], Step [38/245], Loss: 1.8047\n",
      "Epoch [1/1], Step [39/245], Loss: 1.8885\n",
      "Epoch [1/1], Step [40/245], Loss: 1.9132\n",
      "Epoch [1/1], Step [41/245], Loss: 1.8160\n",
      "Epoch [1/1], Step [42/245], Loss: 1.8955\n",
      "Epoch [1/1], Step [43/245], Loss: 1.9351\n",
      "Epoch [1/1], Step [44/245], Loss: 1.8895\n",
      "Epoch [1/1], Step [45/245], Loss: 1.9579\n",
      "Epoch [1/1], Step [46/245], Loss: 1.8820\n",
      "Epoch [1/1], Step [47/245], Loss: 1.9179\n",
      "Epoch [1/1], Step [48/245], Loss: 1.8443\n",
      "Epoch [1/1], Step [49/245], Loss: 1.9355\n",
      "Epoch [1/1], Step [50/245], Loss: 1.9060\n",
      "Epoch [1/1], Step [51/245], Loss: 1.8854\n",
      "Epoch [1/1], Step [52/245], Loss: 1.8366\n",
      "Epoch [1/1], Step [53/245], Loss: 1.8338\n",
      "Epoch [1/1], Step [54/245], Loss: 1.9668\n",
      "Epoch [1/1], Step [55/245], Loss: 1.9217\n",
      "Epoch [1/1], Step [56/245], Loss: 1.7938\n",
      "Epoch [1/1], Step [57/245], Loss: 1.8788\n",
      "Epoch [1/1], Step [58/245], Loss: 1.8582\n",
      "Epoch [1/1], Step [59/245], Loss: 1.8378\n",
      "Epoch [1/1], Step [60/245], Loss: 1.7993\n",
      "Epoch [1/1], Step [61/245], Loss: 1.9008\n",
      "Epoch [1/1], Step [62/245], Loss: 1.8837\n",
      "Epoch [1/1], Step [63/245], Loss: 1.7606\n",
      "Epoch [1/1], Step [64/245], Loss: 1.8593\n",
      "Epoch [1/1], Step [65/245], Loss: 1.7814\n",
      "Epoch [1/1], Step [66/245], Loss: 1.7746\n",
      "Epoch [1/1], Step [67/245], Loss: 1.7805\n",
      "Epoch [1/1], Step [68/245], Loss: 1.9122\n",
      "Epoch [1/1], Step [69/245], Loss: 1.7709\n",
      "Epoch [1/1], Step [70/245], Loss: 1.8289\n",
      "Epoch [1/1], Step [71/245], Loss: 1.7986\n",
      "Epoch [1/1], Step [72/245], Loss: 1.8138\n",
      "Epoch [1/1], Step [73/245], Loss: 1.7967\n",
      "Epoch [1/1], Step [74/245], Loss: 1.6704\n",
      "Epoch [1/1], Step [75/245], Loss: 1.7606\n",
      "Epoch [1/1], Step [76/245], Loss: 1.7437\n",
      "Epoch [1/1], Step [77/245], Loss: 1.8260\n",
      "Epoch [1/1], Step [78/245], Loss: 1.8053\n",
      "Epoch [1/1], Step [79/245], Loss: 1.8471\n",
      "Epoch [1/1], Step [80/245], Loss: 1.6855\n",
      "Epoch [1/1], Step [81/245], Loss: 1.7133\n",
      "Epoch [1/1], Step [82/245], Loss: 1.7920\n",
      "Epoch [1/1], Step [83/245], Loss: 1.7221\n",
      "Epoch [1/1], Step [84/245], Loss: 1.7688\n",
      "Epoch [1/1], Step [85/245], Loss: 1.7899\n",
      "Epoch [1/1], Step [86/245], Loss: 1.7986\n",
      "Epoch [1/1], Step [87/245], Loss: 1.8491\n",
      "Epoch [1/1], Step [88/245], Loss: 1.8287\n",
      "Epoch [1/1], Step [89/245], Loss: 1.7575\n",
      "Epoch [1/1], Step [90/245], Loss: 1.7732\n",
      "Epoch [1/1], Step [91/245], Loss: 1.6419\n",
      "Epoch [1/1], Step [92/245], Loss: 1.7216\n",
      "Epoch [1/1], Step [93/245], Loss: 1.6048\n",
      "Epoch [1/1], Step [94/245], Loss: 1.7934\n",
      "Epoch [1/1], Step [95/245], Loss: 1.8533\n",
      "Epoch [1/1], Step [96/245], Loss: 1.7989\n",
      "Epoch [1/1], Step [97/245], Loss: 1.7256\n",
      "Epoch [1/1], Step [98/245], Loss: 1.8294\n",
      "Epoch [1/1], Step [99/245], Loss: 1.7854\n",
      "Epoch [1/1], Step [100/245], Loss: 1.7281\n",
      "Epoch [1/1], Step [101/245], Loss: 1.7836\n",
      "Epoch [1/1], Step [102/245], Loss: 1.7604\n",
      "Epoch [1/1], Step [103/245], Loss: 1.7105\n",
      "Epoch [1/1], Step [104/245], Loss: 1.7763\n",
      "Epoch [1/1], Step [105/245], Loss: 1.7125\n",
      "Epoch [1/1], Step [106/245], Loss: 1.7420\n",
      "Epoch [1/1], Step [107/245], Loss: 1.7737\n",
      "Epoch [1/1], Step [108/245], Loss: 1.6698\n",
      "Epoch [1/1], Step [109/245], Loss: 1.6953\n",
      "Epoch [1/1], Step [110/245], Loss: 1.6849\n",
      "Epoch [1/1], Step [111/245], Loss: 1.8501\n",
      "Epoch [1/1], Step [112/245], Loss: 1.7435\n",
      "Epoch [1/1], Step [113/245], Loss: 1.6269\n",
      "Epoch [1/1], Step [114/245], Loss: 1.7690\n",
      "Epoch [1/1], Step [115/245], Loss: 1.6862\n",
      "Epoch [1/1], Step [116/245], Loss: 1.8085\n",
      "Epoch [1/1], Step [117/245], Loss: 1.7062\n",
      "Epoch [1/1], Step [118/245], Loss: 1.8124\n",
      "Epoch [1/1], Step [119/245], Loss: 1.7539\n",
      "Epoch [1/1], Step [120/245], Loss: 1.7556\n",
      "Epoch [1/1], Step [121/245], Loss: 1.7449\n",
      "Epoch [1/1], Step [122/245], Loss: 1.7328\n",
      "Epoch [1/1], Step [123/245], Loss: 1.6869\n",
      "Epoch [1/1], Step [124/245], Loss: 1.6919\n",
      "Epoch [1/1], Step [125/245], Loss: 1.6438\n",
      "Epoch [1/1], Step [126/245], Loss: 1.7266\n",
      "Epoch [1/1], Step [127/245], Loss: 1.6197\n",
      "Epoch [1/1], Step [128/245], Loss: 1.6943\n",
      "Epoch [1/1], Step [129/245], Loss: 1.7814\n",
      "Epoch [1/1], Step [130/245], Loss: 1.7805\n",
      "Epoch [1/1], Step [131/245], Loss: 1.6831\n",
      "Epoch [1/1], Step [132/245], Loss: 1.8116\n",
      "Epoch [1/1], Step [133/245], Loss: 1.6740\n",
      "Epoch [1/1], Step [134/245], Loss: 1.8145\n",
      "Epoch [1/1], Step [135/245], Loss: 1.6210\n",
      "Epoch [1/1], Step [136/245], Loss: 1.7178\n",
      "Epoch [1/1], Step [137/245], Loss: 1.7782\n",
      "Epoch [1/1], Step [138/245], Loss: 1.5474\n",
      "Epoch [1/1], Step [139/245], Loss: 1.6933\n",
      "Epoch [1/1], Step [140/245], Loss: 1.6409\n",
      "Epoch [1/1], Step [141/245], Loss: 1.6778\n",
      "Epoch [1/1], Step [142/245], Loss: 1.6943\n",
      "Epoch [1/1], Step [143/245], Loss: 1.6296\n",
      "Epoch [1/1], Step [144/245], Loss: 1.6459\n",
      "Epoch [1/1], Step [145/245], Loss: 1.7401\n",
      "Epoch [1/1], Step [146/245], Loss: 1.6323\n",
      "Epoch [1/1], Step [147/245], Loss: 1.7041\n",
      "Epoch [1/1], Step [148/245], Loss: 1.7207\n",
      "Epoch [1/1], Step [149/245], Loss: 1.6655\n",
      "Epoch [1/1], Step [150/245], Loss: 1.6537\n",
      "Epoch [1/1], Step [151/245], Loss: 1.7465\n",
      "Epoch [1/1], Step [152/245], Loss: 1.7006\n",
      "Epoch [1/1], Step [153/245], Loss: 1.6862\n",
      "Epoch [1/1], Step [154/245], Loss: 1.5919\n",
      "Epoch [1/1], Step [155/245], Loss: 1.5337\n",
      "Epoch [1/1], Step [156/245], Loss: 1.6810\n",
      "Epoch [1/1], Step [157/245], Loss: 1.7400\n",
      "Epoch [1/1], Step [158/245], Loss: 1.6973\n",
      "Epoch [1/1], Step [159/245], Loss: 1.7796\n",
      "Epoch [1/1], Step [160/245], Loss: 1.8619\n",
      "Epoch [1/1], Step [161/245], Loss: 1.5095\n",
      "Epoch [1/1], Step [162/245], Loss: 1.6268\n",
      "Epoch [1/1], Step [163/245], Loss: 1.6201\n",
      "Epoch [1/1], Step [164/245], Loss: 1.6245\n",
      "Epoch [1/1], Step [165/245], Loss: 1.8249\n",
      "Epoch [1/1], Step [166/245], Loss: 1.7726\n",
      "Epoch [1/1], Step [167/245], Loss: 1.6647\n",
      "Epoch [1/1], Step [168/245], Loss: 1.6815\n",
      "Epoch [1/1], Step [169/245], Loss: 1.7214\n",
      "Epoch [1/1], Step [170/245], Loss: 1.6620\n",
      "Epoch [1/1], Step [171/245], Loss: 1.6632\n",
      "Epoch [1/1], Step [172/245], Loss: 1.7285\n",
      "Epoch [1/1], Step [173/245], Loss: 1.6538\n",
      "Epoch [1/1], Step [174/245], Loss: 1.6263\n",
      "Epoch [1/1], Step [175/245], Loss: 1.7040\n",
      "Epoch [1/1], Step [176/245], Loss: 1.6416\n",
      "Epoch [1/1], Step [177/245], Loss: 1.6938\n",
      "Epoch [1/1], Step [178/245], Loss: 1.6868\n",
      "Epoch [1/1], Step [179/245], Loss: 1.7105\n",
      "Epoch [1/1], Step [180/245], Loss: 1.6827\n",
      "Epoch [1/1], Step [181/245], Loss: 1.6075\n",
      "Epoch [1/1], Step [182/245], Loss: 1.7085\n",
      "Epoch [1/1], Step [183/245], Loss: 1.6650\n",
      "Epoch [1/1], Step [184/245], Loss: 1.6345\n",
      "Epoch [1/1], Step [185/245], Loss: 1.5721\n",
      "Epoch [1/1], Step [186/245], Loss: 1.5843\n",
      "Epoch [1/1], Step [187/245], Loss: 1.7190\n",
      "Epoch [1/1], Step [188/245], Loss: 1.7243\n",
      "Epoch [1/1], Step [189/245], Loss: 1.5491\n",
      "Epoch [1/1], Step [190/245], Loss: 1.5883\n",
      "Epoch [1/1], Step [191/245], Loss: 1.6521\n",
      "Epoch [1/1], Step [192/245], Loss: 1.6426\n",
      "Epoch [1/1], Step [193/245], Loss: 1.6756\n",
      "Epoch [1/1], Step [194/245], Loss: 1.5520\n",
      "Epoch [1/1], Step [195/245], Loss: 1.6123\n",
      "Epoch [1/1], Step [196/245], Loss: 1.5726\n",
      "Epoch [1/1], Step [197/245], Loss: 1.6363\n",
      "Epoch [1/1], Step [198/245], Loss: 1.6528\n",
      "Epoch [1/1], Step [199/245], Loss: 1.6161\n",
      "Epoch [1/1], Step [200/245], Loss: 1.6648\n",
      "Epoch [1/1], Step [201/245], Loss: 1.6089\n",
      "Epoch [1/1], Step [202/245], Loss: 1.6627\n",
      "Epoch [1/1], Step [203/245], Loss: 1.6652\n",
      "Epoch [1/1], Step [204/245], Loss: 1.6515\n",
      "Epoch [1/1], Step [205/245], Loss: 1.6136\n",
      "Epoch [1/1], Step [206/245], Loss: 1.7272\n",
      "Epoch [1/1], Step [207/245], Loss: 1.7192\n",
      "Epoch [1/1], Step [208/245], Loss: 1.5675\n",
      "Epoch [1/1], Step [209/245], Loss: 1.8603\n",
      "Epoch [1/1], Step [210/245], Loss: 1.7146\n",
      "Epoch [1/1], Step [211/245], Loss: 1.6428\n",
      "Epoch [1/1], Step [212/245], Loss: 1.7024\n",
      "Epoch [1/1], Step [213/245], Loss: 1.7109\n",
      "Epoch [1/1], Step [214/245], Loss: 1.6681\n",
      "Epoch [1/1], Step [215/245], Loss: 1.6280\n",
      "Epoch [1/1], Step [216/245], Loss: 1.7062\n",
      "Epoch [1/1], Step [217/245], Loss: 1.5612\n",
      "Epoch [1/1], Step [218/245], Loss: 1.8424\n",
      "Epoch [1/1], Step [219/245], Loss: 1.6253\n",
      "Epoch [1/1], Step [220/245], Loss: 1.5973\n",
      "Epoch [1/1], Step [221/245], Loss: 1.6862\n",
      "Epoch [1/1], Step [222/245], Loss: 1.5698\n",
      "Epoch [1/1], Step [223/245], Loss: 1.4588\n",
      "Epoch [1/1], Step [224/245], Loss: 1.7755\n",
      "Epoch [1/1], Step [225/245], Loss: 1.5991\n",
      "Epoch [1/1], Step [226/245], Loss: 1.6180\n",
      "Epoch [1/1], Step [227/245], Loss: 1.6287\n",
      "Epoch [1/1], Step [228/245], Loss: 1.7435\n",
      "Epoch [1/1], Step [229/245], Loss: 1.6882\n",
      "Epoch [1/1], Step [230/245], Loss: 1.6274\n",
      "Epoch [1/1], Step [231/245], Loss: 1.6907\n",
      "Epoch [1/1], Step [232/245], Loss: 1.5491\n",
      "Epoch [1/1], Step [233/245], Loss: 1.5262\n",
      "Epoch [1/1], Step [234/245], Loss: 1.6555\n",
      "Epoch [1/1], Step [235/245], Loss: 1.6687\n",
      "Epoch [1/1], Step [236/245], Loss: 1.5206\n",
      "Epoch [1/1], Step [237/245], Loss: 1.4962\n",
      "Epoch [1/1], Step [238/245], Loss: 1.5952\n",
      "Epoch [1/1], Step [239/245], Loss: 1.6267\n",
      "Epoch [1/1], Step [240/245], Loss: 1.5816\n",
      "Epoch [1/1], Step [241/245], Loss: 1.6858\n",
      "Epoch [1/1], Step [242/245], Loss: 1.5742\n",
      "Epoch [1/1], Step [243/245], Loss: 1.6561\n",
      "Epoch [1/1], Step [244/245], Loss: 1.6540\n",
      "Epoch [1/1], Step [245/245], Loss: 1.6512\n",
      "Train accuracy is: 36.83673469387755 %\n",
      "Validation accuracy is: 43.0 %\n"
     ]
    }
   ],
   "source": [
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.train import train\n",
    "from client.utils import weights_init, update_lr\n",
    "\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "modelpath = 'client/models/'\n",
    "train_flag = True\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "modelname = 'model1'\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "# Training\n",
    "model.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model, modelpath, modelname, dataloaders, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 41.7 %\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "best_model = torch.load(pjoin(modelpath, modelname + '.ckpt'))\n",
    "model.load_state_dict(best_model)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockchain and FL code \n",
    "TODO remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the saved model to Fabric-SDK via Gateway Client (REST call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "best_model = torch.load(pjoin(modelpath, 'model.ckpt'))\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "submit_local_model(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve future global model and convert it again to pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0165,  0.0194,  0.0171,  ..., -0.0069, -0.0136, -0.0147],\n",
      "        [ 0.0060,  0.0150,  0.0190,  ..., -0.0616, -0.0620, -0.0606],\n",
      "        [-0.0043, -0.0144, -0.0166,  ..., -0.0392, -0.0331, -0.0260],\n",
      "        ...,\n",
      "        [-0.0403, -0.0381, -0.0357,  ...,  0.0240,  0.0226,  0.0221],\n",
      "        [-0.0104, -0.0060, -0.0007,  ..., -0.0304, -0.0349, -0.0367],\n",
      "        [-0.0251, -0.0113, -0.0075,  ..., -0.0311, -0.0307, -0.0287]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0196,  0.0870,  0.0041,  0.0843,  0.0820,  0.0826,  0.0252, -0.0290,\n",
      "         0.0780, -0.0044,  0.0506,  0.0846,  0.0469, -0.0107, -0.0231,  0.0393,\n",
      "         0.0472,  0.0455,  0.0254,  0.0367,  0.0750,  0.0290,  0.0442,  0.0764,\n",
      "         0.0833,  0.0745,  0.0692, -0.0153, -0.0339, -0.0164, -0.0105,  0.0816,\n",
      "         0.0416,  0.0468,  0.0355,  0.0966, -0.0204, -0.0316, -0.0081,  0.0500,\n",
      "        -0.0329, -0.0091,  0.0948,  0.0269,  0.0601,  0.0513, -0.0091,  0.0158,\n",
      "         0.0381,  0.0419], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0729e-02,  1.6369e-03,  1.7457e-02, -2.0261e-02, -3.5362e-02,\n",
      "         -3.4316e-02,  3.6650e-02, -1.5674e-02,  3.5169e-02, -4.9696e-03,\n",
      "          2.1244e-02, -4.4616e-03, -3.1764e-03, -5.2060e-02,  1.2644e-03,\n",
      "         -2.6990e-02,  3.5615e-02, -6.0836e-02,  4.5971e-02, -5.7113e-02,\n",
      "          2.6357e-02,  2.4081e-02, -2.8939e-02, -3.6222e-03, -3.1129e-02,\n",
      "          1.5366e-02,  1.4063e-02, -4.9014e-02, -3.6614e-02,  3.4805e-02,\n",
      "          7.3209e-03,  5.6828e-03,  1.5886e-02, -1.7330e-02,  2.9071e-02,\n",
      "          7.1362e-03, -7.3371e-03, -4.1359e-02, -3.8137e-03,  1.3146e-02,\n",
      "          1.1137e-05,  1.8308e-02,  9.7951e-03,  2.5185e-02, -6.4025e-02,\n",
      "          3.8309e-02,  9.7468e-03,  4.7813e-02,  1.0280e-02, -4.0392e-02],\n",
      "        [ 4.0677e-02, -4.0055e-02, -5.0535e-03, -3.4151e-02, -2.8060e-02,\n",
      "         -6.6479e-03, -1.6352e-02,  2.4971e-02, -3.1873e-03,  4.9322e-02,\n",
      "          2.1286e-04, -5.5128e-02, -1.9015e-02, -2.7914e-03,  1.9923e-02,\n",
      "         -1.5661e-02,  1.3851e-02, -6.0672e-03,  2.8890e-02,  2.8528e-02,\n",
      "         -7.0068e-02,  6.6781e-02, -7.1055e-02, -4.6835e-02, -4.3818e-02,\n",
      "         -6.8113e-02, -5.0396e-02,  5.0821e-02, -1.7345e-02,  2.9608e-02,\n",
      "          8.0719e-03, -4.0224e-02, -1.7112e-02, -8.2332e-02,  3.7117e-02,\n",
      "         -3.7885e-02,  5.8904e-02,  4.0080e-02,  7.6002e-02,  1.5286e-02,\n",
      "          3.9737e-02,  4.8322e-02, -6.3285e-02,  1.9329e-02, -7.3543e-02,\n",
      "         -3.1983e-02,  1.1420e-02, -4.3303e-02, -3.7898e-02,  1.1530e-02],\n",
      "        [ 5.1135e-03,  4.3775e-02, -5.1879e-02,  2.8158e-02,  9.4534e-03,\n",
      "          1.5822e-02,  2.7002e-02, -5.1034e-02,  2.7472e-02, -2.2898e-02,\n",
      "          2.8557e-02,  4.8029e-02,  4.9925e-03, -3.6357e-02, -4.3372e-02,\n",
      "         -5.7086e-02,  4.5895e-03, -4.5066e-02, -1.5990e-02, -9.2061e-03,\n",
      "          9.9713e-03,  6.2816e-03, -1.8915e-02, -4.1257e-02,  1.5503e-02,\n",
      "          2.3076e-02,  1.8857e-02, -6.8645e-02, -2.8294e-02, -4.5510e-02,\n",
      "         -2.2175e-02,  2.6987e-02, -3.0930e-03,  3.8709e-02,  4.1945e-03,\n",
      "          3.3668e-02, -3.5705e-02, -2.9138e-02, -6.5195e-02,  1.1524e-02,\n",
      "         -6.7092e-02, -4.7139e-02,  3.4183e-02,  1.0564e-03,  1.1486e-02,\n",
      "          2.4159e-02, -1.8142e-02,  4.0312e-02, -4.4129e-03,  1.5848e-02],\n",
      "        [ 6.4835e-03,  2.9141e-03, -3.6810e-02,  2.6821e-02,  2.8724e-03,\n",
      "         -3.3154e-02,  1.4336e-02, -1.3313e-02, -3.6903e-02, -2.7823e-03,\n",
      "         -1.7521e-02,  3.7468e-02, -2.5651e-02,  3.0820e-02, -1.5655e-02,\n",
      "          1.0979e-02, -2.6938e-02,  3.8833e-02, -2.3817e-02, -1.0902e-03,\n",
      "         -1.3144e-03, -4.5085e-02,  3.3939e-02,  1.1828e-02,  4.4826e-02,\n",
      "         -1.7708e-02, -1.1288e-03,  1.6820e-02,  1.8762e-02, -4.6482e-02,\n",
      "         -1.7984e-02, -4.0451e-02, -1.6677e-02,  2.0457e-02, -4.1380e-02,\n",
      "         -1.4254e-02, -2.3425e-02, -5.4581e-03, -1.3648e-02, -3.2707e-02,\n",
      "         -5.7194e-03, -3.5938e-02,  1.0109e-02, -2.7139e-02,  4.4248e-02,\n",
      "         -2.2278e-02, -2.2631e-02,  9.3685e-03,  4.4332e-02, -2.7354e-02],\n",
      "        [-2.0130e-02,  3.0514e-02, -6.5955e-03,  1.6490e-02,  2.8352e-02,\n",
      "          4.0692e-02, -3.1509e-02, -1.9234e-02,  5.6574e-03, -3.1896e-02,\n",
      "         -1.0069e-03,  1.5148e-02, -2.1184e-03, -3.6907e-02, -2.3894e-02,\n",
      "          3.9960e-02,  1.1737e-03, -6.9260e-02, -1.5439e-02,  1.4565e-02,\n",
      "          6.9864e-03, -3.3363e-02,  1.3908e-03, -6.4284e-02,  1.2797e-02,\n",
      "          2.1341e-02,  3.0231e-03, -3.8696e-02, -4.2690e-02, -5.9903e-02,\n",
      "         -1.8093e-02,  3.3463e-02,  3.6324e-03,  2.0562e-03, -3.7257e-02,\n",
      "          3.2991e-02, -2.6334e-02, -2.9864e-02, -5.2377e-02, -6.0393e-03,\n",
      "         -2.2137e-02, -3.9132e-02,  1.1605e-02, -6.3626e-03, -1.8322e-02,\n",
      "          2.2543e-03, -5.3605e-03,  3.4478e-04, -2.0593e-02,  3.3782e-02],\n",
      "        [-2.2607e-03,  4.2619e-03, -3.6676e-02,  1.2746e-02,  1.0394e-02,\n",
      "          1.3298e-02,  8.4137e-04, -2.1658e-02, -3.5153e-02, -2.0769e-02,\n",
      "         -2.5972e-02,  2.3643e-02, -4.5379e-02,  2.5453e-02, -1.3394e-02,\n",
      "          1.8683e-02, -5.2322e-02,  5.0827e-02, -5.7434e-02, -2.9557e-02,\n",
      "          1.3341e-02, -5.0450e-02,  3.6588e-02,  5.7802e-02,  2.0705e-02,\n",
      "         -2.6878e-03, -3.5256e-02,  6.6329e-03,  2.5154e-02, -2.8496e-02,\n",
      "         -1.8993e-02, -2.4490e-02, -3.2609e-02,  2.2031e-02, -3.1251e-02,\n",
      "         -8.3745e-04, -2.6679e-02, -1.7705e-02, -2.1892e-02, -3.3461e-02,\n",
      "         -2.5473e-02, -3.3855e-02,  1.0034e-02, -2.9329e-02,  2.8374e-02,\n",
      "         -2.8478e-02, -6.6972e-02,  5.7546e-03,  5.5306e-02, -3.8859e-02],\n",
      "        [ 4.1360e-02,  5.8332e-03, -1.0114e-01,  2.9276e-02,  2.1201e-02,\n",
      "         -1.6665e-02,  4.5933e-02, -2.5295e-02, -3.9692e-02, -5.6481e-02,\n",
      "         -5.7863e-02, -2.2734e-02, -3.6094e-02,  3.5519e-02, -3.9387e-02,\n",
      "         -7.0849e-02, -3.3828e-02,  1.3839e-02, -3.4889e-03,  5.7389e-02,\n",
      "          4.2728e-02, -5.4623e-02,  7.2693e-03, -5.2655e-02,  1.9580e-02,\n",
      "         -1.6002e-02, -2.3239e-02,  1.8644e-02,  1.2915e-02, -5.7977e-02,\n",
      "         -4.5042e-02,  2.4047e-02, -3.7642e-02, -2.6265e-02, -1.2569e-01,\n",
      "          2.9775e-02, -7.8523e-02, -2.4089e-03, -4.7579e-02, -3.8696e-02,\n",
      "         -3.0146e-02, -4.9563e-02,  1.5590e-02, -2.2054e-02,  3.5864e-02,\n",
      "         -1.3118e-02, -2.2331e-02, -2.9608e-02, -5.5599e-02,  1.7675e-02],\n",
      "        [-2.7565e-02,  5.7017e-03,  8.0654e-02, -4.8323e-02,  4.5906e-03,\n",
      "          5.1731e-02, -1.5187e-02, -1.7326e-02, -3.4534e-02, -3.7526e-02,\n",
      "         -3.6040e-02, -8.6271e-03,  6.0021e-03,  4.2519e-02, -6.1421e-03,\n",
      "          4.6621e-02, -2.6473e-02,  1.6814e-02, -5.8197e-02, -4.3945e-02,\n",
      "          3.5526e-03, -6.7536e-02,  1.0287e-02, -4.0017e-02, -4.3858e-02,\n",
      "         -1.5944e-02, -2.0308e-02, -6.9859e-02,  6.1161e-02,  4.8329e-02,\n",
      "         -9.9715e-03,  3.8831e-02, -4.6523e-03,  5.2848e-02,  1.9283e-02,\n",
      "          2.4239e-03, -1.9706e-02, -6.6856e-02, -4.7638e-02, -2.7360e-02,\n",
      "          5.5064e-03, -3.7828e-02, -3.0662e-02, -2.8714e-02, -2.9841e-02,\n",
      "         -5.0502e-02,  1.5599e-04, -8.7439e-03, -1.2974e-02,  3.7572e-02],\n",
      "        [-5.2629e-02, -5.5082e-02, -1.1162e-02, -3.6207e-02, -5.3749e-02,\n",
      "         -5.6588e-02, -5.4584e-02,  1.2437e-02,  1.8592e-02,  1.1946e-02,\n",
      "          2.2748e-02, -7.8589e-02,  4.6211e-02, -4.5418e-02,  2.0640e-02,\n",
      "          4.4336e-02,  1.0735e-02, -9.9783e-03,  7.4187e-04, -3.2423e-02,\n",
      "         -7.0452e-02, -5.8836e-03, -3.5816e-02,  6.7193e-02, -1.0548e-02,\n",
      "          5.5698e-02,  2.2157e-02,  1.9586e-02, -4.4516e-02,  1.5006e-02,\n",
      "          2.2231e-02, -8.3922e-02,  3.1813e-02, -7.0768e-02,  5.2100e-03,\n",
      "         -5.3597e-02,  1.2646e-02,  1.7634e-02,  1.6778e-02,  1.7663e-02,\n",
      "          2.3700e-04,  4.6726e-02,  3.2413e-02,  2.0649e-02,  7.0356e-03,\n",
      "          2.5313e-02,  1.9646e-02, -6.4696e-02, -4.5302e-02, -2.3444e-02],\n",
      "        [-1.7913e-02, -7.5489e-02,  4.3676e-02, -4.1669e-02, -1.2601e-02,\n",
      "         -5.9112e-02, -5.0633e-02,  4.0956e-02, -1.1365e-02,  1.9720e-02,\n",
      "         -2.2278e-03, -3.1187e-02, -2.3911e-03,  1.9413e-02,  2.8838e-02,\n",
      "         -2.4914e-02, -2.0249e-03, -1.9299e-02,  1.4343e-02,  2.3412e-02,\n",
      "         -2.1101e-02,  2.2086e-02, -7.1484e-03, -2.7701e-02, -5.4808e-02,\n",
      "         -6.5412e-02,  1.7008e-02,  2.8570e-02,  1.8748e-02,  1.3159e-02,\n",
      "          2.8915e-02,  1.8272e-02, -2.8994e-03,  9.2520e-03,  2.1501e-02,\n",
      "         -6.6443e-02,  2.0426e-02,  3.6627e-02,  2.3290e-02,  1.1776e-02,\n",
      "          3.1246e-02,  5.2659e-03, -5.9701e-02, -1.4105e-02, -4.7947e-02,\n",
      "         -3.2944e-02,  2.4009e-02, -3.1633e-02, -7.3891e-03, -3.4503e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0173, -0.0383,  0.0372,  0.0072,  0.0288,  0.0234, -0.0029, -0.0065,\n",
      "         0.0080, -0.0434], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json\n",
    "\n",
    "model_params = get_model('modelID')\n",
    "# print(client_response)\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "load_model_from_json(model, model_params)\n",
    "print(model)\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample neural network code that checks if the tensors and model are running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "# Instantiate the model (on the default device\n",
    "model = Perceptron(n_features).to('cuda')\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "# Binary target values (0 or 1)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    start_event.record()\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    print(f\"Elapsed time (in milliseconds): {elapsed_time_ms}\")\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
