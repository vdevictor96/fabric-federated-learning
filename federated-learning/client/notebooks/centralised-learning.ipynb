{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the module directory to import python files (RUN JUST ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available and set the default tensor type to CUDA\n",
    "print('Using device: %s' % device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a sample Perceptron to try the blockchain integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc.weight \t torch.Size([1, 10])\n",
      "fc.bias \t torch.Size([1])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]\n",
      "cuda:0\n",
      "Epoch 0, Loss: 0.821436882019043\n",
      "Epoch 1, Loss: 0.8204890489578247\n",
      "Epoch 2, Loss: 0.8195452094078064\n",
      "Epoch 3, Loss: 0.8186054825782776\n",
      "Epoch 4, Loss: 0.8176696300506592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# torch.set_default_device('cpu')\n",
    "\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "num_classes = 1  # Example number of classes\n",
    "# model = Perceptron(n_features, num_classes)  # Instantiate the model (on the default device\n",
    "model = Perceptron(n_features)  # Instantiate the model (on the default device\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "    \n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "# Binary target values (0 or 1)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CIFAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "root = 'client/data/'\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(\n",
    "    root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(\n",
    "    root, batch_size, num_training, num_validation, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/1], Step [1/245], Loss: 2.3026\n",
      "Epoch [1/1], Step [2/245], Loss: 2.2999\n",
      "Epoch [1/1], Step [3/245], Loss: 2.2931\n",
      "Epoch [1/1], Step [4/245], Loss: 2.2768\n",
      "Epoch [1/1], Step [5/245], Loss: 2.2638\n",
      "Epoch [1/1], Step [6/245], Loss: 2.2501\n",
      "Epoch [1/1], Step [7/245], Loss: 2.2196\n",
      "Epoch [1/1], Step [8/245], Loss: 2.2181\n",
      "Epoch [1/1], Step [9/245], Loss: 2.2221\n",
      "Epoch [1/1], Step [10/245], Loss: 2.1657\n",
      "Epoch [1/1], Step [11/245], Loss: 2.1538\n",
      "Epoch [1/1], Step [12/245], Loss: 2.1077\n",
      "Epoch [1/1], Step [13/245], Loss: 2.0933\n",
      "Epoch [1/1], Step [14/245], Loss: 2.1610\n",
      "Epoch [1/1], Step [15/245], Loss: 2.0913\n",
      "Epoch [1/1], Step [16/245], Loss: 2.1184\n",
      "Epoch [1/1], Step [17/245], Loss: 1.9994\n",
      "Epoch [1/1], Step [18/245], Loss: 1.9845\n",
      "Epoch [1/1], Step [19/245], Loss: 2.0183\n",
      "Epoch [1/1], Step [20/245], Loss: 2.0641\n",
      "Epoch [1/1], Step [21/245], Loss: 1.9244\n",
      "Epoch [1/1], Step [22/245], Loss: 2.0066\n",
      "Epoch [1/1], Step [23/245], Loss: 2.0939\n",
      "Epoch [1/1], Step [24/245], Loss: 1.8987\n",
      "Epoch [1/1], Step [25/245], Loss: 1.9149\n",
      "Epoch [1/1], Step [26/245], Loss: 1.9593\n",
      "Epoch [1/1], Step [27/245], Loss: 2.0102\n",
      "Epoch [1/1], Step [28/245], Loss: 1.9439\n",
      "Epoch [1/1], Step [29/245], Loss: 1.9168\n",
      "Epoch [1/1], Step [30/245], Loss: 1.9571\n",
      "Epoch [1/1], Step [31/245], Loss: 1.8834\n",
      "Epoch [1/1], Step [32/245], Loss: 1.9419\n",
      "Epoch [1/1], Step [33/245], Loss: 1.8882\n",
      "Epoch [1/1], Step [34/245], Loss: 1.9725\n",
      "Epoch [1/1], Step [35/245], Loss: 1.8121\n",
      "Epoch [1/1], Step [36/245], Loss: 2.0201\n",
      "Epoch [1/1], Step [37/245], Loss: 1.9405\n",
      "Epoch [1/1], Step [38/245], Loss: 1.8048\n",
      "Epoch [1/1], Step [39/245], Loss: 1.8987\n",
      "Epoch [1/1], Step [40/245], Loss: 1.9132\n",
      "Epoch [1/1], Step [41/245], Loss: 1.8070\n",
      "Epoch [1/1], Step [42/245], Loss: 1.8816\n",
      "Epoch [1/1], Step [43/245], Loss: 1.9330\n",
      "Epoch [1/1], Step [44/245], Loss: 1.9009\n",
      "Epoch [1/1], Step [45/245], Loss: 1.9580\n",
      "Epoch [1/1], Step [46/245], Loss: 1.8841\n",
      "Epoch [1/1], Step [47/245], Loss: 1.9203\n",
      "Epoch [1/1], Step [48/245], Loss: 1.8563\n",
      "Epoch [1/1], Step [49/245], Loss: 1.9337\n",
      "Epoch [1/1], Step [50/245], Loss: 1.9228\n",
      "Epoch [1/1], Step [51/245], Loss: 1.8983\n",
      "Epoch [1/1], Step [52/245], Loss: 1.8488\n",
      "Epoch [1/1], Step [53/245], Loss: 1.8503\n",
      "Epoch [1/1], Step [54/245], Loss: 1.9767\n",
      "Epoch [1/1], Step [55/245], Loss: 1.9385\n",
      "Epoch [1/1], Step [56/245], Loss: 1.8140\n",
      "Epoch [1/1], Step [57/245], Loss: 1.8870\n",
      "Epoch [1/1], Step [58/245], Loss: 1.8566\n",
      "Epoch [1/1], Step [59/245], Loss: 1.8499\n",
      "Epoch [1/1], Step [60/245], Loss: 1.8020\n",
      "Epoch [1/1], Step [61/245], Loss: 1.9152\n",
      "Epoch [1/1], Step [62/245], Loss: 1.8885\n",
      "Epoch [1/1], Step [63/245], Loss: 1.7687\n",
      "Epoch [1/1], Step [64/245], Loss: 1.8780\n",
      "Epoch [1/1], Step [65/245], Loss: 1.7961\n",
      "Epoch [1/1], Step [66/245], Loss: 1.7856\n",
      "Epoch [1/1], Step [67/245], Loss: 1.7913\n",
      "Epoch [1/1], Step [68/245], Loss: 1.9410\n",
      "Epoch [1/1], Step [69/245], Loss: 1.7838\n",
      "Epoch [1/1], Step [70/245], Loss: 1.8169\n",
      "Epoch [1/1], Step [71/245], Loss: 1.8269\n",
      "Epoch [1/1], Step [72/245], Loss: 1.8315\n",
      "Epoch [1/1], Step [73/245], Loss: 1.8139\n",
      "Epoch [1/1], Step [74/245], Loss: 1.6958\n",
      "Epoch [1/1], Step [75/245], Loss: 1.7710\n",
      "Epoch [1/1], Step [76/245], Loss: 1.7450\n",
      "Epoch [1/1], Step [77/245], Loss: 1.8510\n",
      "Epoch [1/1], Step [78/245], Loss: 1.8249\n",
      "Epoch [1/1], Step [79/245], Loss: 1.8525\n",
      "Epoch [1/1], Step [80/245], Loss: 1.6952\n",
      "Epoch [1/1], Step [81/245], Loss: 1.6976\n",
      "Epoch [1/1], Step [82/245], Loss: 1.8166\n",
      "Epoch [1/1], Step [83/245], Loss: 1.7221\n",
      "Epoch [1/1], Step [84/245], Loss: 1.7528\n",
      "Epoch [1/1], Step [85/245], Loss: 1.8134\n",
      "Epoch [1/1], Step [86/245], Loss: 1.8186\n",
      "Epoch [1/1], Step [87/245], Loss: 1.8658\n",
      "Epoch [1/1], Step [88/245], Loss: 1.8308\n",
      "Epoch [1/1], Step [89/245], Loss: 1.7596\n",
      "Epoch [1/1], Step [90/245], Loss: 1.7757\n",
      "Epoch [1/1], Step [91/245], Loss: 1.6696\n",
      "Epoch [1/1], Step [92/245], Loss: 1.7161\n",
      "Epoch [1/1], Step [93/245], Loss: 1.6310\n",
      "Epoch [1/1], Step [94/245], Loss: 1.8059\n",
      "Epoch [1/1], Step [95/245], Loss: 1.8688\n",
      "Epoch [1/1], Step [96/245], Loss: 1.8239\n",
      "Epoch [1/1], Step [97/245], Loss: 1.7386\n",
      "Epoch [1/1], Step [98/245], Loss: 1.8359\n",
      "Epoch [1/1], Step [99/245], Loss: 1.7966\n",
      "Epoch [1/1], Step [100/245], Loss: 1.7413\n",
      "Epoch [1/1], Step [101/245], Loss: 1.8064\n",
      "Epoch [1/1], Step [102/245], Loss: 1.7718\n",
      "Epoch [1/1], Step [103/245], Loss: 1.7310\n",
      "Epoch [1/1], Step [104/245], Loss: 1.7865\n",
      "Epoch [1/1], Step [105/245], Loss: 1.7175\n",
      "Epoch [1/1], Step [106/245], Loss: 1.7418\n",
      "Epoch [1/1], Step [107/245], Loss: 1.7867\n",
      "Epoch [1/1], Step [108/245], Loss: 1.6942\n",
      "Epoch [1/1], Step [109/245], Loss: 1.6973\n",
      "Epoch [1/1], Step [110/245], Loss: 1.6896\n",
      "Epoch [1/1], Step [111/245], Loss: 1.8747\n",
      "Epoch [1/1], Step [112/245], Loss: 1.7678\n",
      "Epoch [1/1], Step [113/245], Loss: 1.6338\n",
      "Epoch [1/1], Step [114/245], Loss: 1.7897\n",
      "Epoch [1/1], Step [115/245], Loss: 1.7010\n",
      "Epoch [1/1], Step [116/245], Loss: 1.8155\n",
      "Epoch [1/1], Step [117/245], Loss: 1.7231\n",
      "Epoch [1/1], Step [118/245], Loss: 1.8044\n",
      "Epoch [1/1], Step [119/245], Loss: 1.7494\n",
      "Epoch [1/1], Step [120/245], Loss: 1.7661\n",
      "Epoch [1/1], Step [121/245], Loss: 1.7517\n",
      "Epoch [1/1], Step [122/245], Loss: 1.7269\n",
      "Epoch [1/1], Step [123/245], Loss: 1.7097\n",
      "Epoch [1/1], Step [124/245], Loss: 1.7050\n",
      "Epoch [1/1], Step [125/245], Loss: 1.6430\n",
      "Epoch [1/1], Step [126/245], Loss: 1.7470\n",
      "Epoch [1/1], Step [127/245], Loss: 1.6226\n",
      "Epoch [1/1], Step [128/245], Loss: 1.7025\n",
      "Epoch [1/1], Step [129/245], Loss: 1.7949\n",
      "Epoch [1/1], Step [130/245], Loss: 1.7913\n",
      "Epoch [1/1], Step [131/245], Loss: 1.6874\n",
      "Epoch [1/1], Step [132/245], Loss: 1.8147\n",
      "Epoch [1/1], Step [133/245], Loss: 1.6922\n",
      "Epoch [1/1], Step [134/245], Loss: 1.8292\n",
      "Epoch [1/1], Step [135/245], Loss: 1.6186\n",
      "Epoch [1/1], Step [136/245], Loss: 1.7136\n",
      "Epoch [1/1], Step [137/245], Loss: 1.7885\n",
      "Epoch [1/1], Step [138/245], Loss: 1.5570\n",
      "Epoch [1/1], Step [139/245], Loss: 1.7012\n",
      "Epoch [1/1], Step [140/245], Loss: 1.6183\n",
      "Epoch [1/1], Step [141/245], Loss: 1.6706\n",
      "Epoch [1/1], Step [142/245], Loss: 1.7112\n",
      "Epoch [1/1], Step [143/245], Loss: 1.6327\n",
      "Epoch [1/1], Step [144/245], Loss: 1.6583\n",
      "Epoch [1/1], Step [145/245], Loss: 1.7739\n",
      "Epoch [1/1], Step [146/245], Loss: 1.6183\n",
      "Epoch [1/1], Step [147/245], Loss: 1.7105\n",
      "Epoch [1/1], Step [148/245], Loss: 1.7372\n",
      "Epoch [1/1], Step [149/245], Loss: 1.6831\n",
      "Epoch [1/1], Step [150/245], Loss: 1.6521\n",
      "Epoch [1/1], Step [151/245], Loss: 1.7520\n",
      "Epoch [1/1], Step [152/245], Loss: 1.7302\n",
      "Epoch [1/1], Step [153/245], Loss: 1.6825\n",
      "Epoch [1/1], Step [154/245], Loss: 1.6136\n",
      "Epoch [1/1], Step [155/245], Loss: 1.5585\n",
      "Epoch [1/1], Step [156/245], Loss: 1.6743\n",
      "Epoch [1/1], Step [157/245], Loss: 1.7389\n",
      "Epoch [1/1], Step [158/245], Loss: 1.6908\n",
      "Epoch [1/1], Step [159/245], Loss: 1.7914\n",
      "Epoch [1/1], Step [160/245], Loss: 1.8698\n",
      "Epoch [1/1], Step [161/245], Loss: 1.5315\n",
      "Epoch [1/1], Step [162/245], Loss: 1.6294\n",
      "Epoch [1/1], Step [163/245], Loss: 1.6262\n",
      "Epoch [1/1], Step [164/245], Loss: 1.6306\n",
      "Epoch [1/1], Step [165/245], Loss: 1.8343\n",
      "Epoch [1/1], Step [166/245], Loss: 1.7720\n",
      "Epoch [1/1], Step [167/245], Loss: 1.6670\n",
      "Epoch [1/1], Step [168/245], Loss: 1.6769\n",
      "Epoch [1/1], Step [169/245], Loss: 1.7178\n",
      "Epoch [1/1], Step [170/245], Loss: 1.6707\n",
      "Epoch [1/1], Step [171/245], Loss: 1.6647\n",
      "Epoch [1/1], Step [172/245], Loss: 1.7161\n",
      "Epoch [1/1], Step [173/245], Loss: 1.6759\n",
      "Epoch [1/1], Step [174/245], Loss: 1.6281\n",
      "Epoch [1/1], Step [175/245], Loss: 1.7122\n",
      "Epoch [1/1], Step [176/245], Loss: 1.6420\n",
      "Epoch [1/1], Step [177/245], Loss: 1.7090\n",
      "Epoch [1/1], Step [178/245], Loss: 1.7121\n",
      "Epoch [1/1], Step [179/245], Loss: 1.7048\n",
      "Epoch [1/1], Step [180/245], Loss: 1.6839\n",
      "Epoch [1/1], Step [181/245], Loss: 1.6135\n",
      "Epoch [1/1], Step [182/245], Loss: 1.7115\n",
      "Epoch [1/1], Step [183/245], Loss: 1.6725\n",
      "Epoch [1/1], Step [184/245], Loss: 1.6315\n",
      "Epoch [1/1], Step [185/245], Loss: 1.5810\n",
      "Epoch [1/1], Step [186/245], Loss: 1.5936\n",
      "Epoch [1/1], Step [187/245], Loss: 1.6986\n",
      "Epoch [1/1], Step [188/245], Loss: 1.7468\n",
      "Epoch [1/1], Step [189/245], Loss: 1.5402\n",
      "Epoch [1/1], Step [190/245], Loss: 1.5881\n",
      "Epoch [1/1], Step [191/245], Loss: 1.6564\n",
      "Epoch [1/1], Step [192/245], Loss: 1.6324\n",
      "Epoch [1/1], Step [193/245], Loss: 1.6942\n",
      "Epoch [1/1], Step [194/245], Loss: 1.5617\n",
      "Epoch [1/1], Step [195/245], Loss: 1.6340\n",
      "Epoch [1/1], Step [196/245], Loss: 1.5901\n",
      "Epoch [1/1], Step [197/245], Loss: 1.6188\n",
      "Epoch [1/1], Step [198/245], Loss: 1.6528\n",
      "Epoch [1/1], Step [199/245], Loss: 1.6157\n",
      "Epoch [1/1], Step [200/245], Loss: 1.6657\n",
      "Epoch [1/1], Step [201/245], Loss: 1.6130\n",
      "Epoch [1/1], Step [202/245], Loss: 1.6537\n",
      "Epoch [1/1], Step [203/245], Loss: 1.6714\n",
      "Epoch [1/1], Step [204/245], Loss: 1.6764\n",
      "Epoch [1/1], Step [205/245], Loss: 1.6261\n",
      "Epoch [1/1], Step [206/245], Loss: 1.7387\n",
      "Epoch [1/1], Step [207/245], Loss: 1.7322\n",
      "Epoch [1/1], Step [208/245], Loss: 1.5731\n",
      "Epoch [1/1], Step [209/245], Loss: 1.8602\n",
      "Epoch [1/1], Step [210/245], Loss: 1.7090\n",
      "Epoch [1/1], Step [211/245], Loss: 1.6651\n",
      "Epoch [1/1], Step [212/245], Loss: 1.7015\n",
      "Epoch [1/1], Step [213/245], Loss: 1.7006\n",
      "Epoch [1/1], Step [214/245], Loss: 1.6778\n",
      "Epoch [1/1], Step [215/245], Loss: 1.6064\n",
      "Epoch [1/1], Step [216/245], Loss: 1.7131\n",
      "Epoch [1/1], Step [217/245], Loss: 1.5462\n",
      "Epoch [1/1], Step [218/245], Loss: 1.8278\n",
      "Epoch [1/1], Step [219/245], Loss: 1.6299\n",
      "Epoch [1/1], Step [220/245], Loss: 1.6039\n",
      "Epoch [1/1], Step [221/245], Loss: 1.6824\n",
      "Epoch [1/1], Step [222/245], Loss: 1.5720\n",
      "Epoch [1/1], Step [223/245], Loss: 1.4549\n",
      "Epoch [1/1], Step [224/245], Loss: 1.7840\n",
      "Epoch [1/1], Step [225/245], Loss: 1.6099\n",
      "Epoch [1/1], Step [226/245], Loss: 1.6028\n",
      "Epoch [1/1], Step [227/245], Loss: 1.6160\n",
      "Epoch [1/1], Step [228/245], Loss: 1.7450\n",
      "Epoch [1/1], Step [229/245], Loss: 1.6771\n",
      "Epoch [1/1], Step [230/245], Loss: 1.6514\n",
      "Epoch [1/1], Step [231/245], Loss: 1.6805\n",
      "Epoch [1/1], Step [232/245], Loss: 1.5481\n",
      "Epoch [1/1], Step [233/245], Loss: 1.5259\n",
      "Epoch [1/1], Step [234/245], Loss: 1.6530\n",
      "Epoch [1/1], Step [235/245], Loss: 1.7005\n",
      "Epoch [1/1], Step [236/245], Loss: 1.5129\n",
      "Epoch [1/1], Step [237/245], Loss: 1.5142\n",
      "Epoch [1/1], Step [238/245], Loss: 1.6082\n",
      "Epoch [1/1], Step [239/245], Loss: 1.6360\n",
      "Epoch [1/1], Step [240/245], Loss: 1.5743\n",
      "Epoch [1/1], Step [241/245], Loss: 1.7016\n",
      "Epoch [1/1], Step [242/245], Loss: 1.5631\n",
      "Epoch [1/1], Step [243/245], Loss: 1.6716\n",
      "Epoch [1/1], Step [244/245], Loss: 1.6438\n",
      "Epoch [1/1], Step [245/245], Loss: 1.6560\n",
      "Train accuracy is: 36.66122448979592 %\n",
      "Validation accuracy is: 43.8 %\n"
     ]
    }
   ],
   "source": [
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.train import train\n",
    "from client.utils import weights_init, update_lr\n",
    "\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "modelpath = 'client/models/'\n",
    "train_flag = True\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "# Training\n",
    "model.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model, dataloaders, modelpath, criterion, optimizer,\n",
    "      learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 40.9 %\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "best_model = torch.load(pjoin(modelpath, 'model.ckpt'))\n",
    "model.load_state_dict(best_model)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "\n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(\n",
    "        total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the saved model to Fabric-SDK via Gateway Client (REST call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "\n",
    "modelpath = 'client/models/'\n",
    "\n",
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "best_model = torch.load(pjoin(modelpath, 'model.ckpt'))\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "submit_local_model(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve future global model and convert it again to pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0165,  0.0194,  0.0171,  ..., -0.0069, -0.0136, -0.0147],\n",
      "        [ 0.0060,  0.0150,  0.0190,  ..., -0.0616, -0.0620, -0.0606],\n",
      "        [-0.0043, -0.0144, -0.0166,  ..., -0.0392, -0.0331, -0.0260],\n",
      "        ...,\n",
      "        [-0.0403, -0.0381, -0.0357,  ...,  0.0240,  0.0226,  0.0221],\n",
      "        [-0.0104, -0.0060, -0.0007,  ..., -0.0304, -0.0349, -0.0367],\n",
      "        [-0.0251, -0.0113, -0.0075,  ..., -0.0311, -0.0307, -0.0287]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0196,  0.0870,  0.0041,  0.0843,  0.0820,  0.0826,  0.0252, -0.0290,\n",
      "         0.0780, -0.0044,  0.0506,  0.0846,  0.0469, -0.0107, -0.0231,  0.0393,\n",
      "         0.0472,  0.0455,  0.0254,  0.0367,  0.0750,  0.0290,  0.0442,  0.0764,\n",
      "         0.0833,  0.0745,  0.0692, -0.0153, -0.0339, -0.0164, -0.0105,  0.0816,\n",
      "         0.0416,  0.0468,  0.0355,  0.0966, -0.0204, -0.0316, -0.0081,  0.0500,\n",
      "        -0.0329, -0.0091,  0.0948,  0.0269,  0.0601,  0.0513, -0.0091,  0.0158,\n",
      "         0.0381,  0.0419], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0729e-02,  1.6369e-03,  1.7457e-02, -2.0261e-02, -3.5362e-02,\n",
      "         -3.4316e-02,  3.6650e-02, -1.5674e-02,  3.5169e-02, -4.9696e-03,\n",
      "          2.1244e-02, -4.4616e-03, -3.1764e-03, -5.2060e-02,  1.2644e-03,\n",
      "         -2.6990e-02,  3.5615e-02, -6.0836e-02,  4.5971e-02, -5.7113e-02,\n",
      "          2.6357e-02,  2.4081e-02, -2.8939e-02, -3.6222e-03, -3.1129e-02,\n",
      "          1.5366e-02,  1.4063e-02, -4.9014e-02, -3.6614e-02,  3.4805e-02,\n",
      "          7.3209e-03,  5.6828e-03,  1.5886e-02, -1.7330e-02,  2.9071e-02,\n",
      "          7.1362e-03, -7.3371e-03, -4.1359e-02, -3.8137e-03,  1.3146e-02,\n",
      "          1.1137e-05,  1.8308e-02,  9.7951e-03,  2.5185e-02, -6.4025e-02,\n",
      "          3.8309e-02,  9.7468e-03,  4.7813e-02,  1.0280e-02, -4.0392e-02],\n",
      "        [ 4.0677e-02, -4.0055e-02, -5.0535e-03, -3.4151e-02, -2.8060e-02,\n",
      "         -6.6479e-03, -1.6352e-02,  2.4971e-02, -3.1873e-03,  4.9322e-02,\n",
      "          2.1286e-04, -5.5128e-02, -1.9015e-02, -2.7914e-03,  1.9923e-02,\n",
      "         -1.5661e-02,  1.3851e-02, -6.0672e-03,  2.8890e-02,  2.8528e-02,\n",
      "         -7.0068e-02,  6.6781e-02, -7.1055e-02, -4.6835e-02, -4.3818e-02,\n",
      "         -6.8113e-02, -5.0396e-02,  5.0821e-02, -1.7345e-02,  2.9608e-02,\n",
      "          8.0719e-03, -4.0224e-02, -1.7112e-02, -8.2332e-02,  3.7117e-02,\n",
      "         -3.7885e-02,  5.8904e-02,  4.0080e-02,  7.6002e-02,  1.5286e-02,\n",
      "          3.9737e-02,  4.8322e-02, -6.3285e-02,  1.9329e-02, -7.3543e-02,\n",
      "         -3.1983e-02,  1.1420e-02, -4.3303e-02, -3.7898e-02,  1.1530e-02],\n",
      "        [ 5.1135e-03,  4.3775e-02, -5.1879e-02,  2.8158e-02,  9.4534e-03,\n",
      "          1.5822e-02,  2.7002e-02, -5.1034e-02,  2.7472e-02, -2.2898e-02,\n",
      "          2.8557e-02,  4.8029e-02,  4.9925e-03, -3.6357e-02, -4.3372e-02,\n",
      "         -5.7086e-02,  4.5895e-03, -4.5066e-02, -1.5990e-02, -9.2061e-03,\n",
      "          9.9713e-03,  6.2816e-03, -1.8915e-02, -4.1257e-02,  1.5503e-02,\n",
      "          2.3076e-02,  1.8857e-02, -6.8645e-02, -2.8294e-02, -4.5510e-02,\n",
      "         -2.2175e-02,  2.6987e-02, -3.0930e-03,  3.8709e-02,  4.1945e-03,\n",
      "          3.3668e-02, -3.5705e-02, -2.9138e-02, -6.5195e-02,  1.1524e-02,\n",
      "         -6.7092e-02, -4.7139e-02,  3.4183e-02,  1.0564e-03,  1.1486e-02,\n",
      "          2.4159e-02, -1.8142e-02,  4.0312e-02, -4.4129e-03,  1.5848e-02],\n",
      "        [ 6.4835e-03,  2.9141e-03, -3.6810e-02,  2.6821e-02,  2.8724e-03,\n",
      "         -3.3154e-02,  1.4336e-02, -1.3313e-02, -3.6903e-02, -2.7823e-03,\n",
      "         -1.7521e-02,  3.7468e-02, -2.5651e-02,  3.0820e-02, -1.5655e-02,\n",
      "          1.0979e-02, -2.6938e-02,  3.8833e-02, -2.3817e-02, -1.0902e-03,\n",
      "         -1.3144e-03, -4.5085e-02,  3.3939e-02,  1.1828e-02,  4.4826e-02,\n",
      "         -1.7708e-02, -1.1288e-03,  1.6820e-02,  1.8762e-02, -4.6482e-02,\n",
      "         -1.7984e-02, -4.0451e-02, -1.6677e-02,  2.0457e-02, -4.1380e-02,\n",
      "         -1.4254e-02, -2.3425e-02, -5.4581e-03, -1.3648e-02, -3.2707e-02,\n",
      "         -5.7194e-03, -3.5938e-02,  1.0109e-02, -2.7139e-02,  4.4248e-02,\n",
      "         -2.2278e-02, -2.2631e-02,  9.3685e-03,  4.4332e-02, -2.7354e-02],\n",
      "        [-2.0130e-02,  3.0514e-02, -6.5955e-03,  1.6490e-02,  2.8352e-02,\n",
      "          4.0692e-02, -3.1509e-02, -1.9234e-02,  5.6574e-03, -3.1896e-02,\n",
      "         -1.0069e-03,  1.5148e-02, -2.1184e-03, -3.6907e-02, -2.3894e-02,\n",
      "          3.9960e-02,  1.1737e-03, -6.9260e-02, -1.5439e-02,  1.4565e-02,\n",
      "          6.9864e-03, -3.3363e-02,  1.3908e-03, -6.4284e-02,  1.2797e-02,\n",
      "          2.1341e-02,  3.0231e-03, -3.8696e-02, -4.2690e-02, -5.9903e-02,\n",
      "         -1.8093e-02,  3.3463e-02,  3.6324e-03,  2.0562e-03, -3.7257e-02,\n",
      "          3.2991e-02, -2.6334e-02, -2.9864e-02, -5.2377e-02, -6.0393e-03,\n",
      "         -2.2137e-02, -3.9132e-02,  1.1605e-02, -6.3626e-03, -1.8322e-02,\n",
      "          2.2543e-03, -5.3605e-03,  3.4478e-04, -2.0593e-02,  3.3782e-02],\n",
      "        [-2.2607e-03,  4.2619e-03, -3.6676e-02,  1.2746e-02,  1.0394e-02,\n",
      "          1.3298e-02,  8.4137e-04, -2.1658e-02, -3.5153e-02, -2.0769e-02,\n",
      "         -2.5972e-02,  2.3643e-02, -4.5379e-02,  2.5453e-02, -1.3394e-02,\n",
      "          1.8683e-02, -5.2322e-02,  5.0827e-02, -5.7434e-02, -2.9557e-02,\n",
      "          1.3341e-02, -5.0450e-02,  3.6588e-02,  5.7802e-02,  2.0705e-02,\n",
      "         -2.6878e-03, -3.5256e-02,  6.6329e-03,  2.5154e-02, -2.8496e-02,\n",
      "         -1.8993e-02, -2.4490e-02, -3.2609e-02,  2.2031e-02, -3.1251e-02,\n",
      "         -8.3745e-04, -2.6679e-02, -1.7705e-02, -2.1892e-02, -3.3461e-02,\n",
      "         -2.5473e-02, -3.3855e-02,  1.0034e-02, -2.9329e-02,  2.8374e-02,\n",
      "         -2.8478e-02, -6.6972e-02,  5.7546e-03,  5.5306e-02, -3.8859e-02],\n",
      "        [ 4.1360e-02,  5.8332e-03, -1.0114e-01,  2.9276e-02,  2.1201e-02,\n",
      "         -1.6665e-02,  4.5933e-02, -2.5295e-02, -3.9692e-02, -5.6481e-02,\n",
      "         -5.7863e-02, -2.2734e-02, -3.6094e-02,  3.5519e-02, -3.9387e-02,\n",
      "         -7.0849e-02, -3.3828e-02,  1.3839e-02, -3.4889e-03,  5.7389e-02,\n",
      "          4.2728e-02, -5.4623e-02,  7.2693e-03, -5.2655e-02,  1.9580e-02,\n",
      "         -1.6002e-02, -2.3239e-02,  1.8644e-02,  1.2915e-02, -5.7977e-02,\n",
      "         -4.5042e-02,  2.4047e-02, -3.7642e-02, -2.6265e-02, -1.2569e-01,\n",
      "          2.9775e-02, -7.8523e-02, -2.4089e-03, -4.7579e-02, -3.8696e-02,\n",
      "         -3.0146e-02, -4.9563e-02,  1.5590e-02, -2.2054e-02,  3.5864e-02,\n",
      "         -1.3118e-02, -2.2331e-02, -2.9608e-02, -5.5599e-02,  1.7675e-02],\n",
      "        [-2.7565e-02,  5.7017e-03,  8.0654e-02, -4.8323e-02,  4.5906e-03,\n",
      "          5.1731e-02, -1.5187e-02, -1.7326e-02, -3.4534e-02, -3.7526e-02,\n",
      "         -3.6040e-02, -8.6271e-03,  6.0021e-03,  4.2519e-02, -6.1421e-03,\n",
      "          4.6621e-02, -2.6473e-02,  1.6814e-02, -5.8197e-02, -4.3945e-02,\n",
      "          3.5526e-03, -6.7536e-02,  1.0287e-02, -4.0017e-02, -4.3858e-02,\n",
      "         -1.5944e-02, -2.0308e-02, -6.9859e-02,  6.1161e-02,  4.8329e-02,\n",
      "         -9.9715e-03,  3.8831e-02, -4.6523e-03,  5.2848e-02,  1.9283e-02,\n",
      "          2.4239e-03, -1.9706e-02, -6.6856e-02, -4.7638e-02, -2.7360e-02,\n",
      "          5.5064e-03, -3.7828e-02, -3.0662e-02, -2.8714e-02, -2.9841e-02,\n",
      "         -5.0502e-02,  1.5599e-04, -8.7439e-03, -1.2974e-02,  3.7572e-02],\n",
      "        [-5.2629e-02, -5.5082e-02, -1.1162e-02, -3.6207e-02, -5.3749e-02,\n",
      "         -5.6588e-02, -5.4584e-02,  1.2437e-02,  1.8592e-02,  1.1946e-02,\n",
      "          2.2748e-02, -7.8589e-02,  4.6211e-02, -4.5418e-02,  2.0640e-02,\n",
      "          4.4336e-02,  1.0735e-02, -9.9783e-03,  7.4187e-04, -3.2423e-02,\n",
      "         -7.0452e-02, -5.8836e-03, -3.5816e-02,  6.7193e-02, -1.0548e-02,\n",
      "          5.5698e-02,  2.2157e-02,  1.9586e-02, -4.4516e-02,  1.5006e-02,\n",
      "          2.2231e-02, -8.3922e-02,  3.1813e-02, -7.0768e-02,  5.2100e-03,\n",
      "         -5.3597e-02,  1.2646e-02,  1.7634e-02,  1.6778e-02,  1.7663e-02,\n",
      "          2.3700e-04,  4.6726e-02,  3.2413e-02,  2.0649e-02,  7.0356e-03,\n",
      "          2.5313e-02,  1.9646e-02, -6.4696e-02, -4.5302e-02, -2.3444e-02],\n",
      "        [-1.7913e-02, -7.5489e-02,  4.3676e-02, -4.1669e-02, -1.2601e-02,\n",
      "         -5.9112e-02, -5.0633e-02,  4.0956e-02, -1.1365e-02,  1.9720e-02,\n",
      "         -2.2278e-03, -3.1187e-02, -2.3911e-03,  1.9413e-02,  2.8838e-02,\n",
      "         -2.4914e-02, -2.0249e-03, -1.9299e-02,  1.4343e-02,  2.3412e-02,\n",
      "         -2.1101e-02,  2.2086e-02, -7.1484e-03, -2.7701e-02, -5.4808e-02,\n",
      "         -6.5412e-02,  1.7008e-02,  2.8570e-02,  1.8748e-02,  1.3159e-02,\n",
      "          2.8915e-02,  1.8272e-02, -2.8994e-03,  9.2520e-03,  2.1501e-02,\n",
      "         -6.6443e-02,  2.0426e-02,  3.6627e-02,  2.3290e-02,  1.1776e-02,\n",
      "          3.1246e-02,  5.2659e-03, -5.9701e-02, -1.4105e-02, -4.7947e-02,\n",
      "         -3.2944e-02,  2.4009e-02, -3.1633e-02, -7.3891e-03, -3.4503e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0173, -0.0383,  0.0372,  0.0072,  0.0288,  0.0234, -0.0029, -0.0065,\n",
      "         0.0080, -0.0434], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json\n",
    "\n",
    "model_params = get_model('modelID')\n",
    "# print(client_response)\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "load_model_from_json(model, model_params)\n",
    "print(model)\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two models and aggregate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0165,  0.0194,  0.0171,  ..., -0.0069, -0.0136, -0.0147],\n",
      "        [ 0.0060,  0.0150,  0.0190,  ..., -0.0616, -0.0620, -0.0606],\n",
      "        [-0.0043, -0.0144, -0.0166,  ..., -0.0392, -0.0331, -0.0260],\n",
      "        ...,\n",
      "        [-0.0403, -0.0381, -0.0357,  ...,  0.0240,  0.0226,  0.0221],\n",
      "        [-0.0104, -0.0060, -0.0007,  ..., -0.0304, -0.0349, -0.0367],\n",
      "        [-0.0251, -0.0113, -0.0075,  ..., -0.0311, -0.0307, -0.0287]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0196,  0.0870,  0.0041,  0.0843,  0.0820,  0.0826,  0.0252, -0.0290,\n",
      "         0.0780, -0.0044,  0.0506,  0.0846,  0.0469, -0.0107, -0.0231,  0.0393,\n",
      "         0.0472,  0.0455,  0.0254,  0.0367,  0.0750,  0.0290,  0.0442,  0.0764,\n",
      "         0.0833,  0.0745,  0.0692, -0.0153, -0.0339, -0.0164, -0.0105,  0.0816,\n",
      "         0.0416,  0.0468,  0.0355,  0.0966, -0.0204, -0.0316, -0.0081,  0.0500,\n",
      "        -0.0329, -0.0091,  0.0948,  0.0269,  0.0601,  0.0513, -0.0091,  0.0158,\n",
      "         0.0381,  0.0419], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0729e-02,  1.6369e-03,  1.7457e-02, -2.0261e-02, -3.5362e-02,\n",
      "         -3.4316e-02,  3.6650e-02, -1.5674e-02,  3.5169e-02, -4.9696e-03,\n",
      "          2.1244e-02, -4.4616e-03, -3.1764e-03, -5.2060e-02,  1.2644e-03,\n",
      "         -2.6990e-02,  3.5615e-02, -6.0836e-02,  4.5971e-02, -5.7113e-02,\n",
      "          2.6357e-02,  2.4081e-02, -2.8939e-02, -3.6222e-03, -3.1129e-02,\n",
      "          1.5366e-02,  1.4063e-02, -4.9014e-02, -3.6614e-02,  3.4805e-02,\n",
      "          7.3209e-03,  5.6828e-03,  1.5886e-02, -1.7330e-02,  2.9071e-02,\n",
      "          7.1362e-03, -7.3371e-03, -4.1359e-02, -3.8137e-03,  1.3146e-02,\n",
      "          1.1137e-05,  1.8308e-02,  9.7951e-03,  2.5185e-02, -6.4025e-02,\n",
      "          3.8309e-02,  9.7468e-03,  4.7813e-02,  1.0280e-02, -4.0392e-02],\n",
      "        [ 4.0677e-02, -4.0055e-02, -5.0535e-03, -3.4151e-02, -2.8060e-02,\n",
      "         -6.6479e-03, -1.6352e-02,  2.4971e-02, -3.1873e-03,  4.9322e-02,\n",
      "          2.1286e-04, -5.5128e-02, -1.9015e-02, -2.7914e-03,  1.9923e-02,\n",
      "         -1.5661e-02,  1.3851e-02, -6.0672e-03,  2.8890e-02,  2.8528e-02,\n",
      "         -7.0068e-02,  6.6781e-02, -7.1055e-02, -4.6835e-02, -4.3818e-02,\n",
      "         -6.8113e-02, -5.0396e-02,  5.0821e-02, -1.7345e-02,  2.9608e-02,\n",
      "          8.0719e-03, -4.0224e-02, -1.7112e-02, -8.2332e-02,  3.7117e-02,\n",
      "         -3.7885e-02,  5.8904e-02,  4.0080e-02,  7.6002e-02,  1.5286e-02,\n",
      "          3.9737e-02,  4.8322e-02, -6.3285e-02,  1.9329e-02, -7.3543e-02,\n",
      "         -3.1983e-02,  1.1420e-02, -4.3303e-02, -3.7898e-02,  1.1530e-02],\n",
      "        [ 5.1135e-03,  4.3775e-02, -5.1879e-02,  2.8158e-02,  9.4534e-03,\n",
      "          1.5822e-02,  2.7002e-02, -5.1034e-02,  2.7472e-02, -2.2898e-02,\n",
      "          2.8557e-02,  4.8029e-02,  4.9925e-03, -3.6357e-02, -4.3372e-02,\n",
      "         -5.7086e-02,  4.5895e-03, -4.5066e-02, -1.5990e-02, -9.2061e-03,\n",
      "          9.9713e-03,  6.2816e-03, -1.8915e-02, -4.1257e-02,  1.5503e-02,\n",
      "          2.3076e-02,  1.8857e-02, -6.8645e-02, -2.8294e-02, -4.5510e-02,\n",
      "         -2.2175e-02,  2.6987e-02, -3.0930e-03,  3.8709e-02,  4.1945e-03,\n",
      "          3.3668e-02, -3.5705e-02, -2.9138e-02, -6.5195e-02,  1.1524e-02,\n",
      "         -6.7092e-02, -4.7139e-02,  3.4183e-02,  1.0564e-03,  1.1486e-02,\n",
      "          2.4159e-02, -1.8142e-02,  4.0312e-02, -4.4129e-03,  1.5848e-02],\n",
      "        [ 6.4835e-03,  2.9141e-03, -3.6810e-02,  2.6821e-02,  2.8724e-03,\n",
      "         -3.3154e-02,  1.4336e-02, -1.3313e-02, -3.6903e-02, -2.7823e-03,\n",
      "         -1.7521e-02,  3.7468e-02, -2.5651e-02,  3.0820e-02, -1.5655e-02,\n",
      "          1.0979e-02, -2.6938e-02,  3.8833e-02, -2.3817e-02, -1.0902e-03,\n",
      "         -1.3144e-03, -4.5085e-02,  3.3939e-02,  1.1828e-02,  4.4826e-02,\n",
      "         -1.7708e-02, -1.1288e-03,  1.6820e-02,  1.8762e-02, -4.6482e-02,\n",
      "         -1.7984e-02, -4.0451e-02, -1.6677e-02,  2.0457e-02, -4.1380e-02,\n",
      "         -1.4254e-02, -2.3425e-02, -5.4581e-03, -1.3648e-02, -3.2707e-02,\n",
      "         -5.7194e-03, -3.5938e-02,  1.0109e-02, -2.7139e-02,  4.4248e-02,\n",
      "         -2.2278e-02, -2.2631e-02,  9.3685e-03,  4.4332e-02, -2.7354e-02],\n",
      "        [-2.0130e-02,  3.0514e-02, -6.5955e-03,  1.6490e-02,  2.8352e-02,\n",
      "          4.0692e-02, -3.1509e-02, -1.9234e-02,  5.6574e-03, -3.1896e-02,\n",
      "         -1.0069e-03,  1.5148e-02, -2.1184e-03, -3.6907e-02, -2.3894e-02,\n",
      "          3.9960e-02,  1.1737e-03, -6.9260e-02, -1.5439e-02,  1.4565e-02,\n",
      "          6.9864e-03, -3.3363e-02,  1.3908e-03, -6.4284e-02,  1.2797e-02,\n",
      "          2.1341e-02,  3.0231e-03, -3.8696e-02, -4.2690e-02, -5.9903e-02,\n",
      "         -1.8093e-02,  3.3463e-02,  3.6324e-03,  2.0562e-03, -3.7257e-02,\n",
      "          3.2991e-02, -2.6334e-02, -2.9864e-02, -5.2377e-02, -6.0393e-03,\n",
      "         -2.2137e-02, -3.9132e-02,  1.1605e-02, -6.3626e-03, -1.8322e-02,\n",
      "          2.2543e-03, -5.3605e-03,  3.4478e-04, -2.0593e-02,  3.3782e-02],\n",
      "        [-2.2607e-03,  4.2619e-03, -3.6676e-02,  1.2746e-02,  1.0394e-02,\n",
      "          1.3298e-02,  8.4137e-04, -2.1658e-02, -3.5153e-02, -2.0769e-02,\n",
      "         -2.5972e-02,  2.3643e-02, -4.5379e-02,  2.5453e-02, -1.3394e-02,\n",
      "          1.8683e-02, -5.2322e-02,  5.0827e-02, -5.7434e-02, -2.9557e-02,\n",
      "          1.3341e-02, -5.0450e-02,  3.6588e-02,  5.7802e-02,  2.0705e-02,\n",
      "         -2.6878e-03, -3.5256e-02,  6.6329e-03,  2.5154e-02, -2.8496e-02,\n",
      "         -1.8993e-02, -2.4490e-02, -3.2609e-02,  2.2031e-02, -3.1251e-02,\n",
      "         -8.3745e-04, -2.6679e-02, -1.7705e-02, -2.1892e-02, -3.3461e-02,\n",
      "         -2.5473e-02, -3.3855e-02,  1.0034e-02, -2.9329e-02,  2.8374e-02,\n",
      "         -2.8478e-02, -6.6972e-02,  5.7546e-03,  5.5306e-02, -3.8859e-02],\n",
      "        [ 4.1360e-02,  5.8332e-03, -1.0114e-01,  2.9276e-02,  2.1201e-02,\n",
      "         -1.6665e-02,  4.5933e-02, -2.5295e-02, -3.9692e-02, -5.6481e-02,\n",
      "         -5.7863e-02, -2.2734e-02, -3.6094e-02,  3.5519e-02, -3.9387e-02,\n",
      "         -7.0849e-02, -3.3828e-02,  1.3839e-02, -3.4889e-03,  5.7389e-02,\n",
      "          4.2728e-02, -5.4623e-02,  7.2693e-03, -5.2655e-02,  1.9580e-02,\n",
      "         -1.6002e-02, -2.3239e-02,  1.8644e-02,  1.2915e-02, -5.7977e-02,\n",
      "         -4.5042e-02,  2.4047e-02, -3.7642e-02, -2.6265e-02, -1.2569e-01,\n",
      "          2.9775e-02, -7.8523e-02, -2.4089e-03, -4.7579e-02, -3.8696e-02,\n",
      "         -3.0146e-02, -4.9563e-02,  1.5590e-02, -2.2054e-02,  3.5864e-02,\n",
      "         -1.3118e-02, -2.2331e-02, -2.9608e-02, -5.5599e-02,  1.7675e-02],\n",
      "        [-2.7565e-02,  5.7017e-03,  8.0654e-02, -4.8323e-02,  4.5906e-03,\n",
      "          5.1731e-02, -1.5187e-02, -1.7326e-02, -3.4534e-02, -3.7526e-02,\n",
      "         -3.6040e-02, -8.6271e-03,  6.0021e-03,  4.2519e-02, -6.1421e-03,\n",
      "          4.6621e-02, -2.6473e-02,  1.6814e-02, -5.8197e-02, -4.3945e-02,\n",
      "          3.5526e-03, -6.7536e-02,  1.0287e-02, -4.0017e-02, -4.3858e-02,\n",
      "         -1.5944e-02, -2.0308e-02, -6.9859e-02,  6.1161e-02,  4.8329e-02,\n",
      "         -9.9715e-03,  3.8831e-02, -4.6523e-03,  5.2848e-02,  1.9283e-02,\n",
      "          2.4239e-03, -1.9706e-02, -6.6856e-02, -4.7638e-02, -2.7360e-02,\n",
      "          5.5064e-03, -3.7828e-02, -3.0662e-02, -2.8714e-02, -2.9841e-02,\n",
      "         -5.0502e-02,  1.5599e-04, -8.7439e-03, -1.2974e-02,  3.7572e-02],\n",
      "        [-5.2629e-02, -5.5082e-02, -1.1162e-02, -3.6207e-02, -5.3749e-02,\n",
      "         -5.6588e-02, -5.4584e-02,  1.2437e-02,  1.8592e-02,  1.1946e-02,\n",
      "          2.2748e-02, -7.8589e-02,  4.6211e-02, -4.5418e-02,  2.0640e-02,\n",
      "          4.4336e-02,  1.0735e-02, -9.9783e-03,  7.4187e-04, -3.2423e-02,\n",
      "         -7.0452e-02, -5.8836e-03, -3.5816e-02,  6.7193e-02, -1.0548e-02,\n",
      "          5.5698e-02,  2.2157e-02,  1.9586e-02, -4.4516e-02,  1.5006e-02,\n",
      "          2.2231e-02, -8.3922e-02,  3.1813e-02, -7.0768e-02,  5.2100e-03,\n",
      "         -5.3597e-02,  1.2646e-02,  1.7634e-02,  1.6778e-02,  1.7663e-02,\n",
      "          2.3700e-04,  4.6726e-02,  3.2413e-02,  2.0649e-02,  7.0356e-03,\n",
      "          2.5313e-02,  1.9646e-02, -6.4696e-02, -4.5302e-02, -2.3444e-02],\n",
      "        [-1.7913e-02, -7.5489e-02,  4.3676e-02, -4.1669e-02, -1.2601e-02,\n",
      "         -5.9112e-02, -5.0633e-02,  4.0956e-02, -1.1365e-02,  1.9720e-02,\n",
      "         -2.2278e-03, -3.1187e-02, -2.3911e-03,  1.9413e-02,  2.8838e-02,\n",
      "         -2.4914e-02, -2.0249e-03, -1.9299e-02,  1.4343e-02,  2.3412e-02,\n",
      "         -2.1101e-02,  2.2086e-02, -7.1484e-03, -2.7701e-02, -5.4808e-02,\n",
      "         -6.5412e-02,  1.7008e-02,  2.8570e-02,  1.8748e-02,  1.3159e-02,\n",
      "          2.8915e-02,  1.8272e-02, -2.8994e-03,  9.2520e-03,  2.1501e-02,\n",
      "         -6.6443e-02,  2.0426e-02,  3.6627e-02,  2.3290e-02,  1.1776e-02,\n",
      "          3.1246e-02,  5.2659e-03, -5.9701e-02, -1.4105e-02, -4.7947e-02,\n",
      "         -3.2944e-02,  2.4009e-02, -3.1633e-02, -7.3891e-03, -3.4503e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0173, -0.0383,  0.0372,  0.0072,  0.0288,  0.0234, -0.0029, -0.0065,\n",
      "         0.0080, -0.0434], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0082,  0.0097,  0.0086,  ..., -0.0034, -0.0068, -0.0074],\n",
      "        [ 0.0030,  0.0075,  0.0095,  ..., -0.0308, -0.0310, -0.0303],\n",
      "        [-0.0021, -0.0072, -0.0083,  ..., -0.0196, -0.0166, -0.0130],\n",
      "        ...,\n",
      "        [-0.0202, -0.0191, -0.0178,  ...,  0.0120,  0.0113,  0.0111],\n",
      "        [-0.0052, -0.0030, -0.0003,  ..., -0.0152, -0.0174, -0.0184],\n",
      "        [-0.0125, -0.0056, -0.0037,  ..., -0.0155, -0.0154, -0.0144]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0098,  0.0435,  0.0020,  0.0421,  0.0410,  0.0413,  0.0126, -0.0145,\n",
      "         0.0390, -0.0022,  0.0253,  0.0423,  0.0234, -0.0054, -0.0115,  0.0197,\n",
      "         0.0236,  0.0227,  0.0127,  0.0183,  0.0375,  0.0145,  0.0221,  0.0382,\n",
      "         0.0416,  0.0372,  0.0346, -0.0076, -0.0170, -0.0082, -0.0053,  0.0408,\n",
      "         0.0208,  0.0234,  0.0178,  0.0483, -0.0102, -0.0158, -0.0041,  0.0250,\n",
      "        -0.0164, -0.0045,  0.0474,  0.0134,  0.0300,  0.0256, -0.0046,  0.0079,\n",
      "         0.0191,  0.0209], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-5.3643e-03,  8.1844e-04,  8.7285e-03, -1.0131e-02, -1.7681e-02,\n",
      "         -1.7158e-02,  1.8325e-02, -7.8372e-03,  1.7585e-02, -2.4848e-03,\n",
      "          1.0622e-02, -2.2308e-03, -1.5882e-03, -2.6030e-02,  6.3220e-04,\n",
      "         -1.3495e-02,  1.7808e-02, -3.0418e-02,  2.2986e-02, -2.8556e-02,\n",
      "          1.3178e-02,  1.2041e-02, -1.4469e-02, -1.8111e-03, -1.5564e-02,\n",
      "          7.6832e-03,  7.0317e-03, -2.4507e-02, -1.8307e-02,  1.7402e-02,\n",
      "          3.6604e-03,  2.8414e-03,  7.9430e-03, -8.6651e-03,  1.4535e-02,\n",
      "          3.5681e-03, -3.6686e-03, -2.0680e-02, -1.9069e-03,  6.5732e-03,\n",
      "          5.5684e-06,  9.1538e-03,  4.8976e-03,  1.2592e-02, -3.2013e-02,\n",
      "          1.9155e-02,  4.8734e-03,  2.3906e-02,  5.1401e-03, -2.0196e-02],\n",
      "        [ 2.0339e-02, -2.0027e-02, -2.5268e-03, -1.7075e-02, -1.4030e-02,\n",
      "         -3.3239e-03, -8.1758e-03,  1.2485e-02, -1.5937e-03,  2.4661e-02,\n",
      "          1.0643e-04, -2.7564e-02, -9.5074e-03, -1.3957e-03,  9.9615e-03,\n",
      "         -7.8305e-03,  6.9253e-03, -3.0336e-03,  1.4445e-02,  1.4264e-02,\n",
      "         -3.5034e-02,  3.3390e-02, -3.5527e-02, -2.3417e-02, -2.1909e-02,\n",
      "         -3.4057e-02, -2.5198e-02,  2.5410e-02, -8.6724e-03,  1.4804e-02,\n",
      "          4.0360e-03, -2.0112e-02, -8.5560e-03, -4.1166e-02,  1.8558e-02,\n",
      "         -1.8942e-02,  2.9452e-02,  2.0040e-02,  3.8001e-02,  7.6431e-03,\n",
      "          1.9868e-02,  2.4161e-02, -3.1642e-02,  9.6645e-03, -3.6772e-02,\n",
      "         -1.5992e-02,  5.7099e-03, -2.1652e-02, -1.8949e-02,  5.7651e-03],\n",
      "        [ 2.5568e-03,  2.1888e-02, -2.5940e-02,  1.4079e-02,  4.7267e-03,\n",
      "          7.9111e-03,  1.3501e-02, -2.5517e-02,  1.3736e-02, -1.1449e-02,\n",
      "          1.4279e-02,  2.4014e-02,  2.4963e-03, -1.8179e-02, -2.1686e-02,\n",
      "         -2.8543e-02,  2.2948e-03, -2.2533e-02, -7.9949e-03, -4.6030e-03,\n",
      "          4.9856e-03,  3.1408e-03, -9.4577e-03, -2.0628e-02,  7.7513e-03,\n",
      "          1.1538e-02,  9.4283e-03, -3.4322e-02, -1.4147e-02, -2.2755e-02,\n",
      "         -1.1088e-02,  1.3494e-02, -1.5465e-03,  1.9354e-02,  2.0973e-03,\n",
      "          1.6834e-02, -1.7853e-02, -1.4569e-02, -3.2598e-02,  5.7620e-03,\n",
      "         -3.3546e-02, -2.3570e-02,  1.7091e-02,  5.2819e-04,  5.7429e-03,\n",
      "          1.2079e-02, -9.0708e-03,  2.0156e-02, -2.2065e-03,  7.9242e-03],\n",
      "        [ 3.2418e-03,  1.4570e-03, -1.8405e-02,  1.3411e-02,  1.4362e-03,\n",
      "         -1.6577e-02,  7.1682e-03, -6.6567e-03, -1.8451e-02, -1.3912e-03,\n",
      "         -8.7605e-03,  1.8734e-02, -1.2825e-02,  1.5410e-02, -7.8275e-03,\n",
      "          5.4895e-03, -1.3469e-02,  1.9417e-02, -1.1909e-02, -5.4508e-04,\n",
      "         -6.5719e-04, -2.2542e-02,  1.6969e-02,  5.9138e-03,  2.2413e-02,\n",
      "         -8.8542e-03, -5.6441e-04,  8.4101e-03,  9.3810e-03, -2.3241e-02,\n",
      "         -8.9922e-03, -2.0225e-02, -8.3385e-03,  1.0229e-02, -2.0690e-02,\n",
      "         -7.1271e-03, -1.1713e-02, -2.7291e-03, -6.8242e-03, -1.6354e-02,\n",
      "         -2.8597e-03, -1.7969e-02,  5.0547e-03, -1.3569e-02,  2.2124e-02,\n",
      "         -1.1139e-02, -1.1316e-02,  4.6842e-03,  2.2166e-02, -1.3677e-02],\n",
      "        [-1.0065e-02,  1.5257e-02, -3.2977e-03,  8.2452e-03,  1.4176e-02,\n",
      "          2.0346e-02, -1.5755e-02, -9.6170e-03,  2.8287e-03, -1.5948e-02,\n",
      "         -5.0346e-04,  7.5740e-03, -1.0592e-03, -1.8453e-02, -1.1947e-02,\n",
      "          1.9980e-02,  5.8686e-04, -3.4630e-02, -7.7195e-03,  7.2824e-03,\n",
      "          3.4932e-03, -1.6681e-02,  6.9540e-04, -3.2142e-02,  6.3986e-03,\n",
      "          1.0671e-02,  1.5115e-03, -1.9348e-02, -2.1345e-02, -2.9951e-02,\n",
      "         -9.0465e-03,  1.6732e-02,  1.8162e-03,  1.0281e-03, -1.8628e-02,\n",
      "          1.6495e-02, -1.3167e-02, -1.4932e-02, -2.6188e-02, -3.0196e-03,\n",
      "         -1.1069e-02, -1.9566e-02,  5.8025e-03, -3.1813e-03, -9.1612e-03,\n",
      "          1.1271e-03, -2.6803e-03,  1.7239e-04, -1.0296e-02,  1.6891e-02],\n",
      "        [-1.1303e-03,  2.1310e-03, -1.8338e-02,  6.3728e-03,  5.1969e-03,\n",
      "          6.6490e-03,  4.2069e-04, -1.0829e-02, -1.7577e-02, -1.0385e-02,\n",
      "         -1.2986e-02,  1.1821e-02, -2.2689e-02,  1.2726e-02, -6.6968e-03,\n",
      "          9.3415e-03, -2.6161e-02,  2.5414e-02, -2.8717e-02, -1.4779e-02,\n",
      "          6.6703e-03, -2.5225e-02,  1.8294e-02,  2.8901e-02,  1.0352e-02,\n",
      "         -1.3439e-03, -1.7628e-02,  3.3165e-03,  1.2577e-02, -1.4248e-02,\n",
      "         -9.4966e-03, -1.2245e-02, -1.6305e-02,  1.1016e-02, -1.5625e-02,\n",
      "         -4.1873e-04, -1.3339e-02, -8.8527e-03, -1.0946e-02, -1.6731e-02,\n",
      "         -1.2736e-02, -1.6927e-02,  5.0171e-03, -1.4664e-02,  1.4187e-02,\n",
      "         -1.4239e-02, -3.3486e-02,  2.8773e-03,  2.7653e-02, -1.9429e-02],\n",
      "        [ 2.0680e-02,  2.9166e-03, -5.0568e-02,  1.4638e-02,  1.0601e-02,\n",
      "         -8.3324e-03,  2.2967e-02, -1.2647e-02, -1.9846e-02, -2.8241e-02,\n",
      "         -2.8931e-02, -1.1367e-02, -1.8047e-02,  1.7760e-02, -1.9693e-02,\n",
      "         -3.5424e-02, -1.6914e-02,  6.9195e-03, -1.7444e-03,  2.8694e-02,\n",
      "          2.1364e-02, -2.7311e-02,  3.6346e-03, -2.6327e-02,  9.7902e-03,\n",
      "         -8.0011e-03, -1.1620e-02,  9.3221e-03,  6.4573e-03, -2.8989e-02,\n",
      "         -2.2521e-02,  1.2023e-02, -1.8821e-02, -1.3133e-02, -6.2844e-02,\n",
      "          1.4887e-02, -3.9261e-02, -1.2044e-03, -2.3790e-02, -1.9348e-02,\n",
      "         -1.5073e-02, -2.4782e-02,  7.7949e-03, -1.1027e-02,  1.7932e-02,\n",
      "         -6.5590e-03, -1.1165e-02, -1.4804e-02, -2.7800e-02,  8.8374e-03],\n",
      "        [-1.3782e-02,  2.8509e-03,  4.0327e-02, -2.4161e-02,  2.2953e-03,\n",
      "          2.5866e-02, -7.5933e-03, -8.6632e-03, -1.7267e-02, -1.8763e-02,\n",
      "         -1.8020e-02, -4.3136e-03,  3.0011e-03,  2.1259e-02, -3.0711e-03,\n",
      "          2.3310e-02, -1.3237e-02,  8.4072e-03, -2.9098e-02, -2.1973e-02,\n",
      "          1.7763e-03, -3.3768e-02,  5.1436e-03, -2.0009e-02, -2.1929e-02,\n",
      "         -7.9722e-03, -1.0154e-02, -3.4930e-02,  3.0581e-02,  2.4164e-02,\n",
      "         -4.9857e-03,  1.9415e-02, -2.3262e-03,  2.6424e-02,  9.6413e-03,\n",
      "          1.2120e-03, -9.8532e-03, -3.3428e-02, -2.3819e-02, -1.3680e-02,\n",
      "          2.7532e-03, -1.8914e-02, -1.5331e-02, -1.4357e-02, -1.4920e-02,\n",
      "         -2.5251e-02,  7.7996e-05, -4.3719e-03, -6.4869e-03,  1.8786e-02],\n",
      "        [-2.6314e-02, -2.7541e-02, -5.5812e-03, -1.8104e-02, -2.6875e-02,\n",
      "         -2.8294e-02, -2.7292e-02,  6.2184e-03,  9.2958e-03,  5.9728e-03,\n",
      "          1.1374e-02, -3.9295e-02,  2.3106e-02, -2.2709e-02,  1.0320e-02,\n",
      "          2.2168e-02,  5.3675e-03, -4.9891e-03,  3.7094e-04, -1.6211e-02,\n",
      "         -3.5226e-02, -2.9418e-03, -1.7908e-02,  3.3596e-02, -5.2739e-03,\n",
      "          2.7849e-02,  1.1079e-02,  9.7928e-03, -2.2258e-02,  7.5032e-03,\n",
      "          1.1115e-02, -4.1961e-02,  1.5907e-02, -3.5384e-02,  2.6050e-03,\n",
      "         -2.6798e-02,  6.3228e-03,  8.8172e-03,  8.3891e-03,  8.8316e-03,\n",
      "          1.1850e-04,  2.3363e-02,  1.6207e-02,  1.0324e-02,  3.5178e-03,\n",
      "          1.2656e-02,  9.8231e-03, -3.2348e-02, -2.2651e-02, -1.1722e-02],\n",
      "        [-8.9563e-03, -3.7744e-02,  2.1838e-02, -2.0835e-02, -6.3004e-03,\n",
      "         -2.9556e-02, -2.5317e-02,  2.0478e-02, -5.6823e-03,  9.8599e-03,\n",
      "         -1.1139e-03, -1.5593e-02, -1.1955e-03,  9.7065e-03,  1.4419e-02,\n",
      "         -1.2457e-02, -1.0124e-03, -9.6494e-03,  7.1713e-03,  1.1706e-02,\n",
      "         -1.0550e-02,  1.1043e-02, -3.5742e-03, -1.3850e-02, -2.7404e-02,\n",
      "         -3.2706e-02,  8.5042e-03,  1.4285e-02,  9.3738e-03,  6.5794e-03,\n",
      "          1.4458e-02,  9.1360e-03, -1.4497e-03,  4.6260e-03,  1.0751e-02,\n",
      "         -3.3222e-02,  1.0213e-02,  1.8313e-02,  1.1645e-02,  5.8879e-03,\n",
      "          1.5623e-02,  2.6330e-03, -2.9850e-02, -7.0527e-03, -2.3974e-02,\n",
      "         -1.6472e-02,  1.2004e-02, -1.5816e-02, -3.6945e-03, -1.7251e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0086, -0.0191,  0.0186,  0.0036,  0.0144,  0.0117, -0.0014, -0.0033,\n",
      "         0.0040, -0.0217], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os.path import join as pjoin\n",
    "import json\n",
    "from client.services.gateway_client import submit_local_model, get_all_models, get_model\n",
    "from client.utils import load_model_from_json, weights_zero_init\n",
    "from client.aggregators import federated_aggregate\n",
    "\n",
    "model_params = get_model('modelID')\n",
    "# print(client_response)\n",
    "\n",
    "\n",
    "model1 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model1.to(device)\n",
    "# print(model1)\n",
    "\n",
    "load_model_from_json(model1, model_params)\n",
    "\n",
    "model2 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model1.to(device)\n",
    "# print(model2)\n",
    "load_model_from_json(model2, model_params)\n",
    "\n",
    "\n",
    "model3 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model3.to(device)\n",
    "weights_zero_init(model3)\n",
    "# print(model3)\n",
    "\n",
    "model_avg = federated_aggregate([model1, model3])\n",
    "\n",
    "# print(model_avg)\n",
    "\n",
    "\n",
    "\n",
    "for parameter in model1.parameters():\n",
    "    print(parameter)\n",
    "    \n",
    "for parameter in model_avg.parameters():\n",
    "    print(parameter)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from client.utils import weights_zero_init\n",
    "\n",
    "model3 = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model3.to(device)\n",
    "weights_zero_init(model3)\n",
    "print(model3)\n",
    "\n",
    "for parameter in model3.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample neural network code that checks if the tensors and model are running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "# Instantiate the model (on the default device\n",
    "model = Perceptron(n_features).to('cuda')\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "# Binary target values (0 or 1)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    start_event.record()\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    print(f\"Elapsed time (in milliseconds): {elapsed_time_ms}\")\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
