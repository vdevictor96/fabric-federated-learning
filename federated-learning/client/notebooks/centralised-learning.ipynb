{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the module directory to import python files (RUN JUST ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/victor/_bcfl/fabric-federated-learning/federated-learning/client/notebooks', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python311.zip', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/lib-dynload', '', '/home/victor/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages', '/home/victor/_bcfl/fabric-federated-learning/federated-learning']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your project\n",
    "import sys\n",
    "sys.path.append('/home/victor/_bcfl/fabric-federated-learning/federated-learning')  # Replace with the path to your models directory\n",
    "print(sys.path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "NVIDIA GeForce MX150\n",
      "major and minor cuda capability of the device: (6, 1)\n",
      "Using device: cuda\n",
      "Cuda set as default device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "# Get the name of the CUDA device\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    print(f\"major and minor cuda capability of the device: {torch.cuda.get_device_capability()}\")\n",
    "except Exception:\n",
    "    print(\"No Cuda available\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device: %s'%device)# Check if CUDA is available and set the default tensor type to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print(\"Cuda set as default device\")\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"Cuda not available, CPU set as default device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a sample Perceptron to try the blockchain integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0, Loss: 0.696426510810852\n",
      "Epoch 1, Loss: 0.6962855458259583\n",
      "Epoch 2, Loss: 0.6961452960968018\n",
      "Epoch 3, Loss: 0.6960055232048035\n",
      "Epoch 4, Loss: 0.6958665251731873\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# torch.set_default_device('cpu')\n",
    "\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "num_classes = 1  # Example number of classes\n",
    "# model = Perceptron(n_features, num_classes)  # Instantiate the model (on the default device\n",
    "model = Perceptron(n_features)  # Instantiate the model (on the default device\n",
    "\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()  # Binary target values (0 or 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CIFAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from client.dataloader import get_cifar10_dataloaders, get_cifar10_datasets\n",
    "\n",
    "root = 'client/data/'\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "batch_size = 200\n",
    "train_dataset, val_dataset, test_dataset = get_cifar10_datasets(root, num_training, num_validation)\n",
    "train_loader, val_loader, test_loader = get_cifar10_dataloaders(root, batch_size, num_training, num_validation, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/3], Step [1/245], Loss: 2.3026\n",
      "Epoch [1/3], Step [2/245], Loss: 2.3007\n",
      "Epoch [1/3], Step [3/245], Loss: 2.2931\n",
      "Epoch [1/3], Step [4/245], Loss: 2.2820\n",
      "Epoch [1/3], Step [5/245], Loss: 2.2712\n",
      "Epoch [1/3], Step [6/245], Loss: 2.2492\n",
      "Epoch [1/3], Step [7/245], Loss: 2.2236\n",
      "Epoch [1/3], Step [8/245], Loss: 2.2153\n",
      "Epoch [1/3], Step [9/245], Loss: 2.2060\n",
      "Epoch [1/3], Step [10/245], Loss: 2.1720\n",
      "Epoch [1/3], Step [11/245], Loss: 2.1496\n",
      "Epoch [1/3], Step [12/245], Loss: 2.1400\n",
      "Epoch [1/3], Step [13/245], Loss: 2.1210\n",
      "Epoch [1/3], Step [14/245], Loss: 2.0502\n",
      "Epoch [1/3], Step [15/245], Loss: 2.0776\n",
      "Epoch [1/3], Step [16/245], Loss: 2.0475\n",
      "Epoch [1/3], Step [17/245], Loss: 2.0183\n",
      "Epoch [1/3], Step [18/245], Loss: 2.0669\n",
      "Epoch [1/3], Step [19/245], Loss: 2.0755\n",
      "Epoch [1/3], Step [20/245], Loss: 2.0481\n",
      "Epoch [1/3], Step [21/245], Loss: 2.0394\n",
      "Epoch [1/3], Step [22/245], Loss: 1.9936\n",
      "Epoch [1/3], Step [23/245], Loss: 2.0556\n",
      "Epoch [1/3], Step [24/245], Loss: 2.0427\n",
      "Epoch [1/3], Step [25/245], Loss: 1.8788\n",
      "Epoch [1/3], Step [26/245], Loss: 1.9714\n",
      "Epoch [1/3], Step [27/245], Loss: 2.0228\n",
      "Epoch [1/3], Step [28/245], Loss: 2.0219\n",
      "Epoch [1/3], Step [29/245], Loss: 1.8947\n",
      "Epoch [1/3], Step [30/245], Loss: 1.9526\n",
      "Epoch [1/3], Step [31/245], Loss: 1.8552\n",
      "Epoch [1/3], Step [32/245], Loss: 1.9426\n",
      "Epoch [1/3], Step [33/245], Loss: 1.9542\n",
      "Epoch [1/3], Step [34/245], Loss: 1.8244\n",
      "Epoch [1/3], Step [35/245], Loss: 1.9653\n",
      "Epoch [1/3], Step [36/245], Loss: 1.8413\n",
      "Epoch [1/3], Step [37/245], Loss: 1.9031\n",
      "Epoch [1/3], Step [38/245], Loss: 1.7972\n",
      "Epoch [1/3], Step [39/245], Loss: 1.8703\n",
      "Epoch [1/3], Step [40/245], Loss: 1.8474\n",
      "Epoch [1/3], Step [41/245], Loss: 1.7954\n",
      "Epoch [1/3], Step [42/245], Loss: 1.8673\n",
      "Epoch [1/3], Step [43/245], Loss: 1.7622\n",
      "Epoch [1/3], Step [44/245], Loss: 1.9039\n",
      "Epoch [1/3], Step [45/245], Loss: 1.9275\n",
      "Epoch [1/3], Step [46/245], Loss: 1.8602\n",
      "Epoch [1/3], Step [47/245], Loss: 1.8787\n",
      "Epoch [1/3], Step [48/245], Loss: 1.8139\n",
      "Epoch [1/3], Step [49/245], Loss: 1.8411\n",
      "Epoch [1/3], Step [50/245], Loss: 1.8854\n",
      "Epoch [1/3], Step [51/245], Loss: 1.9509\n",
      "Epoch [1/3], Step [52/245], Loss: 1.7776\n",
      "Epoch [1/3], Step [53/245], Loss: 1.8659\n",
      "Epoch [1/3], Step [54/245], Loss: 1.8024\n",
      "Epoch [1/3], Step [55/245], Loss: 1.8936\n",
      "Epoch [1/3], Step [56/245], Loss: 1.7866\n",
      "Epoch [1/3], Step [57/245], Loss: 1.9144\n",
      "Epoch [1/3], Step [58/245], Loss: 1.8624\n",
      "Epoch [1/3], Step [59/245], Loss: 1.8734\n",
      "Epoch [1/3], Step [60/245], Loss: 2.0136\n",
      "Epoch [1/3], Step [61/245], Loss: 1.9304\n",
      "Epoch [1/3], Step [62/245], Loss: 1.7723\n",
      "Epoch [1/3], Step [63/245], Loss: 1.8201\n",
      "Epoch [1/3], Step [64/245], Loss: 1.8667\n",
      "Epoch [1/3], Step [65/245], Loss: 1.8822\n",
      "Epoch [1/3], Step [66/245], Loss: 1.8299\n",
      "Epoch [1/3], Step [67/245], Loss: 1.8387\n",
      "Epoch [1/3], Step [68/245], Loss: 1.9353\n",
      "Epoch [1/3], Step [69/245], Loss: 1.7480\n",
      "Epoch [1/3], Step [70/245], Loss: 1.9187\n",
      "Epoch [1/3], Step [71/245], Loss: 1.8571\n",
      "Epoch [1/3], Step [72/245], Loss: 1.7428\n",
      "Epoch [1/3], Step [73/245], Loss: 1.8486\n",
      "Epoch [1/3], Step [74/245], Loss: 1.8174\n",
      "Epoch [1/3], Step [75/245], Loss: 1.8084\n",
      "Epoch [1/3], Step [76/245], Loss: 1.9317\n",
      "Epoch [1/3], Step [77/245], Loss: 1.8860\n",
      "Epoch [1/3], Step [78/245], Loss: 1.8395\n",
      "Epoch [1/3], Step [79/245], Loss: 1.7799\n",
      "Epoch [1/3], Step [80/245], Loss: 1.7829\n",
      "Epoch [1/3], Step [81/245], Loss: 1.8536\n",
      "Epoch [1/3], Step [82/245], Loss: 1.8039\n",
      "Epoch [1/3], Step [83/245], Loss: 1.8276\n",
      "Epoch [1/3], Step [84/245], Loss: 1.7330\n",
      "Epoch [1/3], Step [85/245], Loss: 1.7711\n",
      "Epoch [1/3], Step [86/245], Loss: 1.7873\n",
      "Epoch [1/3], Step [87/245], Loss: 1.8192\n",
      "Epoch [1/3], Step [88/245], Loss: 1.7763\n",
      "Epoch [1/3], Step [89/245], Loss: 1.8277\n",
      "Epoch [1/3], Step [90/245], Loss: 1.7734\n",
      "Epoch [1/3], Step [91/245], Loss: 1.7816\n",
      "Epoch [1/3], Step [92/245], Loss: 1.7169\n",
      "Epoch [1/3], Step [93/245], Loss: 1.7524\n",
      "Epoch [1/3], Step [94/245], Loss: 1.7524\n",
      "Epoch [1/3], Step [95/245], Loss: 1.7345\n",
      "Epoch [1/3], Step [96/245], Loss: 1.7946\n",
      "Epoch [1/3], Step [97/245], Loss: 1.7824\n",
      "Epoch [1/3], Step [98/245], Loss: 1.7494\n",
      "Epoch [1/3], Step [99/245], Loss: 1.7622\n",
      "Epoch [1/3], Step [100/245], Loss: 1.7445\n",
      "Epoch [1/3], Step [101/245], Loss: 1.7198\n",
      "Epoch [1/3], Step [102/245], Loss: 1.7712\n",
      "Epoch [1/3], Step [103/245], Loss: 1.8695\n",
      "Epoch [1/3], Step [104/245], Loss: 1.7316\n",
      "Epoch [1/3], Step [105/245], Loss: 1.8607\n",
      "Epoch [1/3], Step [106/245], Loss: 1.6965\n",
      "Epoch [1/3], Step [107/245], Loss: 1.6870\n",
      "Epoch [1/3], Step [108/245], Loss: 1.6968\n",
      "Epoch [1/3], Step [109/245], Loss: 1.7846\n",
      "Epoch [1/3], Step [110/245], Loss: 1.7613\n",
      "Epoch [1/3], Step [111/245], Loss: 1.6806\n",
      "Epoch [1/3], Step [112/245], Loss: 1.7924\n",
      "Epoch [1/3], Step [113/245], Loss: 1.7804\n",
      "Epoch [1/3], Step [114/245], Loss: 1.7266\n",
      "Epoch [1/3], Step [115/245], Loss: 1.7747\n",
      "Epoch [1/3], Step [116/245], Loss: 1.7167\n",
      "Epoch [1/3], Step [117/245], Loss: 1.7556\n",
      "Epoch [1/3], Step [118/245], Loss: 1.6897\n",
      "Epoch [1/3], Step [119/245], Loss: 1.7435\n",
      "Epoch [1/3], Step [120/245], Loss: 1.6663\n",
      "Epoch [1/3], Step [121/245], Loss: 1.7925\n",
      "Epoch [1/3], Step [122/245], Loss: 1.7137\n",
      "Epoch [1/3], Step [123/245], Loss: 1.7983\n",
      "Epoch [1/3], Step [124/245], Loss: 1.6436\n",
      "Epoch [1/3], Step [125/245], Loss: 1.7831\n",
      "Epoch [1/3], Step [126/245], Loss: 1.7195\n",
      "Epoch [1/3], Step [127/245], Loss: 1.8156\n",
      "Epoch [1/3], Step [128/245], Loss: 1.6505\n",
      "Epoch [1/3], Step [129/245], Loss: 1.7230\n",
      "Epoch [1/3], Step [130/245], Loss: 1.7612\n",
      "Epoch [1/3], Step [131/245], Loss: 1.7072\n",
      "Epoch [1/3], Step [132/245], Loss: 1.6974\n",
      "Epoch [1/3], Step [133/245], Loss: 1.7017\n",
      "Epoch [1/3], Step [134/245], Loss: 1.6887\n",
      "Epoch [1/3], Step [135/245], Loss: 1.7037\n",
      "Epoch [1/3], Step [136/245], Loss: 1.6601\n",
      "Epoch [1/3], Step [137/245], Loss: 1.7051\n",
      "Epoch [1/3], Step [138/245], Loss: 1.6103\n",
      "Epoch [1/3], Step [139/245], Loss: 1.6392\n",
      "Epoch [1/3], Step [140/245], Loss: 1.6624\n",
      "Epoch [1/3], Step [141/245], Loss: 1.6704\n",
      "Epoch [1/3], Step [142/245], Loss: 1.6859\n",
      "Epoch [1/3], Step [143/245], Loss: 1.7818\n",
      "Epoch [1/3], Step [144/245], Loss: 1.6838\n",
      "Epoch [1/3], Step [145/245], Loss: 1.6960\n",
      "Epoch [1/3], Step [146/245], Loss: 1.7509\n",
      "Epoch [1/3], Step [147/245], Loss: 1.7404\n",
      "Epoch [1/3], Step [148/245], Loss: 1.7796\n",
      "Epoch [1/3], Step [149/245], Loss: 1.6197\n",
      "Epoch [1/3], Step [150/245], Loss: 1.6067\n",
      "Epoch [1/3], Step [151/245], Loss: 1.6968\n",
      "Epoch [1/3], Step [152/245], Loss: 1.6940\n",
      "Epoch [1/3], Step [153/245], Loss: 1.7616\n",
      "Epoch [1/3], Step [154/245], Loss: 1.7224\n",
      "Epoch [1/3], Step [155/245], Loss: 1.6680\n",
      "Epoch [1/3], Step [156/245], Loss: 1.6736\n",
      "Epoch [1/3], Step [157/245], Loss: 1.6642\n",
      "Epoch [1/3], Step [158/245], Loss: 1.6622\n",
      "Epoch [1/3], Step [159/245], Loss: 1.6874\n",
      "Epoch [1/3], Step [160/245], Loss: 1.6936\n",
      "Epoch [1/3], Step [161/245], Loss: 1.6470\n",
      "Epoch [1/3], Step [162/245], Loss: 1.6484\n",
      "Epoch [1/3], Step [163/245], Loss: 1.6479\n",
      "Epoch [1/3], Step [164/245], Loss: 1.6962\n",
      "Epoch [1/3], Step [165/245], Loss: 1.7374\n",
      "Epoch [1/3], Step [166/245], Loss: 1.5369\n",
      "Epoch [1/3], Step [167/245], Loss: 1.7115\n",
      "Epoch [1/3], Step [168/245], Loss: 1.6453\n",
      "Epoch [1/3], Step [169/245], Loss: 1.7372\n",
      "Epoch [1/3], Step [170/245], Loss: 1.7923\n",
      "Epoch [1/3], Step [171/245], Loss: 1.7280\n",
      "Epoch [1/3], Step [172/245], Loss: 1.6342\n",
      "Epoch [1/3], Step [173/245], Loss: 1.6794\n",
      "Epoch [1/3], Step [174/245], Loss: 1.7201\n",
      "Epoch [1/3], Step [175/245], Loss: 1.5681\n",
      "Epoch [1/3], Step [176/245], Loss: 1.7393\n",
      "Epoch [1/3], Step [177/245], Loss: 1.6242\n",
      "Epoch [1/3], Step [178/245], Loss: 1.5976\n",
      "Epoch [1/3], Step [179/245], Loss: 1.7155\n",
      "Epoch [1/3], Step [180/245], Loss: 1.7006\n",
      "Epoch [1/3], Step [181/245], Loss: 1.5669\n",
      "Epoch [1/3], Step [182/245], Loss: 1.6760\n",
      "Epoch [1/3], Step [183/245], Loss: 1.7900\n",
      "Epoch [1/3], Step [184/245], Loss: 1.6508\n",
      "Epoch [1/3], Step [185/245], Loss: 1.6419\n",
      "Epoch [1/3], Step [186/245], Loss: 1.5378\n",
      "Epoch [1/3], Step [187/245], Loss: 1.6580\n",
      "Epoch [1/3], Step [188/245], Loss: 1.6520\n",
      "Epoch [1/3], Step [189/245], Loss: 1.6533\n",
      "Epoch [1/3], Step [190/245], Loss: 1.6396\n",
      "Epoch [1/3], Step [191/245], Loss: 1.5181\n",
      "Epoch [1/3], Step [192/245], Loss: 1.6783\n",
      "Epoch [1/3], Step [193/245], Loss: 1.7678\n",
      "Epoch [1/3], Step [194/245], Loss: 1.6597\n",
      "Epoch [1/3], Step [195/245], Loss: 1.7944\n",
      "Epoch [1/3], Step [196/245], Loss: 1.7823\n",
      "Epoch [1/3], Step [197/245], Loss: 1.7264\n",
      "Epoch [1/3], Step [198/245], Loss: 1.6685\n",
      "Epoch [1/3], Step [199/245], Loss: 1.6044\n",
      "Epoch [1/3], Step [200/245], Loss: 1.6209\n",
      "Epoch [1/3], Step [201/245], Loss: 1.5989\n",
      "Epoch [1/3], Step [202/245], Loss: 1.7513\n",
      "Epoch [1/3], Step [203/245], Loss: 1.6046\n",
      "Epoch [1/3], Step [204/245], Loss: 1.7423\n",
      "Epoch [1/3], Step [205/245], Loss: 1.6322\n",
      "Epoch [1/3], Step [206/245], Loss: 1.6342\n",
      "Epoch [1/3], Step [207/245], Loss: 1.7585\n",
      "Epoch [1/3], Step [208/245], Loss: 1.6845\n",
      "Epoch [1/3], Step [209/245], Loss: 1.5795\n",
      "Epoch [1/3], Step [210/245], Loss: 1.6715\n",
      "Epoch [1/3], Step [211/245], Loss: 1.4908\n",
      "Epoch [1/3], Step [212/245], Loss: 1.7103\n",
      "Epoch [1/3], Step [213/245], Loss: 1.6567\n",
      "Epoch [1/3], Step [214/245], Loss: 1.6802\n",
      "Epoch [1/3], Step [215/245], Loss: 1.7162\n",
      "Epoch [1/3], Step [216/245], Loss: 1.7092\n",
      "Epoch [1/3], Step [217/245], Loss: 1.7393\n",
      "Epoch [1/3], Step [218/245], Loss: 1.5784\n",
      "Epoch [1/3], Step [219/245], Loss: 1.5603\n",
      "Epoch [1/3], Step [220/245], Loss: 1.7350\n",
      "Epoch [1/3], Step [221/245], Loss: 1.6821\n",
      "Epoch [1/3], Step [222/245], Loss: 1.6093\n",
      "Epoch [1/3], Step [223/245], Loss: 1.6540\n",
      "Epoch [1/3], Step [224/245], Loss: 1.7322\n",
      "Epoch [1/3], Step [225/245], Loss: 1.7224\n",
      "Epoch [1/3], Step [226/245], Loss: 1.6399\n",
      "Epoch [1/3], Step [227/245], Loss: 1.5135\n",
      "Epoch [1/3], Step [228/245], Loss: 1.5292\n",
      "Epoch [1/3], Step [229/245], Loss: 1.7630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mreg)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclient/models/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/_bcfl/fabric-federated-learning/federated-learning/client/train.py:21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, modelpath, criterion, optimizer, learning_rate, learning_rate_decay, input_size, num_epochs, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if(i>2):\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#     break\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move tensors to the configured device\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/torchvision/datasets/cifar.py:115\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/bcfl-fabric/lib/python3.11/site-packages/PIL/Image.py:3099\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3099\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3101\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from client.model.perceptron import MultiLayerPerceptron\n",
    "from client.train import train\n",
    "from client.utils import weights_init, update_lr\n",
    "\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'validation': val_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "    \n",
    "# Training\n",
    "model.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "train(model, dataloaders, 'client/models/', criterion, optimizer, learning_rate, learning_rate_decay, input_size, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2725263433.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * c\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = MultiLayerPerceptron(input_size, hidden_size, num_classes)\n",
    "best_model = torch.load(pjoin(modelpath, 'model.ckpt'))\n",
    "model.load_state_dict(best_model)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "       \n",
    "        # reshape images to input size\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        # set the model for evaluation\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample neural network code that checks if the tensors and model are running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Elapsed time (in milliseconds): 0.2836480140686035\n",
      "Epoch 0, Loss: 0.7509689331054688\n",
      "Elapsed time (in milliseconds): 0.23996800184249878\n",
      "Epoch 1, Loss: 0.7505505681037903\n",
      "Elapsed time (in milliseconds): 0.1724800020456314\n",
      "Epoch 2, Loss: 0.7501339316368103\n",
      "Elapsed time (in milliseconds): 0.151296004652977\n",
      "Epoch 3, Loss: 0.7497190833091736\n",
      "Elapsed time (in milliseconds): 0.11884800344705582\n",
      "Epoch 4, Loss: 0.7493058443069458\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from client.model.perceptron import Perceptron\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "n_features = 10  # Example number of input features\n",
    "model = Perceptron(n_features).to('cuda')  # Instantiate the model (on the default device\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "# Example (dummy) training data\n",
    "dummy_inputs = torch.randn(100, n_features)  # 100 samples, n_features each\n",
    "print(dummy_inputs.device)\n",
    "dummy_targets = torch.randint(0, 2, (100, 1)).float()  # Binary target values (0 or 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clearing the gradients\n",
    "    start_event.record()\n",
    "    outputs = model(dummy_inputs)  # Forward pass\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    print(f\"Elapsed time (in milliseconds): {elapsed_time_ms}\")\n",
    "    loss = loss_function(outputs, dummy_targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcfl-fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
